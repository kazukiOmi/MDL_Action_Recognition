{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvision.datasets.UFC101の使い方\n",
    "\n",
    "torchvisionのdatasetを使ってUFC101を読み込む．\n",
    "\n",
    "- https://pytorch.org/vision/stable/datasets.html?highlight=ucf101#torchvision.datasets.UCF101\n",
    "\n",
    "\n",
    "# データセットの準備\n",
    "\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．\n",
    "- `/dataset/UCF101/video/`以下に，101クラスのサブディレクトリがある\n",
    "- `/dataset/UCF101/ucfTrainTestlist/`以下に，UCF101のアノテーションファイルであるtrainlist0{1,2,3}.txtなどがある\n",
    "\n",
    "## フォルダ構成\n",
    "\n",
    "```bash\n",
    "$ tree -I \"*.avi\" /dataset/UCF101\n",
    "/dataset/UCF101\n",
    "├── ucfTrainTestlist\n",
    "│   ├── classInd.txt\n",
    "│   ├── testlist01.txt\n",
    "│   ├── testlist02.txt\n",
    "│   ├── testlist03.txt\n",
    "│   ├── trainlist01.txt\n",
    "│   ├── trainlist02.txt\n",
    "│   └── trainlist03.txt\n",
    "└── video\n",
    "    ├── ApplyEyeMakeup\n",
    "    ├── ApplyLipstick\n",
    "    ├── Archery\n",
    "    ├── BabyCrawling\n",
    "    ├── BalanceBeam\n",
    "...\n",
    "    ├── Typing\n",
    "    ├── UnevenBars\n",
    "    ├── VolleyballSpiking\n",
    "    ├── WalkingWithDog\n",
    "    ├── WallPushups\n",
    "    ├── WritingOnBoard\n",
    "    └── YoYo\n",
    "\n",
    "$ head -5 /dataset/UCF101/ucfTrainTestlist/*\n",
    "==> /dataset/UCF101/ucfTrainTestlist/classInd.txt <==\n",
    "1 ApplyEyeMakeup\n",
    "2 ApplyLipstick\n",
    "3 Archery\n",
    "4 BabyCrawling\n",
    "5 BalanceBeam\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/testlist01.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/testlist02.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/testlist03.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c01.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c02.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c03.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c04.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c05.avi\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/trainlist01.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/trainlist02.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi 1\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/trainlist03.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import UCF101\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "- rootで指定したディレクトリには，101クラスのサブディレクトリがあること\n",
    "- annotation_pathには，UCF101のアノテーションファイルであるtrainlist0{1,2,3}.txtなどがあること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/dataset/UCF101/'\n",
    "        self.root = self.metadata_path + 'video/'\n",
    "        self.annotation_path = \\\n",
    "            self.metadata_path + 'ucfTrainTestlist/'\n",
    "        self.frames_per_clip = 8\n",
    "        self.step_between_clips = 8\n",
    "        self.batch_size = 8\n",
    "        self.num_workers = 1\n",
    "\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "source": [
    "## transform\n",
    "\n",
    "- UCF101を読み込むとuint8なので，255で割ってfloatにする．\n",
    "- torchvisionのUCF101データセットは(T, H, W, C)の形式．しかしpytorchvideoのx3dの入力形式は(B, C, T, H, W)らしいので，それに合わせる．\n",
    "\n",
    "- 画像を224x224にリサイズする．torchvision.transforms.Resizeはshapeが`[..., H, W]`ならOKなので，画像だけでなく動画もOK．Cropなども同様．\n",
    "https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/transforms/transforms.py#L227\n",
    "\n",
    "```text\n",
    "    If the image is torch Tensor, it is expected\n",
    "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x / 255.),\n",
    "\n",
    "    # (T, H, W, C) --> (C, T, H, W)\n",
    "    transforms.Lambda(lambda x: x.permute(3, 0, 1, 2)),\n",
    "\n",
    "    transforms.Resize(224),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットはimage, audio, labelの三組を返す．\n",
    "\n",
    "しかしUCF101には音声がない動画が約半数．\n",
    "- 音声あり: 6837\n",
    "- 音声なし: 6483\n",
    "\n",
    "そのためdatasetをそのまま使うと，dataloaderで「バッチにできない」というエラーが出てしまう（audioの次元数がサンプルによって異なるため）．\n",
    "\n",
    "そこでカスタムcollateでaudioを取り除く．取り除いたら`default_collate`に渡す．\n",
    "\n",
    "参考：https://www.kaggle.com/pevogam/starter-ucf101-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_audio_collate(batch):\n",
    "    '''\n",
    "    remove audio channel because\n",
    "    not all of UCF101 vidoes have audio channel\n",
    "    '''\n",
    "    video_only_batch = []\n",
    "    for video, audio, label in batch:\n",
    "        video_only_batch.append((video, label))\n",
    "    return default_collate(video_only_batch)\n",
    "\n",
    "custom_collate = remove_audio_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータの準備\n",
    "\n",
    "torchvision.datasets.UCF101は，全動画をスキャンして，FPSなどのメタデータ情報を取得するらしい．\n",
    "これにかなり時間がかかる（4-5分）．\n",
    "何もしないと，毎回スキャンするため，時間の無駄になる．\n",
    "そこで，メタデータを保存して再利用することにする．\n",
    "\n",
    "## 方針\n",
    "\n",
    "コードを見たところ，foldやtrainには無関係で，fpcとsbcにだけ依存するらしい．\n",
    "\n",
    "- https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/datasets/ucf101.py#L60\n",
    "\n",
    "```python\n",
    "        video_clips = VideoClips(\n",
    "            video_list,\n",
    "            frames_per_clip,\n",
    "            step_between_clips,\n",
    "            frame_rate,\n",
    "            _precomputed_metadata,\n",
    "            num_workers=num_workers,\n",
    "            _video_width=_video_width,\n",
    "            _video_height=_video_height,\n",
    "            _video_min_dimension=_video_min_dimension,\n",
    "            _audio_samples=_audio_samples,\n",
    "        )\n",
    "```\n",
    "\n",
    "上の`VideoClip`が呼び出されたら，`_precomputed_metadata`がNoneならメタデータを作成して，\n",
    "`.metadata`属性に保存している．\n",
    "\n",
    "そこで，fpcとsbcををファイル名につけて保存する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filename = os.path.join(\n",
    "    args.metadata_path,\n",
    "    'UCF101metadata_fpc{}_sbc{}.pickle'.format(\n",
    "        args.frames_per_clip,\n",
    "        args.step_between_clips))\n",
    "\n",
    "if not os.path.exists(metadata_filename):\n",
    "    # if no metadata, precompute and save metadata\n",
    "    dataset_dict = UCF101(\n",
    "        root=args.root,\n",
    "        annotation_path=args.annotation_path,\n",
    "        frames_per_clip=args.frames_per_clip,\n",
    "        step_between_clips=args.step_between_clips,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "    # now metadata is stored in dataset_dict.metadata\n",
    "\n",
    "    with open(metadata_filename, \"wb\") as f:\n",
    "        pickle.dump(dataset_dict.metadata, f)\n",
    "\n",
    "with open(metadata_filename, 'rb') as f:\n",
    "    metadata = pickle.load(f)\n"
   ]
  },
  {
   "source": [
    "## メタデータの注意点\n",
    "\n",
    "metadataはdict形式で，キー`video_paths`に動画ファイルへのパスを保持している．\n",
    "そのため動画ファイルのパスが変わったら，metadataを作成し直す（もしくは`video_paths`の中身を書き換える）必要がある．\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "type(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['video_paths', 'video_pts', 'video_fps'])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/dataset/UCF101/video/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi',\n",
       " '/dataset/UCF101/video/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi',\n",
       " '/dataset/UCF101/video/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "metadata['video_paths'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "          15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "          29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "          43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "          57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "          71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "          85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "          99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "         113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "         127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "         141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
       "         155, 156, 157, 158, 159, 160, 161, 162, 163, 164]),\n",
       " tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "          15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "          29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "          43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "          57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "          71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "          85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "          99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "         113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]),\n",
       " tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "          15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "          29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "          43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "          57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "          71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "          85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "          99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "         113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "         127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "         141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
       "         155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "         169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "         183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "         197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
       "         211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
       "         225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
       "         239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
       "         253, 254, 255, 256, 257, 258, 259])]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "metadata['video_pts'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[25.0, 25.0, 25.0]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "metadata['video_fps'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF101データセットオブジェクトの作成\n",
    "\n",
    "- frames_per_clip：1クリップに使うフレーム数\n",
    "- step_between_clips：次のクリップを開始するフレーム間隔．指定しなければframes_per_clipと同じ，つまりclip間でフレームの重複はない．frames_per_clipsよりも小さくすると，clip間でフレームが重複する（同じフレームが複数のclipで使われる）．frames_per_clipよりも大きくする\n",
    "- fold：UCF101には3つのスプリットがあるので，1, 2, 3のいずれかを指定する．（論文用の最終結果には，3つのスプリットの平均が報告される）\n",
    "- _precomputed_metadata：先ほど読み込んだmetadataオブジェクトを指定（ファイル名ではないので，あらかじめファイルから読み込んでおく必要がある）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = UCF101(\n",
    "    root=args.root,\n",
    "    annotation_path=args.annotation_path,\n",
    "    frames_per_clip=args.frames_per_clip,\n",
    "    step_between_clips=args.step_between_clips,\n",
    "    fold=1,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    _precomputed_metadata=metadata)\n",
    "\n",
    "val_set = UCF101(\n",
    "    root=args.root,\n",
    "    annotation_path=args.annotation_path,\n",
    "    frames_per_clip=args.frames_per_clip,\n",
    "    step_between_clips=args.step_between_clips,\n",
    "    fold=1,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    _precomputed_metadata=metadata)\n",
    "\n",
    "n_classes = 101\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データローダーの作成．カスタムcollateをここで指定．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=custom_collate,\n",
    "    num_workers=args.num_workers)\n",
    "    \n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    collate_fn=custom_collate,\n",
    "    num_workers=args.num_workers)\n"
   ]
  },
  {
   "source": [
    "# dataloaderの挙動\n",
    "\n",
    "バッチを取り出して挙動を確認．\n",
    "- train_loaderでは`shuffle=True`にしているので，ランダムなラベルが得られている．\n",
    "- val_loaderでは`shuffle=False`なので，最初はラベル0のclipばかり\n",
    "\n",
    "\n",
    "## エラーが発生する場合\n",
    "\n",
    "frames_per_clipを16に設定しているのに17枚ある，というエラーが発生する場合がある．\n",
    "\n",
    "```\n",
    "Original Traceback (most recent call last):\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n",
    "data = fetcher.fetch(index)\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
    "data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in \n",
    "data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torchvision/datasets/ucf101.py\", line 102, in getitem\n",
    "video, audio, info, video_idx = self.video_clips.get_clip(idx)\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torchvision/datasets/video_utils.py\", line 382, in get_clip\n",
    "assert len(video) == self.num_frames, \"{} x {}\".format(\n",
    "AssertionError: torch.Size([17, 224, 224, 3]) x 8\n",
    "```\n",
    "\n",
    "これはtorchvisionのバグのようなので，バージョンを変えてみたほうがよい．\n",
    "- 発生しない：torch==1.8.1+cpu torchvision==0.9.1+cpu\n",
    "- 発生する：torch==1.9.0+cpu torchvision==0.10.0+cpu\n",
    "\n",
    "0.9.1ではtorchvision.io.videoのワーニングが出るが，0.10.0では出ないので，ptsからsecに変更された影響かもしれない\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
    "```\n",
    "\n",
    "参考情報：\n",
    "- UCF101: Dataloader Fail on assertion #4112 https://github.com/pytorch/vision/issues/4112\n",
    "- VideoClips Assertion Error #1884 https://github.com/pytorch/vision/issues/1884\n",
    "- VideoClips Assertion Error https://gitmemory.com/issue/pytorch/vision/1884/595211331\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_loader\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\n",
      "batch 0: [41 36  2 45 31 20 69 92]\n",
      "batch 1: [ 55  88  46  75  48  38 100  33]\n",
      "batch 2: [44 54 35 35  1 95 61 34]\n",
      "batch 3: [68 10 92 77 33 60 17 45]\n",
      "batch 4: [54 50 68 75 24 62 43 68]\n",
      "batch 5: [14 49 70 26 43 64 60 59]\n",
      "batch 6: [46 83 49 71  6 22 82 47]\n",
      "val_loader\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\n",
      "batch 0: [0 0 0 0 0 0 0 0]\n",
      "batch 1: [0 0 0 0 0 0 0 0]\n",
      "batch 2: [0 0 0 0 0 0 0 0]\n",
      "batch 3: [0 0 0 0 0 0 0 0]\n",
      "batch 4: [0 0 0 0 0 0 0 0]\n",
      "batch 5: [0 0 0 0 0 0 0 0]\n",
      "batch 6: [0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # torchvisionのvideo.pyで，ワーニングが多数出るのでそれを抑制するなら\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "#                                    module='torchvision')\n",
    "\n",
    "print('train_loader')\n",
    "for i, (data, label) in enumerate(train_loader):\n",
    "    print('batch {}:'.format(i), label.cpu().numpy())\n",
    "    if i > 5:\n",
    "        break\n",
    "\n",
    "print('val_loader')\n",
    "for i, (data, label) in enumerate(val_loader):\n",
    "    print('batch {}:'.format(i), label.cpu().numpy())\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "source": [
    "データローダーのlenを確認する．\n",
    "\n",
    "- 学習用ビデオ数は9000程度のはずなのに，train_setのlengthは非常に多い\n",
    "  - おそらく，各ビデオからサンプリングしたclip数になっている\n",
    "      - 例えば，16フレームのclipを，16フレーム毎にサンプルすると，160フレームの動画からは10個のclipがサンプリングされる\n",
    "  - 各ビデオから同じ数のクリップがサンプルされているとは限らない（確認できていないが，おそらくそう）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train set\nnum batches 27389\nnum samples 219119\nnum samples / batch_size =  27389.875\n"
     ]
    }
   ],
   "source": [
    "print('train set')\n",
    "print('num batches', len(train_loader))\n",
    "print('num samples', len(train_set))\n",
    "print('num samples / batch_size = ', len(train_set) / args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val set\nnum batches 10690\nnum samples 85526\nnum samples / batch_size =  10690.75\n"
     ]
    }
   ],
   "source": [
    "print('val set')\n",
    "print('num batches', len(val_loader))\n",
    "print('num samples', len(val_set))\n",
    "print('num samples / batch_size = ', len(val_set) / args.batch_size)"
   ]
  },
  {
   "source": [
    "# 学習ループ\n",
    "\n",
    "学習ループを想定して，dataloaderからバッチを取り出す"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  0\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\n",
      "batch  0\n",
      "batch size 8\n",
      "input shape torch.Size([8, 3, 8, 224, 298])\n",
      "target shape torch.Size([8])\n",
      "batch  1\n",
      "batch size 8\n",
      "input shape torch.Size([8, 3, 8, 224, 298])\n",
      "target shape torch.Size([8])\n",
      "batch  2\n",
      "batch size 8\n",
      "input shape torch.Size([8, 3, 8, 224, 298])\n",
      "target shape torch.Size([8])\n",
      "batch  3\n",
      "batch size 8\n",
      "input shape torch.Size([8, 3, 8, 224, 298])\n",
      "target shape torch.Size([8])\n",
      "Epoch  1\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\n",
      "batch  0\n",
      "batch size 8\n",
      "input shape torch.Size([8, 3, 8, 224, 298])\n",
      "target shape torch.Size([8])\n",
      "batch  1\n",
      "batch size 8\n",
      "input shape torch.Size([8, 3, 8, 224, 298])\n",
      "target shape torch.Size([8])\n",
      "batch  2\n",
      "batch size 8\n",
      "input shape torch.Size([8, 3, 8, 224, 298])\n",
      "target shape torch.Size([8])\n",
      "batch  3\n",
      "batch size 8\n",
      "input shape torch.Size([8, 3, 8, 224, 298])\n",
      "target shape torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch \", epoch)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # current batch size may vary at the end of the epoch\n",
    "        #  if drop_last=False for data loaders\n",
    "        bs = inputs.size(0)\n",
    "\n",
    "        print('batch ', batch_idx)\n",
    "        print('batch size', bs)\n",
    "        print('input shape', inputs.shape)\n",
    "        print('target shape', targets.shape)\n",
    "\n",
    "        if batch_idx > 2:\n",
    "            break  # stop for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}