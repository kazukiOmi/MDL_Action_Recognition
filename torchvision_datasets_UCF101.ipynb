{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvision.datasets.UFC101の使い方\n",
    "\n",
    "torchvisionのdatasetを使ってUFC101を読み込む．\n",
    "\n",
    "- https://pytorch.org/vision/stable/datasets.html?highlight=ucf101#torchvision.datasets.UCF101\n",
    "\n",
    "\n",
    "# データセットの準備\n",
    "\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．\n",
    "- `/dataset/UCF101/video/`以下に，101クラスのサブディレクトリがある\n",
    "- `/dataset/UCF101/ucfTrainTestlist/`以下に，UCF101のアノテーションファイルであるtrainlist0{1,2,3}.txtなどがある\n",
    "\n",
    "## フォルダ構成\n",
    "\n",
    "```bash\n",
    "$ tree -I \"*.avi\" /dataset/UCF101\n",
    "/dataset/UCF101\n",
    "├── ucfTrainTestlist\n",
    "│   ├── classInd.txt\n",
    "│   ├── testlist01.txt\n",
    "│   ├── testlist02.txt\n",
    "│   ├── testlist03.txt\n",
    "│   ├── trainlist01.txt\n",
    "│   ├── trainlist02.txt\n",
    "│   └── trainlist03.txt\n",
    "└── video\n",
    "    ├── ApplyEyeMakeup\n",
    "    ├── ApplyLipstick\n",
    "    ├── Archery\n",
    "    ├── BabyCrawling\n",
    "    ├── BalanceBeam\n",
    "...\n",
    "    ├── Typing\n",
    "    ├── UnevenBars\n",
    "    ├── VolleyballSpiking\n",
    "    ├── WalkingWithDog\n",
    "    ├── WallPushups\n",
    "    ├── WritingOnBoard\n",
    "    └── YoYo\n",
    "\n",
    "$ head -5 /dataset/UCF101/ucfTrainTestlist/*\n",
    "==> /dataset/UCF101/ucfTrainTestlist/classInd.txt <==\n",
    "1 ApplyEyeMakeup\n",
    "2 ApplyLipstick\n",
    "3 Archery\n",
    "4 BabyCrawling\n",
    "5 BalanceBeam\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/testlist01.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/testlist02.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/testlist03.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c01.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c02.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c03.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c04.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c05.avi\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/trainlist01.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/trainlist02.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi 1\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/trainlist03.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import UCF101\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "こうしておくとすぐ本物のargpaseに移行できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.data_root = '/dataset/UCF101/'\n",
    "        self.metadata_path = self.data_root\n",
    "        self.video_path = os.path.join(\n",
    "            self.data_root, 'video')\n",
    "        self.annotation_path = os.path.join(\n",
    "            self.data_root, 'ucfTrainTestlist')\n",
    "\n",
    "        self.frames_per_clip = 16\n",
    "        self.step_between_clips = 16\n",
    "\n",
    "        self.batch_size = 8\n",
    "        self.num_workers = 1\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "source": [
    "# transform\n",
    "\n",
    "- 読み込んだフレームの型はuint8なので，255で割ってfloatにする．\n",
    "- 読み込んだフレームのshapeは(T, H, W, C)だが，一般的な入力形式のshapeは(B, C, T, H, W)らしいので，最終的にはそれに合わせる．\n",
    "   - まず(T,H,W,C)を(T,C,H,W)に変換\n",
    "   - 次にNormalize\n",
    "       - `Expected tensor to be a tensor image of size (..., C, H, W)`\n",
    "         https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/transforms/functional.py#L319\n",
    "   - 最後に(T,C,H,W)から(C,T,H,W)へ変換\n",
    "- 短い方を256画素程度に合わせてから，画像を224x224にリサイズする（action認識の一般的なやり方）\n",
    "  - フレームの短い辺を256にリサイズする．\n",
    "      - `If size is an int, smaller edge of the image will be matched to this number.`\n",
    "        https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/transforms/transforms.py#L240\n",
    "  - 最後に224x224に切り出す\n",
    "\n",
    "なおtorchvision.transformsのResizeやCropは，対象とするshapeが`[..., H, W]`ならOKなので，画像だけでなく動画もOK．\n",
    " - `If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions`\n",
    "   https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/transforms/transforms.py#L227\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/pytorchvideo/blob/e236b0bcaf81dcfb78b7a34bf234028eb3b85f21/pytorchvideo/transforms/transforms.py#L153 を参考\n",
    "\n",
    "class NormalizeFrames(torchvision.transforms.Normalize):\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)  # (T,H,W,C) --> (T,C,H,W)\n",
    "        x = super().forward(x)\n",
    "        x = x.permute(1, 0, 2, 3)  # (T,C,H,W) --> (C,T,H,W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x / 255.),\n",
    "    NormalizeFrames((0.45, 0.45, 0.45),\n",
    "                    (0.225, 0.225, 0.225)),\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x / 255.),\n",
    "    NormalizeFrames((0.45, 0.45, 0.45),\n",
    "                    (0.225, 0.225, 0.225)),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットはimage, audio, labelの三組を返す．\n",
    "\n",
    "しかしUCF101には音声がない動画が約半数．\n",
    "- 音声あり: 6837\n",
    "- 音声なし: 6483\n",
    "\n",
    "そのためdatasetをそのまま使うと，dataloaderで「バッチにできない」というエラーが出てしまう（audioの次元数がサンプルによって異なるため）．\n",
    "\n",
    "そこでカスタムcollateでaudioを取り除く．取り除いたら`default_collate`に渡す．\n",
    "\n",
    "参考：https://www.kaggle.com/pevogam/starter-ucf101-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_audio_collate(batch):\n",
    "    '''\n",
    "    remove audio channel because\n",
    "    not all of UCF101 vidoes have audio channel\n",
    "    '''\n",
    "    video_only_batch = []\n",
    "    for video, audio, label in batch:\n",
    "        video_only_batch.append((video, label))\n",
    "    return default_collate(video_only_batch)\n",
    "\n",
    "custom_collate = remove_audio_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータの準備\n",
    "\n",
    "torchvision.datasets.UCF101は，全動画をスキャンして，FPSなどのメタデータ情報を取得するらしい．\n",
    "これにかなり時間がかかる（4-5分）．\n",
    "何もしないと，毎回スキャンするため，時間の無駄になる．\n",
    "そこで，メタデータを保存して再利用することにする．\n",
    "\n",
    "## 方針\n",
    "\n",
    "コードを見たところ，foldやtrainには無関係で，fpcとsbcにだけ依存するらしい．\n",
    "\n",
    "- https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/datasets/ucf101.py#L60\n",
    "\n",
    "```python\n",
    "        video_clips = VideoClips(\n",
    "            video_list,\n",
    "            frames_per_clip,\n",
    "            step_between_clips,\n",
    "            frame_rate,\n",
    "            _precomputed_metadata,\n",
    "            num_workers=num_workers,\n",
    "            _video_width=_video_width,\n",
    "            _video_height=_video_height,\n",
    "            _video_min_dimension=_video_min_dimension,\n",
    "            _audio_samples=_audio_samples,\n",
    "        )\n",
    "```\n",
    "\n",
    "上の`VideoClip`が呼び出されたら，`_precomputed_metadata`がNoneならメタデータを作成して，\n",
    "`.metadata`属性に保存している．\n",
    "\n",
    "そこで，fpcとsbcををファイル名につけて保存する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filename = os.path.join(\n",
    "    args.metadata_path,\n",
    "    'UCF101metadata_fpc{}_sbc{}.pickle'.format(\n",
    "        args.frames_per_clip,\n",
    "        args.step_between_clips))\n",
    "\n",
    "if not os.path.exists(metadata_filename):\n",
    "    # if no metadata, precompute and save metadata\n",
    "    dataset_dict = UCF101(\n",
    "        root=args.root,\n",
    "        annotation_path=args.annotation_path,\n",
    "        frames_per_clip=args.frames_per_clip,\n",
    "        step_between_clips=args.step_between_clips,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "    # now metadata is stored in dataset_dict.metadata\n",
    "\n",
    "    with open(metadata_filename, \"wb\") as f:\n",
    "        pickle.dump(dataset_dict.metadata, f)\n",
    "\n",
    "with open(metadata_filename, 'rb') as f:\n",
    "    metadata = pickle.load(f)\n"
   ]
  },
  {
   "source": [
    "## メタデータの注意点\n",
    "\n",
    "metadataはdict形式で，キー`video_paths`に動画ファイルへのパスを保持している．\n",
    "そのため動画ファイルのパスが変わったら，metadataを作成し直す（もしくは`video_paths`の中身を書き換える）必要がある．\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "type(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['video_paths', 'video_pts', 'video_fps'])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/dataset/UCF101/video/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi',\n",
       " '/dataset/UCF101/video/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "metadata['video_paths'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "          15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "          29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "          43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "          57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "          71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "          85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "          99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "         113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "         127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "         141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
       "         155, 156, 157, 158, 159, 160, 161, 162, 163, 164]),\n",
       " tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "          15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "          29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "          43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "          57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "          71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "          85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "          99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "         113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123])]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "metadata['video_pts'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[25.0, 25.0]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "metadata['video_fps'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF101データセットオブジェクトの作成\n",
    "\n",
    "- frames_per_clip：1クリップに使うフレーム数\n",
    "- step_between_clips：次のクリップを開始するフレーム間隔．指定しなければframes_per_clipと同じ，つまりclip間でフレームの重複はない．frames_per_clipsよりも小さくすると，clip間でフレームが重複する（同じフレームが複数のclipで使われる）．frames_per_clipよりも大きくする\n",
    "- fold：UCF101には3つのスプリットがあるので，1, 2, 3のいずれかを指定する．（論文用の最終結果には，3つのスプリットの平均が報告される）\n",
    "- _precomputed_metadata：先ほど読み込んだmetadataオブジェクトを指定（ファイル名ではないので，あらかじめファイルから読み込んでおく必要がある）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = UCF101(\n",
    "    root=args.video_path,\n",
    "    annotation_path=args.annotation_path,\n",
    "    frames_per_clip=args.frames_per_clip,\n",
    "    step_between_clips=args.step_between_clips,\n",
    "    fold=1,\n",
    "    train=True,\n",
    "    transform=train_transform,\n",
    "    _precomputed_metadata=metadata)\n",
    "\n",
    "val_set = UCF101(\n",
    "    root=args.video_path,\n",
    "    annotation_path=args.annotation_path,\n",
    "    frames_per_clip=args.frames_per_clip,\n",
    "    step_between_clips=args.step_between_clips,\n",
    "    fold=1,\n",
    "    train=False,\n",
    "    transform=val_transform,\n",
    "    _precomputed_metadata=metadata)\n",
    "\n",
    "n_classes = 101\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データローダーの作成．カスタムcollateをここで指定．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=custom_collate,\n",
    "    num_workers=args.num_workers)\n",
    "    \n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    collate_fn=custom_collate,\n",
    "    num_workers=args.num_workers)\n"
   ]
  },
  {
   "source": [
    "# dataloaderの挙動\n",
    "\n",
    "バッチを取り出して挙動を確認．\n",
    "- train_loaderでは`shuffle=True`にしているので，ランダムなラベルが得られている．\n",
    "- val_loaderでは`shuffle=False`なので，最初はラベル0のclipばかり\n",
    "\n",
    "\n",
    "## エラーが発生する場合\n",
    "\n",
    "frames_per_clipを16に設定しているのに17枚ある，というエラーが発生する場合がある．\n",
    "\n",
    "```\n",
    "Original Traceback (most recent call last):\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n",
    "data = fetcher.fetch(index)\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
    "data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in \n",
    "data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torchvision/datasets/ucf101.py\", line 102, in getitem\n",
    "video, audio, info, video_idx = self.video_clips.get_clip(idx)\n",
    "File \"/usr/local/lib/python3.9/dist-packages/torchvision/datasets/video_utils.py\", line 382, in get_clip\n",
    "assert len(video) == self.num_frames, \"{} x {}\".format(\n",
    "AssertionError: torch.Size([17, 224, 224, 3]) x 8\n",
    "```\n",
    "\n",
    "これはtorchvisionのバグのようなので，バージョンを変えてみたほうがよい．\n",
    "- 発生しない：torch==1.8.1+cpu torchvision==0.9.1+cpu\n",
    "- 発生する：torch==1.9.0+cpu torchvision==0.10.0+cpu\n",
    "\n",
    "0.9.1ではtorchvision.io.videoのワーニングが出るが，0.10.0では出ないので，ptsからsecに変更された影響かもしれない\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
    "```\n",
    "\n",
    "参考情報：\n",
    "- UCF101: Dataloader Fail on assertion #4112 https://github.com/pytorch/vision/issues/4112\n",
    "- VideoClips Assertion Error #1884 https://github.com/pytorch/vision/issues/1884\n",
    "- VideoClips Assertion Error https://gitmemory.com/issue/pytorch/vision/1884/595211331\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_loader\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\n",
      "batch 0: [94 26 73 73 73  3 62 88]\n",
      "batch 1: [68 97 18 88 17 20 66 73]\n",
      "batch 2: [73 24 82 64 94 70 33 12]\n",
      "batch 3: [42 63 16 33 11 25 24 65]\n",
      "batch 4: [82 76 51  3 40 37 39 15]\n",
      "batch 5: [ 7 17 60 17 91 71 47 26]\n",
      "batch 6: [43  3 73 39  7 93 92 39]\n",
      "val_loader\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\n",
      "batch 0: [0 0 0 0 0 0 0 0]\n",
      "batch 1: [0 0 0 0 0 0 0 0]\n",
      "batch 2: [0 0 0 0 0 0 0 0]\n",
      "batch 3: [0 0 0 0 0 0 0 0]\n",
      "batch 4: [0 0 0 0 0 0 0 0]\n",
      "batch 5: [0 0 0 0 0 0 0 0]\n",
      "batch 6: [0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # torchvisionのvideo.pyで，ワーニングが多数出るのでそれを抑制するなら\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "#                                    module='torchvision')\n",
    "\n",
    "print('train_loader')\n",
    "for i, (data, label) in enumerate(train_loader):\n",
    "    print('batch {}:'.format(i), label.cpu().numpy())\n",
    "    if i > 5:\n",
    "        break\n",
    "\n",
    "print('val_loader')\n",
    "for i, (data, label) in enumerate(val_loader):\n",
    "    print('batch {}:'.format(i), label.cpu().numpy())\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "source": [
    "データローダーのlenを確認する．\n",
    "\n",
    "- 学習用ビデオ数は9000程度のはずなのに，train_setのlengthは非常に多い\n",
    "  - おそらく，各ビデオからサンプリングしたclip数になっている\n",
    "      - 例えば，16フレームのclipを，16フレーム毎にサンプルすると，160フレームの動画からは10個のclipがサンプリングされる\n",
    "  - 各ビデオから同じ数のクリップがサンプルされているとは限らない（確認できていないが，おそらくそう）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train set\nnum batches 13385\nnum samples 107085\nnum samples / batch_size =  13385.625\n"
     ]
    }
   ],
   "source": [
    "print('train set')\n",
    "print('num batches', len(train_loader))\n",
    "print('num samples', len(train_set))\n",
    "print('num samples / batch_size = ', len(train_set) / args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val set\nnum batches 5221\nnum samples 41771\nnum samples / batch_size =  5221.375\n"
     ]
    }
   ],
   "source": [
    "print('val set')\n",
    "print('num batches', len(val_loader))\n",
    "print('num samples', len(val_set))\n",
    "print('num samples / batch_size = ', len(val_set) / args.batch_size)"
   ]
  },
  {
   "source": [
    "# 学習ループ\n",
    "\n",
    "学習ループを想定して，dataloaderからバッチを取り出す"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  0\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\n",
      "batch  0\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  1\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  2\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  3\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "Epoch  1\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/io/video.py:158: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\n",
      "batch  0\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  1\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  2\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  3\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch \", epoch)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        print('batch ', batch_idx)\n",
    "        print('current batch size', inputs.size(0))\n",
    "        print('input shape', inputs.shape)\n",
    "        print('target shape', targets.shape)\n",
    "\n",
    "        if batch_idx > 2:\n",
    "            break  # stop for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}