{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorchvideo.data.Ucf101の使い方\n",
    "\n",
    "pytorchvideo.dataを使ってUFC101を読み込む．\n",
    "\n",
    "- https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#ucf101\n",
    "\n",
    "\n",
    "\n",
    "# データセットの準備\n",
    "\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．\n",
    "- `/dataset/UCF101/video/`以下に，101クラスのサブディレクトリがある\n",
    "- `/dataset/UCF101/ucfTrainTestlist/`以下に，UCF101のアノテーションファイルであるtrainlist0{1,2,3}.txtなどがある\n",
    "\n",
    "## フォルダ構成\n",
    "\n",
    "```bash\n",
    "$ tree -I \"*.avi\" /dataset/UCF101\n",
    "/dataset/UCF101\n",
    "├── ucfTrainTestlist\n",
    "│   ├── classInd.txt\n",
    "│   ├── testlist01.txt\n",
    "│   ├── testlist02.txt\n",
    "│   ├── testlist03.txt\n",
    "│   ├── trainlist01.txt\n",
    "│   ├── trainlist02.txt\n",
    "│   └── trainlist03.txt\n",
    "└── video\n",
    "    ├── ApplyEyeMakeup\n",
    "    ├── ApplyLipstick\n",
    "    ├── Archery\n",
    "    ├── BabyCrawling\n",
    "    ├── BalanceBeam\n",
    "...\n",
    "    ├── Typing\n",
    "    ├── UnevenBars\n",
    "    ├── VolleyballSpiking\n",
    "    ├── WalkingWithDog\n",
    "    ├── WallPushups\n",
    "    ├── WritingOnBoard\n",
    "    └── YoYo\n",
    "\n",
    "$ head -5 /dataset/UCF101/ucfTrainTestlist/*\n",
    "==> /dataset/UCF101/ucfTrainTestlist/classInd.txt <==\n",
    "1 ApplyEyeMakeup\n",
    "2 ApplyLipstick\n",
    "3 Archery\n",
    "4 BabyCrawling\n",
    "5 BalanceBeam\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/testlist01.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/testlist02.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/testlist03.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c01.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c02.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c03.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c04.avi\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c05.avi\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/trainlist01.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/trainlist02.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi 1\n",
    "\n",
    "==> /dataset/UCF101/ucfTrainTestlist/trainlist03.txt <==\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "from pytorchvideo.data import Ucf101\n",
    "from pytorchvideo.data import RandomClipSampler, make_clip_sampler\n",
    "\n",
    "import pytorchvideo.transforms as videoTrans\n",
    "import torchvision.transforms as visionTrans\n",
    "\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "こうしておくとすぐ本物のargpaseに移行できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.data_root = '/dataset/UCF101/'\n",
    "        self.video_path_prefix = os.path.join(\n",
    "            self.data_root, 'video')\n",
    "        self.annotation_path = os.path.join(\n",
    "            self.data_root, 'ucfTrainTestlist')\n",
    "\n",
    "        self.clip_duration = 16 / 25  # 25FPSを想定して16枚分の時間（0.64秒）\n",
    "        self.frames_per_clip = 16\n",
    "\n",
    "        self.clips_per_video = 3  # only for ConstantClipsPerVideoSampler\n",
    "\n",
    "        self.batch_size = 8\n",
    "        self.num_workers = 1\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "source": [
    "# transform\n",
    "\n",
    "\n",
    "バッチはdict形式なので，video, label, audioなどのそれぞれにtransformが設定できる\n",
    "- ApplyTransformToKeyでkeyを指定して，video/label/audio用のtransformをそれぞれ設定\n",
    "\n",
    "\n",
    "参考：https://pytorchvideo.org/docs/tutorial_classification\n",
    "\n",
    "\n",
    "\n",
    "## video用のtransform\n",
    "\n",
    "pytorchvideo.transformとtorchvision.transformが混在するので，\n",
    "それぞれvideoTransとvisionTransという名前にしてimportしてある．\n",
    "videoTransには動画用のtransformある．\n",
    "画像にも動画にも使えるものはvisionTransを利用する．\n",
    "\n",
    "\n",
    "- UniformTemporalSubsampleで固定枚数をサンプルする\n",
    "    - datasetのclip_samplerには，秒単位でしか与えられないらしい．\n",
    "    - 枚数指定ではfpsが異なる動画ではサンプルされる枚数も変わってしまう．そこで，ここで取得するフレーム数を揃える\n",
    "    - UFC101のFPSは25が多いが30も存在する．\n",
    "        - 25.00 fps: 10810  \n",
    "        - 30 fps: 2510\n",
    "    - 25fpsを仮定して16フレームを1 clipとすると，0.64秒．もし30fspなら約19枚分．したがって，\n",
    "      - 25fpsの動画からは16フレームをサンプリングして，ここで16枚に間引く（つまり何もしない）\n",
    "      - 30fpsの動画からは19フレームをサンプリングして，ここで16枚に間引く（19枚からどうやって16枚を抜き出すのかは，まだソースを見ていないので不明）\n",
    "- 読み込んだフレーム型はfloat32だが値は0-255なので，255で割ってfloatにする．\n",
    "- 読み込んだフレームのshapeは(C, T, H, W)．これがpytorchvideoでは一般的．\n",
    "- Normalizeは画像フレーム単位で行うので，visionTransのものでも良さそうだが，shapeが問題．内部では以下のことをしている．\n",
    "    - shapeを(C, T, H, W)から(T, C, H, W)に変更\n",
    "    - vision.transform.Normalizeを適用\n",
    "        - `Expected tensor to be a tensor image of size (..., C, H, W)`\n",
    "          https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/transforms/functional.py#L320\n",
    "    - shapeを(T, C, H, W)から(C, T, H, W)に戻す\n",
    "    - https://github.com/facebookresearch/pytorchvideo/blob/e236b0bcaf81dcfb78b7a34bf234028eb3b85f21/pytorchvideo/transforms/transforms.py#L153\n",
    "- 短い方を256画素程度に合わせてから，画像を224x224にリサイズする（action認識の一般的なやり方）\n",
    "  - 学習時：RandomShortSideScaleなら厳密には256になっていないが（データ拡張のため），RandcomCropで224x224を切り出す\n",
    "  - テスト時：ShortSideScaleなら256になる．これをCenterCropで224x224を切り出す\n",
    "\n",
    "\n",
    "\n",
    "## label用のtransform\n",
    "\n",
    "UCF101の学習用ラベルファイル（trainlist01.txtなど）には1から101までのラベルが付いているが，それがそのまま使われてしまう（なぜだ．．．）．\n",
    "このままではindex範囲外というエラーが（不定期に）発生してしまうので，ここでラベルの値をtransformで0から100に修正する．\n",
    "\n",
    "テスト用ファイルラベルファイル（testlist01.txtなど）にはラベルは含まれていないので，これをする必要はない．\n",
    "\n",
    "## audio用のtransform\n",
    "\n",
    "audioは使わないのでRemoveKeyで除去する．\n",
    "\n",
    "UCF101には音声がない動画が約半数しかないため．\n",
    "- 音声あり: 6837\n",
    "- 音声なし: 6483\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = visionTrans.Compose([\n",
    "    videoTrans.ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=visionTrans.Compose([\n",
    "            videoTrans.UniformTemporalSubsample(\n",
    "                args.frames_per_clip),\n",
    "            visionTrans.Lambda(lambda x: x / 255.),\n",
    "            videoTrans.Normalize((0.45, 0.45, 0.45),\n",
    "                                 (0.225, 0.225, 0.225)),\n",
    "            videoTrans.RandomShortSideScale(\n",
    "                min_size=256, max_size=320,),\n",
    "            visionTrans.RandomCrop(224),\n",
    "            visionTrans.RandomHorizontalFlip(),\n",
    "        ]),\n",
    "    ),\n",
    "    videoTrans.ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いて0から100にする\n",
    "        transform=visionTrans.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    videoTrans.RemoveKey(\"audio\"),\n",
    "])\n",
    "\n",
    "val_transform = visionTrans.Compose([\n",
    "    videoTrans.ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=visionTrans.Compose([\n",
    "            videoTrans.UniformTemporalSubsample(\n",
    "                args.frames_per_clip),\n",
    "            visionTrans.Lambda(lambda x: x / 255.),\n",
    "            videoTrans.Normalize((0.45, 0.45, 0.45),\n",
    "                                 (0.225, 0.225, 0.225)),\n",
    "            videoTrans.ShortSideScale(256),\n",
    "            visionTrans.CenterCrop(224),\n",
    "        ]),\n",
    "    ),\n",
    "\n",
    "    videoTrans.RemoveKey(\"audio\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF101データセットオブジェクトの作成\n",
    "\n",
    "\n",
    "- clip_sampler=RandomClipSampler(clip_duration=args.clip_duration)：あるビデオ中のクリップのサンプリングをランダムに行う．\n",
    "    - 長さはclip_durationで指定．25fpsの動画から16フレームを抜き出すつもりなら，16/25=0.64にする（単位は秒）\n",
    "- video_sampler=RandomSampler：どのビデオを使うのかはランダムに選択\n",
    "- decode_audio=False：音声は使わない\n",
    "\n",
    "## データの指定方法\n",
    "\n",
    "- data_path：'ucfTrainTestlist/trainlist01.txt'などのアノテーションファイルを指定する．\n",
    "- video_path_prefix：アノテーションファイルで指定されている動画ファイル名の前につけるprefix．\n",
    "\n",
    "例：\n",
    "`trainlist01.txt`で指定されているのは\n",
    "```text\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
    "```\n",
    "という形式で，実際にはファイルが\n",
    "```\n",
    "/dataset/UCF101/video/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi\n",
    "```\n",
    "にあるなら，\n",
    "`/dataset/UCF101/video/`が`video_path_prefix`になる．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_sampler = RandomClipSampler(clip_duration=args.clip_duration)\n",
    "clip_sampler = make_clip_sampler(\"random\", args.clip_duration)\n",
    "# clip_sampler = make_clip_sampler(\"uniform\", args.clip_duration)\n",
    "# clip_sampler = make_clip_sampler(\"constant_clips_per_video\", args.clip_duration, args.clips_per_video)\n",
    "\n",
    "\n",
    "train_set = Ucf101(\n",
    "    data_path=os.path.join(args.annotation_path, 'trainlist01.txt'),\n",
    "    video_path_prefix=args.video_path_prefix,\n",
    "    clip_sampler=clip_sampler,\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    "    )\n",
    "\n",
    "val_set = Ucf101(\n",
    "    data_path=os.path.join(args.annotation_path, 'testlist01.txt'),\n",
    "    video_path_prefix=args.video_path_prefix,\n",
    "    clip_sampler=clip_sampler,\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    "    )\n",
    "\n",
    "num_classes = 101"
   ]
  },
  {
   "source": [
    "動画ファイルが壊れていてclipがサンプルできない場合でも，1エポックで同じstepになることを保証するために，以下のラッパーを利用する．"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/pytorchvideo/blob/ef2d3a96bb939b12aa0f21fb467d2175b0f05e9f/tutorials/video_classification_example/train.py#L343\n",
    "\n",
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    To ensure a constant number of samples are retrieved from the dataset we use this\n",
    "    LimitDataset wrapper. This is necessary because several of the underlying videos\n",
    "    may be corrupted while fetching or decoding, however, we always want the same\n",
    "    number of steps per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ]
  },
  {
   "source": [
    "このラッパーを使ってdataloaderオブジェクトを作成．"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LimitDataset(train_set)\n",
    "val_set = LimitDataset(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=args.batch_size,\n",
    "                          drop_last=True,\n",
    "                          num_workers=args.num_workers)\n",
    "val_loader = DataLoader(val_set,\n",
    "                        batch_size=args.batch_size,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.num_workers)\n"
   ]
  },
  {
   "source": [
    "# data loaderの挙動\n",
    "\n",
    "バッチを取り出して挙動を確認．\n",
    "- バッチはdictでやってくるので，`batch['video']`と`batch['label']`で取り出す\n",
    "- train_loaderでは，RandomClipSamplerならランダムなラベルが得られている．\n",
    "- val_loaderでは，ラベル情報はないのでラベルは0ばかり\n",
    "\n",
    "## clip samplerの影響\n",
    "\n",
    "3種類あるclip samplerの比較．\n",
    "- randomは各サンプルclipがランダム\n",
    "- uniformはある1本の動画から同じ長さのclipをサンプルし続ける（1本あたりのclip数は未定）\n",
    "- constant per clipは，ある1本の動画から，同数のclipをサンプルし続ける（1本あたりのclips数は固定，指定する）\n",
    "\n",
    "batchのキーの解釈\n",
    "- labelは各サンプルのラベル\n",
    "- video_indexは動画ファイルのID\n",
    "- clip_index\n",
    "  - random: 未使用（すべて0）\n",
    "  - uniform: 1エポック中で何番目のclipを表す番号？（何に使う？）\n",
    "    - [リセットしてないので](https://github.com/facebookresearch/pytorchvideo/blob/5c34ca13956425c63533923eebaf7c3b57acc71c/pytorchvideo/data/clip_sampling.py#L160)，エポックが変わってもインクリメントし続けるかも\n",
    "  - constant per clip: 各動画からサンプリングしたclipの番号（この場合は0, 1, 2）\n",
    "\n",
    "random\n",
    "```\n",
    "batch 0\n",
    "label      : [64 74 48 86  1 83 24 65]\n",
    "video_index: [6166 7099 4700 8248  155 7959 2390 6268]\n",
    "clip_index : [0 0 0 0 0 0 0 0]\n",
    "batch 1\n",
    "label      : [30 41 42 19 55 34 30 70]\n",
    "video_index: [2911 3975 4081 1919 5278 3345 2970 6786]\n",
    "clip_index : [0 0 0 0 0 0 0 0]\n",
    "batch 2\n",
    "label      : [74 70 42 58 51 32 24 60]\n",
    "video_index: [7087 6719 4112 5576 4934 3092 2392 5774]\n",
    "clip_index : [0 0 0 0 0 0 0 0]\n",
    "batch 3\n",
    "label      : [  5  53  37  43 100  27  95  23]\n",
    "video_index: [ 552 5153 3580 4191 9489 2651 9069 2309]\n",
    "clip_index : [0 0 0 0 0 0 0 0]\n",
    "```\n",
    "\n",
    "uniform\n",
    "```\n",
    "batch 0\n",
    "label      : [ 9  9  9  9  9 31 31 31]\n",
    "video_index: [ 950  950  950  950  950 2973 2973 2973]\n",
    "clip_index : [0 1 2 3 4 5 6 7]\n",
    "batch 1\n",
    "label      : [31 31 31 31 31 98 98 98]\n",
    "video_index: [2973 2973 2973 2973 2973 9332 9332 9332]\n",
    "clip_index : [ 8  9 10 11 12 13 14 15]\n",
    "batch 2\n",
    "label      : [98 98 98 98 98 98 98 98]\n",
    "video_index: [9332 9332 9332 9332 9332 9332 9332 9332]\n",
    "clip_index : [16 17 18 19 20 21 22 23]\n",
    "batch 3\n",
    "label      : [44 44 44 44 44 44  2  2]\n",
    "video_index: [4262 4262 4262 4262 4262 4262  241  241]\n",
    "clip_index : [24 25 26 27 28 29 30 31]\n",
    "```\n",
    "\n",
    "constant per video\n",
    "```\n",
    "batch 0\n",
    "label      : [49 49 49 71 71 71 24 24]\n",
    "video_index: [4808 4808 4808 6875 6875 6875 2385 2385]\n",
    "clip_index : [0 1 2 0 1 2 0 1]\n",
    "batch 1\n",
    "label      : [24 12 12 12 70 70 70 86]\n",
    "video_index: [2385 1211 1211 1211 6735 6735 6735 8219]\n",
    "clip_index : [2 0 1 2 0 1 2 0]\n",
    "batch 2\n",
    "label      : [86 86 76 76 76 72 72 72]\n",
    "video_index: [8219 8219 7306 7306 7306 6909 6909 6909]\n",
    "clip_index : [1 2 0 1 2 0 1 2]\n",
    "batch 3\n",
    "label      : [52 52 52 80 80 80 38 38]\n",
    "video_index: [5027 5027 5027 7710 7710 7710 3662 3662]\n",
    "clip_index : [0 1 2 0 1 2 0 1]\n",
    "```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train\n",
      "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])\n",
      "batch shape torch.Size([8, 3, 16, 224, 224])\n",
      "batch 0\n",
      "label      : [59 16 41 83 96 19 66 56]\n",
      "video_index: [5675 1608 3982 7993 9114 1872 6403 5395]\n",
      "clip_index : [0 0 0 0 0 0 0 0]\n",
      "batch 1\n",
      "label      : [ 2 20 56 72 81 12 88 33]\n",
      "video_index: [ 284 1969 5350 6904 7757 1241 8383 3245]\n",
      "clip_index : [0 0 0 0 0 0 0 0]\n",
      "batch 2\n",
      "label      : [57 59 24 67  2 94 41 63]\n",
      "video_index: [5437 5667 2392 6513  200 8937 3972 6085]\n",
      "clip_index : [0 0 0 0 0 0 0 0]\n",
      "batch 3\n",
      "label      : [80 21 89 38 65 75 75 94]\n",
      "video_index: [7689 2066 8518 3687 6274 7181 7248 8924]\n",
      "clip_index : [0 0 0 0 0 0 0 0]\n",
      "val\n",
      "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])\n",
      "batch shape torch.Size([8, 3, 16, 224, 224])\n",
      "batch 0\n",
      "label      : [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "video_index: [3505 2081 1728 3105  781 1506 3428 1065]\n",
      "clip_index : [0 0 0 0 0 0 0 0]\n",
      "batch 1\n",
      "label      : [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "video_index: [ 284  252 3773 2603 1966 2388  374 1201]\n",
      "clip_index : [0 0 0 0 0 0 0 0]\n",
      "batch 2\n",
      "label      : [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "video_index: [2872 1765 3537  865  556 2987  210 2799]\n",
      "clip_index : [0 0 0 0 0 0 0 0]\n",
      "batch 3\n",
      "label      : [-1 -1 -1 -1 -1 -1 -1 -1]\n",
      "video_index: [1426 2804  796 1835 1153   27 2199 1847]\n",
      "clip_index : [0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(batch.keys())\n",
    "        print('batch shape', batch['video'].shape)\n",
    "    print('batch', i)\n",
    "    print('label      :', batch['label'].cpu().numpy())\n",
    "    print('video_index:', batch['video_index'].cpu().numpy())\n",
    "    print('clip_index :', batch['clip_index'].cpu().numpy())\n",
    "    if i > 2:\n",
    "        break\n",
    "\n",
    "print('val')\n",
    "for i, batch in enumerate(val_loader):\n",
    "    if i == 0:\n",
    "        print(batch.keys())\n",
    "        print('batch shape', batch['video'].shape)\n",
    "    print('batch', i)\n",
    "    print('label      :', batch['label'].cpu().numpy())\n",
    "    print('video_index:', batch['video_index'].cpu().numpy())\n",
    "    print('clip_index :', batch['clip_index'].cpu().numpy())\n",
    "    if i > 2:\n",
    "        break\n"
   ]
  },
  {
   "source": [
    "データローダーのlenを確認する．\n",
    "\n",
    "- 各ビデオから一つのクリップがサンプルされている\n",
    "  - RandomClipSamplerならランダムにクリップをサンプリング"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train set\nnum batches 1192\nnum samples 9537\nnum samples / batch_size =  1192.125\n"
     ]
    }
   ],
   "source": [
    "print('train set')\n",
    "print('num batches', len(train_loader))\n",
    "print('num samples', len(train_set))\n",
    "print('num samples / batch_size = ', len(train_set) / args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val set\nnum batches 472\nnum samples 3783\nnum samples / batch_size =  472.875\n"
     ]
    }
   ],
   "source": [
    "print('val set')\n",
    "print('num batches', len(val_loader))\n",
    "print('num samples', len(val_set))\n",
    "print('num samples / batch_size = ', len(val_set) / args.batch_size)"
   ]
  },
  {
   "source": [
    "# 学習ループ\n",
    "\n",
    "学習ループを想定して，dataloaderからバッチを取り出す"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  0\n",
      "batch  0\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  1\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  2\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  3\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "Epoch  1\n",
      "batch  0\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  1\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  2\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n",
      "batch  3\n",
      "current batch size 8\n",
      "input shape torch.Size([8, 3, 16, 224, 224])\n",
      "target shape torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch \", epoch)\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "        inputs = batch['video'].to(device)\n",
    "        targets = batch['label'].to(device)\n",
    "\n",
    "        print('batch ', batch_idx)\n",
    "        print('current batch size', inputs.size(0))\n",
    "        print('input shape', inputs.shape)\n",
    "        print('target shape', targets.shape)\n",
    "\n",
    "        if batch_idx > 2:\n",
    "            break  # stop for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}