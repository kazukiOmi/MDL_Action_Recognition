{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorchvideo UFC101, pytorchvideo slowfast pretrain/scratch\n",
    "\n",
    "pytorchvideonのdatasetを使ってUFC101を読み込み，pytorchvideoのslowfastモデルをfine-tuningしてみる．\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．\n",
    "\n",
    "- https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#ucf101\n",
    "\n",
    "- https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/\n",
    "\n"
   ]
  },
  {
   "source": [
    "## ダウンロードできないというエラー\n",
    "\n",
    "torchvisionをimportした後ではエラーが発生する（ImportError: cannot import name ***）\n",
    "\n",
    "- https://github.com/pytorch/hub/issues/46\n",
    "\n",
    "\n",
    "## 対応策\n",
    "\n",
    "import torch直後に（import torchvisionをしない状態で）torch.hub.loadして，キャッシュに残しておく\n",
    "\n",
    "こうすると，以降はキャッシュ（~/.cache/torch/hub/checkpoints/）が使われるのでエラーは発生しない"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/tamaki/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import Ucf101, RandomClipSampler, UniformClipSampler\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "- rootで指定したディレクトリには，101クラスのサブディレクトリがあること\n",
    "- annotation_pathにはtrainlist0{1,2,3}.txtなどがあること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/HDD10TB/dataset/UFC101/'\n",
    "        self.root = self.metadata_path + 'video/'\n",
    "        self.annotation_path = self.metadata_path + 'ucfTrainTestlist/'\n",
    "        self.frames_per_clip = 16\n",
    "        self.step_between_clips = 16\n",
    "        self.model = 'slowfast_r50'\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 24\n",
    "\n",
    "        self.clip_duration = 16/25  # 25FPSを想定して16枚\n",
    "        self.video_num_subsampled = 32  # 32枚抜き出す\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "source": [
    "transformの定義．\n",
    "- UniformTemporalSubsampleで固定枚数をサンプルする\n",
    " - datasetのclip_samplerには，秒単位でしか与えられないようなので，fpsが異なる動画ではサンプルされる枚数も変わってくる．そのためここで取得するフレーム数を揃える（もっといい方法はないのか？）\n",
    "- UCF101を読み込むとfloat32だが値は0-255，255で割ってfloatにする．\n",
    "- slowfastを想定して，短い方を256画素程度に合わせてから，画像を256x256にリサイズする．\n",
    "  - RandomShortSideScaleなら厳密には256にならない\n",
    "  - ShortSideScaleなら256になる\n",
    "\n",
    "バッチはdict形式なので，video, label, audioなどのそれぞれにtransformが設定できる\n",
    "- ApplyTransformToKeyでkeyを指定して，video用のtransformを設定\n",
    "- UCF101のラベルファイル（trainlist01.txtなど）には1から101までのラベルが付いているが，それがそのまま使われてしまうので（なぜだ．．．），このままではエラーが（不定期に）発生する．ラベルの値をtransformでから100にしておく\n",
    "- audioは使わないのでRemoveKeyで除去\n",
    "\n",
    "slowfastは，データをfast pathとして受け取って，それを時間方向にダウンサンプリングしたものをslow pathとして受け取る．\n",
    "そのためにtransformでslow pathを作って，fast/slow のリストに変換する．\n",
    "（slowfastのpretrainモデルは，このリストをデータとして受け取る）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.56\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 25  # UCFは25と30が混在\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "print(clip_duration)\n",
    "side_size = 256\n",
    "\n",
    "crop_size = 256\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors. \n",
    "    https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.slowfast_alpha = 4\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // self.slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "\n",
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ## 以下デバッグ用\n",
    "                # transforms.Lambda(lambda x: [\n",
    "                #     x, \n",
    "                #     print(type(x)),\n",
    "                #     print(x.dtype),\n",
    "                #     print(x.max()),\n",
    "                #     print(x.min()),\n",
    "                #     print(x.mean()),\n",
    "                #     ]),\n",
    "                # transforms.Lambda(lambda x: x[0]),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(256),\n",
    "                RandomHorizontalFlip(),\n",
    "                PackPathway(),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(256),\n",
    "                CenterCrop(256),\n",
    "                PackPathway(),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_UCF101 = '/mnt/HDD10TB/dataset/UFC101/'\n",
    "\n",
    "train_set = Ucf101(\n",
    "    data_path=root_UCF101 + 'ucfTrainTestlist/trainlist01.txt',  # ラベルが1から101になっているので，transformで1を引いている\n",
    "    video_path_prefix=root_UCF101 + 'video/',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    "    )\n",
    "val_set = Ucf101(\n",
    "    data_path=root_UCF101 + 'ucfTrainTestlist/testlist01.txt',\n",
    "    video_path_prefix=root_UCF101 + 'video/',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    "    )\n",
    "\n",
    "num_classes = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9537"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_set.num_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3783"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "val_set.num_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/pytorchvideo/blob/ef2d3a96bb939b12aa0f21fb467d2175b0f05e9f/tutorials/video_classification_example/train.py#L343\n",
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    To ensure a constant number of samples are retrieved from the dataset we use this\n",
    "    LimitDataset wrapper. This is necessary because several of the underlying videos\n",
    "    may be corrupted while fetching or decoding, however, we always want the same\n",
    "    number of steps per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(LimitDataset(train_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n",
    "val_loader = DataLoader(LimitDataset(val_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n"
   ]
  },
  {
   "source": [
    "データローダのlenを確認．\n",
    "- trainlist01.txtには9537行あるので「サンプル数＝ビデオ数」\n",
    "- バッチサイズで割るとtrain_loaderのlengthになる"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(596, 9537, 596.0625)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(train_loader), train_set.num_videos, train_set.num_videos / args.batch_size"
   ]
  },
  {
   "source": [
    "data loaderの挙動を確認．\n",
    "- バッチはdictでやってくるので，`batch['video']`と`batch['label']`で取り出す\n",
    "- RandomClipSamplerならランダムなラベルが得られている．"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])\n",
      "torch.Size([16, 3, 8, 256, 256]) torch.Size([16, 3, 32, 256, 256])\n",
      "[25  3 24 85 84 85 23 54 11 43 15 36 12 88  4 15]\n",
      "[ 69  48  61  52  58  69  33  63  24  35   2 100  34  89  39  10]\n",
      "[38 59  6 33  9 69 31 14 17  7 85 79 29 42 53 47]\n",
      "[98 77 84 41 79 38 22 84 96 81 17 51 42 34 38 91]\n",
      "[84 58 60 24 53 40 20  1 10 57 26 53 52 88 34 78]\n",
      "[84 88 12 19 75  5 91 70 41 47 74 81 96 25 10 29]\n",
      "[17 99  2 64 70  0 69 65 39 48 70 99 37 17  6 93]\n",
      "[85  7 38  1 33 53 16 85 33 70 87 10 21 91 94 24]\n",
      "[87 50 59 16 44 91 91 17 62 11 36  2 47 21 86 67]\n",
      "[ 76  87  33   4  99  23  87 100  34  66  16  76  44  84 100  48]\n",
      "[58 10 70 87 67 27 37 20 53 92 19 73 18 60 13 29]\n",
      "[ 1 72 67 74  8 38 54 85 11 15 49 80 37 77 30 86]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(batch.keys())\n",
    "        print(batch['video'][0].shape, batch['video'][1].shape)\n",
    "    print(batch['label'].cpu().numpy())\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchvideoのpretrained x3dモデルをダウンロード．\n",
    "あとでsummaryを見れば分かるように，最終線形層は`model.blocks[6].proj`だからこれをnn.Linearに置き換える\n",
    "\n",
    "- 注意：エラーが発生してダウンロードできない場合には，このnotebookの冒頭の注意書きを確認すること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/tamaki/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "\n",
    "# fine-tuningするなら以下を実行．スクラッチで学習するなら，実行しない\n",
    "do_fine_tune = True\n",
    "if do_fine_tune:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.blocks[6].proj = nn.Linear(model.blocks[6].proj.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# data parallelだと性能が落ちる？\n",
    "# model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムなデータを流し込んで出力されるかを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.1486,  0.1301,  0.0599, -0.0393, -0.2027,  0.2747, -0.2415, -0.0947,\n",
       "         -0.0705, -0.1716,  0.2407,  0.1671, -0.0529,  0.1921, -0.1121, -0.0988,\n",
       "         -0.1914, -0.0197,  0.0376, -0.0521,  0.1427,  0.0643,  0.0063, -0.0729,\n",
       "          0.0035,  0.1370, -0.4055, -0.1332, -0.2426, -0.1637, -0.0376, -0.0058,\n",
       "         -0.0630, -0.1669, -0.0143, -0.3453,  0.2857,  0.0489, -0.3149, -0.2889,\n",
       "          0.0682,  0.0368,  0.0135,  0.3442,  0.0246, -0.1581,  0.2129,  0.2479,\n",
       "         -0.0076, -0.3534, -0.1072, -0.1361,  0.2280,  0.1509,  0.0779,  0.1605,\n",
       "         -0.1879, -0.0268,  0.2102,  0.0739, -0.2548, -0.0615, -0.0194,  0.1608,\n",
       "         -0.1656,  0.0521, -0.1733,  0.1493, -0.0814, -0.1257,  0.1124, -0.1677,\n",
       "         -0.1633, -0.1285, -0.1074,  0.1314, -0.1146, -0.1517,  0.2993, -0.1422,\n",
       "         -0.0342, -0.3682,  0.0826, -0.0454,  0.0404, -0.1555, -0.0079, -0.0033,\n",
       "         -0.0425,  0.2829,  0.1455,  0.0994,  0.1811, -0.1340, -0.0189, -0.2275,\n",
       "          0.0391,  0.1711, -0.0544,  0.0506,  0.0151],\n",
       "        [ 0.0607,  0.1124,  0.1140, -0.0594, -0.0767,  0.3837, -0.3293, -0.1003,\n",
       "          0.0935, -0.2431,  0.1869,  0.0339,  0.2026,  0.0356, -0.1516, -0.1479,\n",
       "         -0.1044,  0.0416, -0.0324, -0.1836,  0.1544, -0.0191,  0.0190, -0.1700,\n",
       "         -0.0433,  0.1239, -0.3824, -0.1480, -0.0641, -0.1298,  0.0550,  0.0170,\n",
       "          0.0983, -0.0921,  0.0430, -0.5005,  0.2471,  0.0242, -0.1552, -0.2566,\n",
       "          0.2459,  0.0264, -0.3680,  0.3467,  0.0039,  0.1028,  0.1984,  0.2644,\n",
       "         -0.0641, -0.4379,  0.0783, -0.1094,  0.0881,  0.0656, -0.0348,  0.2590,\n",
       "         -0.1856,  0.0935,  0.1729, -0.0306, -0.2505,  0.2387, -0.0007,  0.1188,\n",
       "         -0.2963,  0.0964,  0.0806,  0.1821, -0.1724, -0.1948,  0.2049, -0.0785,\n",
       "         -0.0504, -0.1324, -0.0597,  0.0783, -0.0293, -0.1449,  0.2099, -0.1502,\n",
       "          0.1655, -0.0942,  0.1837,  0.0237,  0.1139,  0.0260, -0.0525,  0.0302,\n",
       "         -0.0916,  0.1538, -0.0332,  0.1560,  0.2428, -0.0273,  0.0483,  0.0294,\n",
       "          0.0143,  0.0362,  0.0375,  0.0455,  0.1642]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "data1 = torch.randn(2, 3, 8, 256, 256).to(device)\n",
    "data2 = torch.randn(2, 3, 32, 256, 256).to(device)\n",
    "model([data1, data2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summaryでinput/outputのサイズを確認できない．\n",
    "sloffastは2つのtensorをリストで受け取るため．（なにか方法はあるか？）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type (var_name))                                      Kernel Shape              Param #\n",
       "==============================================================================================================\n",
       "ModuleList                                                   --                        --\n",
       "├─MultiPathWayWithFuse (0)                                   --                        --\n",
       "│    └─ModuleList (multipathway_blocks)                      --                        --\n",
       "│    │    └─ResNetBasicStem (0)                              --                        --\n",
       "│    │    │    └─Conv3d (conv)                               [3, 64, 1, 7, 7]          (9,408)\n",
       "│    │    │    └─BatchNorm3d (norm)                          [64]                      (128)\n",
       "│    │    │    └─ReLU (activation)                           --                        --\n",
       "│    │    │    └─MaxPool3d (pool)                            --                        --\n",
       "│    │    └─ResNetBasicStem (1)                              --                        --\n",
       "│    │    │    └─Conv3d (conv)                               [3, 8, 5, 7, 7]           (5,880)\n",
       "│    │    │    └─BatchNorm3d (norm)                          [8]                       (16)\n",
       "│    │    │    └─ReLU (activation)                           --                        --\n",
       "│    │    │    └─MaxPool3d (pool)                            --                        --\n",
       "│    └─FuseFastToSlow (multipathway_fusion)                  --                        --\n",
       "│    │    └─Conv3d (conv_fast_to_slow)                       [8, 16, 7, 1, 1]          (896)\n",
       "│    │    └─BatchNorm3d (norm)                               [16]                      (32)\n",
       "│    │    └─ReLU (activation)                                --                        --\n",
       "├─MultiPathWayWithFuse (1)                                   --                        --\n",
       "│    └─ModuleList (multipathway_blocks)                      --                        --\n",
       "│    │    └─ResStage (0)                                     --                        --\n",
       "│    │    │    └─ModuleList (res_blocks)                     --                        (220,928)\n",
       "│    │    └─ResStage (1)                                     --                        --\n",
       "│    │    │    └─ModuleList (res_blocks)                     --                        (4,832)\n",
       "│    └─FuseFastToSlow (multipathway_fusion)                  --                        --\n",
       "│    │    └─Conv3d (conv_fast_to_slow)                       [32, 64, 7, 1, 1]         (14,336)\n",
       "│    │    └─BatchNorm3d (norm)                               [64]                      (128)\n",
       "│    │    └─ReLU (activation)                                --                        --\n",
       "├─MultiPathWayWithFuse (2)                                   --                        --\n",
       "│    └─ModuleList (multipathway_blocks)                      --                        --\n",
       "│    │    └─ResStage (0)                                     --                        --\n",
       "│    │    │    └─ModuleList (res_blocks)                     --                        (1,260,544)\n",
       "│    │    └─ResStage (1)                                     --                        --\n",
       "│    │    │    └─ModuleList (res_blocks)                     --                        (27,008)\n",
       "│    └─FuseFastToSlow (multipathway_fusion)                  --                        --\n",
       "│    │    └─Conv3d (conv_fast_to_slow)                       [64, 128, 7, 1, 1]        (57,344)\n",
       "│    │    └─BatchNorm3d (norm)                               [128]                     (256)\n",
       "│    │    └─ReLU (activation)                                --                        --\n",
       "├─MultiPathWayWithFuse (3)                                   --                        --\n",
       "│    └─ModuleList (multipathway_blocks)                      --                        --\n",
       "│    │    └─ResStage (0)                                     --                        --\n",
       "│    │    │    └─ModuleList (res_blocks)                     --                        (10,211,328)\n",
       "│    │    └─ResStage (1)                                     --                        --\n",
       "│    │    │    └─ModuleList (res_blocks)                     --                        (158,208)\n",
       "│    └─FuseFastToSlow (multipathway_fusion)                  --                        --\n",
       "│    │    └─Conv3d (conv_fast_to_slow)                       [128, 256, 7, 1, 1]       (229,376)\n",
       "│    │    └─BatchNorm3d (norm)                               [256]                     (512)\n",
       "│    │    └─ReLU (activation)                                --                        --\n",
       "├─MultiPathWayWithFuse (4)                                   --                        --\n",
       "│    └─ModuleList (multipathway_blocks)                      --                        --\n",
       "│    │    └─ResStage (0)                                     --                        --\n",
       "│    │    │    └─ModuleList (res_blocks)                     --                        (21,125,120)\n",
       "│    │    └─ResStage (1)                                     --                        --\n",
       "│    │    │    └─ModuleList (res_blocks)                     --                        (318,208)\n",
       "│    └─Identity (multipathway_fusion)                        --                        --\n",
       "├─PoolConcatPathway (5)                                      --                        --\n",
       "│    └─ModuleList (pool)                                     --                        --\n",
       "│    │    └─AvgPool3d (0)                                    --                        --\n",
       "│    │    └─AvgPool3d (1)                                    --                        --\n",
       "├─ResNetBasicHead (6)                                        --                        --\n",
       "│    └─Dropout (dropout)                                     --                        --\n",
       "│    └─Linear (proj)                                         [2304, 101]               232,805\n",
       "│    └─AdaptiveAvgPool3d (output_pool)                       --                        --\n",
       "==============================================================================================================\n",
       "Total params: 33,877,293\n",
       "Trainable params: 232,805\n",
       "Non-trainable params: 33,644,488\n",
       "=============================================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "\n",
    "torchinfo.summary(\n",
    "    model.blocks if model.__class__.__name__ != 'DataParallel' else model.module.blocks,\n",
    "    depth=4,\n",
    "    # col_names=[\"input_size\",\n",
    "    #            \"output_size\"],\n",
    "    col_names=[\"kernel_size\", \n",
    "               \"num_params\"],\n",
    "    row_settings=(\"var_names\",)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "便利関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4b5bb99e3bf44419afd43a0429f337c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da932380958a4a96be36d095d54ffdc9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f2036badfb55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mpbar_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[train]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(train_loader),\n",
    "                  total=len(train_loader),\n",
    "                  leave=True) as pbar_loss:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, batch in pbar_loss:\n",
    "                pbar_loss.set_description(\"[train]\")\n",
    "\n",
    "                targets = batch['label'].to(device)\n",
    "                inputs = [b.to(device) for b in batch['video']]\n",
    "                bs = inputs[0].size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, targets), bs)\n",
    "\n",
    "                pbar_loss.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                    train_loss.val, train_acc.val,\n",
    "                ))\n",
    "\n"
   ]
  },
  {
   "source": [
    "fine-tuningなのでまあ速い．\n",
    "- 4GPUでおよそ2.5it/s，1エポック約4分\n",
    "- 1GPUでおよそ1.5it/s，1エポック約6分（596 iterations）\n",
    "\n",
    "\n",
    "以下の設定\n",
    "- video_num_subsampled = 32  # 32枚抜き出す      \n",
    "- batch_size = 16\n",
    "- num_workers = 24"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}