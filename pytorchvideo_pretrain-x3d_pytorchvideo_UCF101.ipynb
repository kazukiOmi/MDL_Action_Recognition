{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# pytorchvideo UFC101, pytorchvideo X3D pretrain/scratch\n",
    "\n",
    "pytorchvideonのdatasetを使ってUFC101を読み込み，pytorchvideoのx3dモデルをfine-tuningしてみる．\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．\n",
    "\n",
    "- https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#ucf101\n",
    "\n",
    "- https://pytorch.org/hub/facebookresearch_pytorchvideo_x3d/\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ダウンロードできないというエラー\n",
    "\n",
    "torchvisionをimportした後ではエラーが発生する（ImportError: cannot import name ***）\n",
    "\n",
    "- https://github.com/pytorch/hub/issues/46\n",
    "\n",
    "\n",
    "## 対応策\n",
    "\n",
    "import torch直後に（import torchvisionをしない状態で）torch.hub.loadして，キャッシュに残しておく\n",
    "\n",
    "こうすると，以降はキャッシュ（~/.cache/torch/hub/checkpoints/）が使われるのでエラーは発生しない"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_xs', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import Ucf101, RandomClipSampler, UniformClipSampler\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "\n",
    "#import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "- rootで指定したディレクトリには，101クラスのサブディレクトリがあること\n",
    "- annotation_pathにはtrainlist0{1,2,3}.txtなどがあること"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/NAS-TVS872XT/dataset/UFC101/'\n",
    "        self.root = self.metadata_path + 'video/'\n",
    "        self.annotation_path = self.metadata_path + 'ucfTrainTestlist/'\n",
    "        self.frames_per_clip = 16\n",
    "        self.step_between_clips = 16\n",
    "        self.model = 'x3d_m'\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 24\n",
    "\n",
    "        self.clip_duration = 16/25  # 25FPSを想定して16枚\n",
    "        self.video_num_subsampled = 16  # 16枚抜き出す\n",
    "\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "transformの定義．\n",
    "- UniformTemporalSubsampleで固定枚数をサンプルする\n",
    " - datasetのclip_samplerには，秒単位でしか与えられないようなので，fpsが異なる動画ではサンプルされる枚数も変わってくる．そのためここで取得するフレーム数を揃える（もっといい方法はないのか？）\n",
    "- UCF101を読み込むとfloat32だが値は0-255，255で割ってfloatにする．\n",
    "- X3D-Mを想定して，短い方を256画素程度に合わせてから，画像を224x224にリサイズする．\n",
    "  - RandomShortSideScaleなら厳密には256にならない\n",
    "  - ShortSideScaleなら256になる\n",
    "\n",
    "バッチはdict形式なので，video, label, audioなどのそれぞれにtransformが設定できる\n",
    "- ApplyTransformToKeyでkeyを指定して，video用のtransformを設定\n",
    "- UCF101のラベルファイル（trainlist01.txtなど）には1から101までのラベルが付いているが，それがそのまま使われてしまうので（なぜだ．．．），このままではエラーが（不定期に）発生する．ラベルの値をtransformでから100にしておく\n",
    "- audioは使わないのでRemoveKeyで除去"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ## 以下デバッグ用\n",
    "                # transforms.Lambda(lambda x: [\n",
    "                #     x, \n",
    "                #     print(type(x)),\n",
    "                #     print(x.dtype),\n",
    "                #     print(x.max()),\n",
    "                #     print(x.min()),\n",
    "                #     print(x.mean()),\n",
    "                #     ]),\n",
    "                # transforms.Lambda(lambda x: x[0]),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(256),\n",
    "                CenterCrop(224),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "root_UCF101 = '/mnt/NAS-TVS872XT/dataset/UCF101/'\n",
    "\n",
    "train_set = Ucf101(\n",
    "    data_path=root_UCF101 + 'ucfTrainTestlist/trainlist01.txt',  # ラベルが1から101になっているので，transformで1を引いている\n",
    "    video_path_prefix=root_UCF101 + 'video/',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=args.clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    "    )\n",
    "val_set = Ucf101(\n",
    "    data_path=root_UCF101 + 'ucfTrainTestlist/testlist01.txt',\n",
    "    video_path_prefix=root_UCF101 + 'video/',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=args.clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    "    )\n",
    "\n",
    "num_classes = 101"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_set.num_videos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9537"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "val_set.num_videos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3783"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# https://github.com/facebookresearch/pytorchvideo/blob/ef2d3a96bb939b12aa0f21fb467d2175b0f05e9f/tutorials/video_classification_example/train.py#L343\n",
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    To ensure a constant number of samples are retrieved from the dataset we use this\n",
    "    LimitDataset wrapper. This is necessary because several of the underlying videos\n",
    "    may be corrupted while fetching or decoding, however, we always want the same\n",
    "    number of steps per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "train_loader = DataLoader(LimitDataset(train_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n",
    "val_loader = DataLoader(LimitDataset(val_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "データローダのlenを確認．\n",
    "- trainlist01.txtには9537行あるので「サンプル数＝ビデオ数」\n",
    "- バッチサイズで割るとtrain_loaderのlengthになる"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "len(train_loader), train_set.num_videos, train_set.num_videos / args.batch_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(596, 9537, 596.0625)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "data loaderの挙動を確認．\n",
    "- バッチはdictでやってくるので，`batch['video']`と`batch['label']`で取り出す\n",
    "- RandomClipSamplerならランダムなラベルが得られている．"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(batch.keys())\n",
    "        print(batch['video'].shape)\n",
    "    print(batch['label'].cpu().numpy())\n",
    "    if i > 10:\n",
    "        break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])\n",
      "torch.Size([16, 3, 16, 224, 224])\n",
      "[16 23  8 12 52 59 43 69 96  7 34 83 12 90 61 93]\n",
      "[56 32 39 73 36 94  5  1 67  9 81  4 52 73 32 32]\n",
      "[ 14  57  49  66  11   3  63  99  51  48  98 100  77  35  62 100]\n",
      "[32 65 91 21 87 40 67 48 48 45 97 51 79 15 76  0]\n",
      "[24 67 52 60  5 11 46 23 23 37 39 59 19 59 73 33]\n",
      "[19 77 28 84 46 26 91 35 31 68 60 96 28  8 11 61]\n",
      "[60 98 65 58 82 74 72 61 47 77 56 78 11 11 29 67]\n",
      "[71 56 70 27 37 64 75 66 73 21 74 30  2 67 16 24]\n",
      "[30 89 90 71 26 67 27 90 69 80 18 23 56 53 10 48]\n",
      "[60  9 70 50 56 32 23 61 77 28 31  9 58 35 67 90]\n",
      "[33 20 38 30 68 56 52 60  6 95 95  0 42 29 89 53]\n",
      "[82 89 72 22  6 45  6 92 66  5 99 14 64 37 21 94]\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "pytorchvideoのpretrained x3dモデルをダウンロード．\n",
    "あとでsummaryを見れば分かるように，最終線形層は`model.blocks[5].proj`だからこれをnn.Linearに置き換える\n",
    "\n",
    "- 注意：エラーが発生してダウンロードできない場合には，このnotebookの冒頭の注意書きを確認すること"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# # X3D-M\n",
    "# # https://github.com/facebookresearch/pytorchvideo/blob/master/pytorchvideo/models/x3d.py#L601\n",
    "# model = x3d.create_x3d(\n",
    "#     input_clip_length=16,\n",
    "#     input_crop_size=224,\n",
    "#     depth_factor=2.2,\n",
    "#     model_num_class=101\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)\n",
    "\n",
    "# fine-tuningするなら以下を実行．スクラッチで学習するなら，実行しない\n",
    "do_fine_tune = True\n",
    "if do_fine_tune:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.blocks[5].proj = nn.Linear(model.blocks[5].proj.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# data parallelだと性能が落ちる（設定次第？）\n",
    "# model = nn.DataParallel(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ランダムなデータを流し込んで出力されるかを確認する"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data = torch.randn(2, 3, 16, 224, 224).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0108, 0.0107, 0.0102, 0.0087, 0.0105, 0.0091, 0.0093, 0.0095, 0.0092,\n",
       "         0.0104, 0.0097, 0.0099, 0.0098, 0.0088, 0.0094, 0.0097, 0.0087, 0.0108,\n",
       "         0.0105, 0.0103, 0.0094, 0.0111, 0.0100, 0.0106, 0.0089, 0.0093, 0.0096,\n",
       "         0.0107, 0.0105, 0.0101, 0.0096, 0.0104, 0.0101, 0.0110, 0.0092, 0.0100,\n",
       "         0.0094, 0.0097, 0.0112, 0.0095, 0.0092, 0.0105, 0.0099, 0.0095, 0.0101,\n",
       "         0.0088, 0.0097, 0.0099, 0.0111, 0.0103, 0.0103, 0.0103, 0.0104, 0.0099,\n",
       "         0.0105, 0.0099, 0.0091, 0.0097, 0.0096, 0.0103, 0.0100, 0.0098, 0.0109,\n",
       "         0.0106, 0.0096, 0.0094, 0.0098, 0.0105, 0.0088, 0.0094, 0.0100, 0.0101,\n",
       "         0.0102, 0.0109, 0.0104, 0.0094, 0.0103, 0.0099, 0.0091, 0.0104, 0.0107,\n",
       "         0.0093, 0.0107, 0.0097, 0.0091, 0.0089, 0.0099, 0.0100, 0.0101, 0.0105,\n",
       "         0.0093, 0.0106, 0.0099, 0.0089, 0.0094, 0.0097, 0.0103, 0.0100, 0.0091,\n",
       "         0.0093, 0.0100],\n",
       "        [0.0102, 0.0102, 0.0100, 0.0105, 0.0096, 0.0100, 0.0096, 0.0088, 0.0081,\n",
       "         0.0114, 0.0107, 0.0107, 0.0101, 0.0096, 0.0104, 0.0102, 0.0101, 0.0101,\n",
       "         0.0090, 0.0103, 0.0095, 0.0090, 0.0095, 0.0093, 0.0089, 0.0105, 0.0095,\n",
       "         0.0095, 0.0090, 0.0088, 0.0094, 0.0098, 0.0092, 0.0099, 0.0090, 0.0106,\n",
       "         0.0093, 0.0103, 0.0109, 0.0106, 0.0093, 0.0107, 0.0102, 0.0089, 0.0106,\n",
       "         0.0097, 0.0085, 0.0124, 0.0088, 0.0094, 0.0097, 0.0096, 0.0084, 0.0097,\n",
       "         0.0103, 0.0095, 0.0091, 0.0112, 0.0092, 0.0094, 0.0100, 0.0114, 0.0090,\n",
       "         0.0101, 0.0097, 0.0099, 0.0091, 0.0108, 0.0090, 0.0096, 0.0103, 0.0117,\n",
       "         0.0096, 0.0094, 0.0093, 0.0105, 0.0088, 0.0105, 0.0100, 0.0098, 0.0110,\n",
       "         0.0101, 0.0101, 0.0091, 0.0096, 0.0109, 0.0105, 0.0104, 0.0088, 0.0106,\n",
       "         0.0093, 0.0106, 0.0124, 0.0107, 0.0104, 0.0102, 0.0097, 0.0096, 0.0102,\n",
       "         0.0098, 0.0102]], device='cuda:2', grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "summaryで中身を確認"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# torchinfo.summary(\n",
    "#     model,\n",
    "#     (4, 3, 16, 224, 224),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "便利関数を定義"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "hyper_params = {\n",
    "    \"num_classes\": 101,\n",
    "    \"batch_size\": args.batch_size,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 0.1\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# !pip install comet_ml"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: comet_ml in /home/omi/.local/lib/python3.8/site-packages (3.16.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/omi/.local/lib/python3.8/site-packages (from comet_ml) (0.9.1)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /opt/conda/lib/python3.8/site-packages (from comet_ml) (1.12.1)\n",
      "Requirement already satisfied: websocket-client>=0.55.0 in /opt/conda/lib/python3.8/site-packages (from comet_ml) (1.2.1)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from comet_ml) (3.0.2)\n",
      "Requirement already satisfied: dulwich>=0.20.6 in /home/omi/.local/lib/python3.8/site-packages (from comet_ml) (0.20.25)\n",
      "Requirement already satisfied: everett[ini]>=1.0.1 in /home/omi/.local/lib/python3.8/site-packages (from comet_ml) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.8/site-packages (from comet_ml) (2.24.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /home/omi/.local/lib/python3.8/site-packages (from comet_ml) (3.0.2)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /home/omi/.local/lib/python3.8/site-packages (from comet_ml) (7.352.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from comet_ml) (1.15.0)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in /home/omi/.local/lib/python3.8/site-packages (from comet_ml) (2.8.5)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in /opt/conda/lib/python3.8/site-packages (from dulwich>=0.20.6->comet_ml) (1.25.11)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from dulwich>=0.20.6->comet_ml) (2020.12.5)\n",
      "Requirement already satisfied: configobj in /home/omi/.local/lib/python3.8/site-packages (from everett[ini]>=1.0.1->comet_ml) (5.0.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (50.3.1.post20201107)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (20.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.18.4->comet_ml) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment(api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "                        project_name=\"test0_comet\", workspace=\"kazukiomi\")\n",
    "experiment.add_tag(\"pytorch\")\n",
    "experiment.log_parameters(hyper_params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/kazukiomi/test-comet/e0e2ec4690a245f890eaa834d46af7c3\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size    : 16\n",
      "COMET INFO:     learning_rate : 0.1\n",
      "COMET INFO:     num_classes   : 101\n",
      "COMET INFO:     num_epochs    : 5\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (127.52 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET ERROR: Error sending a notification, make sure you have opted-in for notifications\n",
      "COMET ERROR: Failed to report experiment status\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/kazukiomi/test0-comet/e96c29e3e1324514b67abda90499f213\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(train_loader),\n",
    "                  total=len(train_loader),\n",
    "                  leave=True) as pbar_loss:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, batch in pbar_loss:\n",
    "                pbar_loss.set_description(\"[train]\")\n",
    "\n",
    "                inputs, targets = batch['video'].to(device), batch['label'].to(device)\n",
    "                bs = inputs.size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, targets), bs)\n",
    "\n",
    "                experiment.log_metric(\"batch_acc\", train_acc.val, step=batch_idx)\n",
    "\n",
    "                pbar_loss.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                    train_loss.val, train_acc.val,\n",
    "                ))\n",
    "            \n",
    "            experiment.log_metric(\"epoch_acc\", train_acc.avg, step=epoch)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9d166ebea8e4bb2b11a5de369229e74"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e59568ad93d415eb59b7aef961e809f"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f17bdf635a3b496493571204a4f657fd"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afd309953d95490d990e6a30dcf03261"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc9f1433c36c4592a40cefc0a6e89301"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6bd69fd684b48e0b69282bcb429d77b"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "fine-tuningなので速い．\n",
    "- 4GPUでおよそ4.5it/s，1エポック約2分\n",
    "- 1GPUでおよそ5it/s，1エポック約3分（596 iterations）\n",
    "\n",
    "スクラッチで学習するなら\n",
    "- 4GPUでおよそ2.6it/s，1エポック約4分\n",
    "- 1GPUでおよそ1.8it/s，1エポック約5.5分（596 iterations）\n",
    "\n",
    "\n",
    "以下の設定\n",
    "- video_num_subsampled = 16\n",
    "- batch_size = 16\n",
    "- num_workers = 24"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import os\n",
    "os.cpu_count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}