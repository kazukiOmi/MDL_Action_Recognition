{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorchvideo UFC101, pytorchvideo X3D pretrain/scratch\n",
    "\n",
    "pytorchvideonのdatasetを使ってUFC101を読み込み，pytorchvideoのx3dモデルをfine-tuningしてみる．\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．\n",
    "\n",
    "- https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#ucf101\n",
    "\n",
    "- https://pytorch.org/hub/facebookresearch_pytorchvideo_x3d/\n",
    "\n"
   ]
  },
  {
   "source": [
    "## ダウンロードできないというエラー\n",
    "\n",
    "torchvisionをimportした後ではエラーが発生する（ImportError: cannot import name ***）\n",
    "\n",
    "- https://github.com/pytorch/hub/issues/46\n",
    "\n",
    "\n",
    "## 対応策\n",
    "\n",
    "import torch直後に（import torchvisionをしない状態で）torch.hub.loadして，キャッシュに残しておく\n",
    "\n",
    "こうすると，以降はキャッシュ（~/.cache/torch/hub/checkpoints/）が使われるのでエラーは発生しない"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/tamaki/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/tamaki/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/tamaki/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_xs', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import Ucf101, RandomClipSampler, UniformClipSampler\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "- rootで指定したディレクトリには，101クラスのサブディレクトリがあること\n",
    "- annotation_pathにはtrainlist0{1,2,3}.txtなどがあること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/HDD10TB/dataset/UFC101/'\n",
    "        self.root = self.metadata_path + 'video/'\n",
    "        self.annotation_path = self.metadata_path + 'ucfTrainTestlist/'\n",
    "        self.frames_per_clip = 16\n",
    "        self.step_between_clips = 16\n",
    "        self.model = 'x3d_m'\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 24\n",
    "\n",
    "        self.clip_duration = 16/25  # 25FPSを想定して16枚\n",
    "        self.video_num_subsampled = 16  # 16枚抜き出す\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "source": [
    "transformの定義．\n",
    "- UniformTemporalSubsampleで固定枚数をサンプルする\n",
    " - datasetのclip_samplerには，秒単位でしか与えられないようなので，fpsが異なる動画ではサンプルされる枚数も変わってくる．そのためここで取得するフレーム数を揃える（もっといい方法はないのか？）\n",
    "- UCF101を読み込むとfloat32だが値は0-255，255で割ってfloatにする．\n",
    "- X3D-Mを想定して，短い方を256画素程度に合わせてから，画像を224x224にリサイズする．\n",
    "  - RandomShortSideScaleなら厳密には256にならない\n",
    "  - ShortSideScaleなら256になる\n",
    "\n",
    "バッチはdict形式なので，video, label, audioなどのそれぞれにtransformが設定できる\n",
    "- ApplyTransformToKeyでkeyを指定して，video用のtransformを設定\n",
    "- UCF101のラベルファイル（trainlist01.txtなど）には1から101までのラベルが付いているが，それがそのまま使われてしまうので（なぜだ．．．），このままではエラーが（不定期に）発生する．ラベルの値をtransformでから100にしておく\n",
    "- audioは使わないのでRemoveKeyで除去"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ## 以下デバッグ用\n",
    "                # transforms.Lambda(lambda x: [\n",
    "                #     x, \n",
    "                #     print(type(x)),\n",
    "                #     print(x.dtype),\n",
    "                #     print(x.max()),\n",
    "                #     print(x.min()),\n",
    "                #     print(x.mean()),\n",
    "                #     ]),\n",
    "                # transforms.Lambda(lambda x: x[0]),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(256),\n",
    "                CenterCrop(224),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_UCF101 = '/mnt/HDD10TB/dataset/UFC101/'\n",
    "\n",
    "train_set = Ucf101(\n",
    "    data_path=root_UCF101 + 'ucfTrainTestlist/trainlist01.txt',  # ラベルが1から101になっているので，transformで1を引いている\n",
    "    video_path_prefix=root_UCF101 + 'video/',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=args.clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    "    )\n",
    "val_set = Ucf101(\n",
    "    data_path=root_UCF101 + 'ucfTrainTestlist/testlist01.txt',\n",
    "    video_path_prefix=root_UCF101 + 'video/',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=args.clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    "    )\n",
    "\n",
    "num_classes = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9537"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_set.num_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3783"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "val_set.num_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/pytorchvideo/blob/ef2d3a96bb939b12aa0f21fb467d2175b0f05e9f/tutorials/video_classification_example/train.py#L343\n",
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    To ensure a constant number of samples are retrieved from the dataset we use this\n",
    "    LimitDataset wrapper. This is necessary because several of the underlying videos\n",
    "    may be corrupted while fetching or decoding, however, we always want the same\n",
    "    number of steps per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(LimitDataset(train_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n",
    "val_loader = DataLoader(LimitDataset(val_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n"
   ]
  },
  {
   "source": [
    "データローダのlenを確認．\n",
    "- trainlist01.txtには9537行あるので「サンプル数＝ビデオ数」\n",
    "- バッチサイズで割るとtrain_loaderのlengthになる"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(596, 9537, 596.0625)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(train_loader), train_set.num_videos, train_set.num_videos / args.batch_size"
   ]
  },
  {
   "source": [
    "data loaderの挙動を確認．\n",
    "- バッチはdictでやってくるので，`batch['video']`と`batch['label']`で取り出す\n",
    "- RandomClipSamplerならランダムなラベルが得られている．"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])\n",
      "torch.Size([16, 3, 16, 224, 224])\n",
      "[64 75 41 46 66 47 32 40 24 79 55 48 39 16 19 23]\n",
      "[100  63  17  38  99  30  56  45   3  21  30  25  75  46  54  10]\n",
      "[33 50 10 60 30 11 99  3 47 86 48 13 79 29 76 62]\n",
      "[11 84 19 38 41 22 79 23 53 63 43 76 22 61 67 59]\n",
      "[40 37 85 42 46  5 34 13 51 68 69 81 70 69 71 33]\n",
      "[96 41 14 50  8  7 97 23 32 52 94 73 41 17 76 28]\n",
      "[88 72 94 45  0 39 43 33 80 66 66 85 10 39  5 29]\n",
      "[22 73  2 23 86  2 96 16 23 48  3 14 95  2  5 16]\n",
      "[92 42 68 39 23 21 21 11 37 84 27 24 39 58 53 21]\n",
      "[99 20 52 30 96 78 27 69 73 57 99 87 55 34 26 64]\n",
      "[14 10 15 85 96 61 91 97 76 31 33  7 23 74 39 73]\n",
      "[ 1 91 44 18 34 42 22 77 92  5 16  7 13 30 93  2]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(batch.keys())\n",
    "        print(batch['video'].shape)\n",
    "    print(batch['label'].cpu().numpy())\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchvideoのpretrained x3dモデルをダウンロード．\n",
    "あとでsummaryを見れば分かるように，最終線形層は`model.blocks[5].proj`だからこれをnn.Linearに置き換える\n",
    "\n",
    "- 注意：エラーが発生してダウンロードできない場合には，このnotebookの冒頭の注意書きを確認すること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/tamaki/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "source": [
    "# # X3D-M\n",
    "# # https://github.com/facebookresearch/pytorchvideo/blob/master/pytorchvideo/models/x3d.py#L601\n",
    "# model = x3d.create_x3d(\n",
    "#     input_clip_length=16,\n",
    "#     input_crop_size=224,\n",
    "#     depth_factor=2.2,\n",
    "#     model_num_class=101\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)\n",
    "\n",
    "# fine-tuningするなら以下を実行．スクラッチで学習するなら，実行しない\n",
    "do_fine_tune = True\n",
    "if do_fine_tune:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.blocks[5].proj = nn.Linear(model.blocks[5].proj.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# data parallelだと性能が落ちる（設定次第？）\n",
    "# model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムなデータを流し込んで出力されるかを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(2, 3, 16, 224, 224).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0094, 0.0092, 0.0107, 0.0096, 0.0099, 0.0104, 0.0101, 0.0095, 0.0100,\n",
       "         0.0090, 0.0109, 0.0083, 0.0091, 0.0091, 0.0098, 0.0092, 0.0096, 0.0101,\n",
       "         0.0108, 0.0110, 0.0100, 0.0098, 0.0106, 0.0099, 0.0093, 0.0100, 0.0105,\n",
       "         0.0094, 0.0099, 0.0108, 0.0111, 0.0093, 0.0094, 0.0099, 0.0099, 0.0106,\n",
       "         0.0105, 0.0082, 0.0103, 0.0108, 0.0111, 0.0100, 0.0099, 0.0094, 0.0097,\n",
       "         0.0101, 0.0101, 0.0106, 0.0102, 0.0105, 0.0092, 0.0101, 0.0106, 0.0106,\n",
       "         0.0114, 0.0102, 0.0089, 0.0092, 0.0089, 0.0101, 0.0097, 0.0102, 0.0096,\n",
       "         0.0096, 0.0103, 0.0099, 0.0095, 0.0101, 0.0105, 0.0087, 0.0100, 0.0095,\n",
       "         0.0088, 0.0095, 0.0082, 0.0099, 0.0088, 0.0098, 0.0091, 0.0100, 0.0110,\n",
       "         0.0109, 0.0099, 0.0100, 0.0108, 0.0100, 0.0112, 0.0094, 0.0106, 0.0104,\n",
       "         0.0091, 0.0085, 0.0085, 0.0103, 0.0101, 0.0097, 0.0110, 0.0104, 0.0115,\n",
       "         0.0090, 0.0093],\n",
       "        [0.0090, 0.0099, 0.0096, 0.0089, 0.0117, 0.0099, 0.0101, 0.0092, 0.0091,\n",
       "         0.0102, 0.0107, 0.0100, 0.0086, 0.0099, 0.0092, 0.0093, 0.0101, 0.0093,\n",
       "         0.0097, 0.0095, 0.0105, 0.0092, 0.0098, 0.0105, 0.0096, 0.0102, 0.0094,\n",
       "         0.0103, 0.0092, 0.0090, 0.0097, 0.0092, 0.0095, 0.0093, 0.0110, 0.0107,\n",
       "         0.0091, 0.0099, 0.0109, 0.0090, 0.0096, 0.0110, 0.0090, 0.0086, 0.0104,\n",
       "         0.0110, 0.0101, 0.0087, 0.0104, 0.0097, 0.0103, 0.0102, 0.0111, 0.0103,\n",
       "         0.0104, 0.0102, 0.0108, 0.0113, 0.0106, 0.0105, 0.0092, 0.0110, 0.0093,\n",
       "         0.0099, 0.0088, 0.0116, 0.0100, 0.0097, 0.0087, 0.0102, 0.0089, 0.0100,\n",
       "         0.0092, 0.0106, 0.0094, 0.0097, 0.0102, 0.0092, 0.0117, 0.0093, 0.0099,\n",
       "         0.0095, 0.0099, 0.0099, 0.0086, 0.0094, 0.0095, 0.0093, 0.0103, 0.0104,\n",
       "         0.0102, 0.0106, 0.0103, 0.0101, 0.0101, 0.0090, 0.0108, 0.0097, 0.0089,\n",
       "         0.0106, 0.0121]], device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summaryで中身を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape\n",
       "==============================================================================================================\n",
       "Net                                                          --                        --\n",
       "├─ModuleList (blocks)                                        --                        --\n",
       "│    └─ResNetBasicStem (0)                                   [4, 3, 16, 224, 224]      [4, 24, 16, 112, 112]\n",
       "│    │    └─Conv2plus1d (conv)                               [4, 3, 16, 224, 224]      [4, 24, 16, 112, 112]\n",
       "│    │    │    └─Conv3d (conv_t)                             [4, 3, 16, 224, 224]      [4, 24, 16, 112, 112]\n",
       "│    │    │    └─Conv3d (conv_xy)                            [4, 24, 16, 112, 112]     [4, 24, 16, 112, 112]\n",
       "│    │    └─BatchNorm3d (norm)                               [4, 24, 16, 112, 112]     [4, 24, 16, 112, 112]\n",
       "│    │    └─ReLU (activation)                                [4, 24, 16, 112, 112]     [4, 24, 16, 112, 112]\n",
       "│    └─ResStage (1)                                          [4, 24, 16, 112, 112]     [4, 24, 16, 56, 56]\n",
       "│    │    └─ModuleList (res_blocks)                          --                        --\n",
       "│    │    │    └─ResBlock (0)                                [4, 24, 16, 112, 112]     [4, 24, 16, 56, 56]\n",
       "│    │    │    └─ResBlock (1)                                [4, 24, 16, 56, 56]       [4, 24, 16, 56, 56]\n",
       "│    │    │    └─ResBlock (2)                                [4, 24, 16, 56, 56]       [4, 24, 16, 56, 56]\n",
       "│    └─ResStage (2)                                          [4, 24, 16, 56, 56]       [4, 48, 16, 28, 28]\n",
       "│    │    └─ModuleList (res_blocks)                          --                        --\n",
       "│    │    │    └─ResBlock (0)                                [4, 24, 16, 56, 56]       [4, 48, 16, 28, 28]\n",
       "│    │    │    └─ResBlock (1)                                [4, 48, 16, 28, 28]       [4, 48, 16, 28, 28]\n",
       "│    │    │    └─ResBlock (2)                                [4, 48, 16, 28, 28]       [4, 48, 16, 28, 28]\n",
       "│    │    │    └─ResBlock (3)                                [4, 48, 16, 28, 28]       [4, 48, 16, 28, 28]\n",
       "│    │    │    └─ResBlock (4)                                [4, 48, 16, 28, 28]       [4, 48, 16, 28, 28]\n",
       "│    └─ResStage (3)                                          [4, 48, 16, 28, 28]       [4, 96, 16, 14, 14]\n",
       "│    │    └─ModuleList (res_blocks)                          --                        --\n",
       "│    │    │    └─ResBlock (0)                                [4, 48, 16, 28, 28]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (1)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (2)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (3)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (4)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (5)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (6)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (7)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (8)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (9)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (10)                               [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    └─ResStage (4)                                          [4, 96, 16, 14, 14]       [4, 192, 16, 7, 7]\n",
       "│    │    └─ModuleList (res_blocks)                          --                        --\n",
       "│    │    │    └─ResBlock (0)                                [4, 96, 16, 14, 14]       [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (1)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (2)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (3)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (4)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (5)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (6)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    └─ResNetBasicHead (5)                                   [4, 192, 16, 7, 7]        [4, 101]\n",
       "│    │    └─ProjectedPool (pool)                             [4, 192, 16, 7, 7]        [4, 2048, 1, 1, 1]\n",
       "│    │    │    └─Conv3d (pre_conv)                           [4, 192, 16, 7, 7]        [4, 432, 16, 7, 7]\n",
       "│    │    │    └─BatchNorm3d (pre_norm)                      [4, 432, 16, 7, 7]        [4, 432, 16, 7, 7]\n",
       "│    │    │    └─ReLU (pre_act)                              [4, 432, 16, 7, 7]        [4, 432, 16, 7, 7]\n",
       "│    │    │    └─AvgPool3d (pool)                            [4, 432, 16, 7, 7]        [4, 432, 1, 1, 1]\n",
       "│    │    │    └─Conv3d (post_conv)                          [4, 432, 1, 1, 1]         [4, 2048, 1, 1, 1]\n",
       "│    │    │    └─ReLU (post_act)                             [4, 2048, 1, 1, 1]        [4, 2048, 1, 1, 1]\n",
       "│    │    └─Dropout (dropout)                                [4, 2048, 1, 1, 1]        [4, 2048, 1, 1, 1]\n",
       "│    │    └─Linear (proj)                                    [4, 1, 1, 1, 2048]        [4, 1, 1, 1, 101]\n",
       "│    │    └─Softmax (activation)                             [4, 101, 1, 1, 1]         [4, 101, 1, 1, 1]\n",
       "│    │    └─AdaptiveAvgPool3d (output_pool)                  [4, 101, 1, 1, 1]         [4, 101, 1, 1, 1]\n",
       "==============================================================================================================\n",
       "Total params: 3,181,623\n",
       "Trainable params: 206,949\n",
       "Non-trainable params: 2,974,674\n",
       "Total mult-adds (G): 18.93\n",
       "==============================================================================================================\n",
       "Input size (MB): 38.54\n",
       "Forward/backward pass size (MB): 5433.65\n",
       "Params size (MB): 12.73\n",
       "Estimated Total Size (MB): 5484.91\n",
       "=============================================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "torchinfo.summary(\n",
    "    model,\n",
    "    (4, 3, 16, 224, 224),\n",
    "    depth=4,\n",
    "    col_names=[\"input_size\",\n",
    "               \"output_size\"],\n",
    "    row_settings=(\"var_names\",)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "便利関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10a6db9edcc642cf820d605db387cdbb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a889299bf234727a27eb3770ed4e1a6"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(train_loader),\n",
    "                  total=len(train_loader),\n",
    "                  leave=True) as pbar_loss:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, batch in pbar_loss:\n",
    "                pbar_loss.set_description(\"[train]\")\n",
    "\n",
    "                inputs, targets = batch['video'].to(device), batch['label'].to(device)\n",
    "                bs = inputs.size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, targets), bs)\n",
    "\n",
    "                pbar_loss.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                    train_loss.val, train_acc.val,\n",
    "                ))\n",
    "\n"
   ]
  },
  {
   "source": [
    "fine-tuningなので速い．\n",
    "- 4GPUでおよそ4.5it/s，1エポック約2分\n",
    "- 1GPUでおよそ5it/s，1エポック約3分（596 iterations）\n",
    "\n",
    "スクラッチで学習するなら\n",
    "- 4GPUでおよそ2.6it/s，1エポック約4分\n",
    "- 1GPUでおよそ1.8it/s，1エポック約5.5分（596 iterations）\n",
    "\n",
    "\n",
    "以下の設定\n",
    "- video_num_subsampled = 16\n",
    "- batch_size = 16\n",
    "- num_workers = 24"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}