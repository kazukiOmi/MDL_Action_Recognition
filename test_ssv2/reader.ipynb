{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import (\n",
    "    Ucf101, \n",
    "    RandomClipSampler, \n",
    "    UniformClipSampler, \n",
    "    Kinetics,\n",
    "    SSv2\n",
    ")\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "\n",
    "#import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/NAS-TVS872XT/dataset/'\n",
    "        self.root = self.metadata_path\n",
    "        self.annotation_path = self.metadata_path\n",
    "        self.FRAMES_PER_CLIP = 16\n",
    "        self.STEP_BETWEEN_CLIPS = 16\n",
    "        self.BATCH_SIZE = 16\n",
    "        self.NUM_WORKERS = 8  # kinetics:8, ucf101:24\n",
    "\n",
    "        self.CLIP_DURATION = (8 * 8) / 30  # (num_frames * sampling_rate)/fps\n",
    "        self.VIDEO_NUM_SUBSAMPLED = 8  # 事前学習済みモデルに合わせて16→8\n",
    "        self.UCF101_NUM_CLASSES = 101\n",
    "        self.KINETIC400_NUM_CLASSES = 400\n",
    "\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "            transforms.Lambda(lambda x: x / 255.),\n",
    "            # Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "            ShortSideScale(size=256),\n",
    "            # RandomShortSideScale(min_size=256, max_size=320,),\n",
    "            # CenterCropVideo(crop_size=(256, 256)),\n",
    "            CenterCrop(256),\n",
    "            # RandomCrop(224),\n",
    "            RandomHorizontalFlip(),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        transform=transforms.Lambda(lambda x: x),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "root_SSv2 = '/mnt/NAS-TVS872XT/dataset/something-something-v2/'\n",
    "\n",
    "# train_dataset = SSv2(\n",
    "#             label_name_file=root_SSv2+\"anno/something-something-v2-labels.json\",\n",
    "#             video_label_file=root_SSv2+\"anno/something-something-v2-train.json\",\n",
    "#             video_path_label_file=root_SSv2+\"PySlowFast/train.csv\",\n",
    "#             video_path_prefix=\"/tmp/\",\n",
    "#             clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "#             video_sampler=SequentialSampler,\n",
    "#             # decode_audio=False,\n",
    "#             transform=transform,\n",
    "#         )\n",
    "\n",
    "val_dataset = SSv2(\n",
    "            label_name_file=root_SSv2+\"anno/something-something-v2-labels.json\",\n",
    "            video_label_file=root_SSv2+\"anno/something-something-v2-validation.json\",\n",
    "            video_path_label_file=root_SSv2+\"PySlowFast/val.csv\",\n",
    "            video_path_prefix=\"/tmp/ssv2_frame\",\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            # decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(len(train_dataset.video_sampler))\n",
    "print(len(val_dataset.video_sampler))\n",
    "data = val_dataset.__next__()\n",
    "print(data[\"video\"].shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "video = []\n",
    "frame_list = []\n",
    "\n",
    "# for i in range(16):\n",
    "#     video.append(data[\"video\"][i:i+1].numpy())\n",
    "#     video[i] = np.squeeze(video[i])\n",
    "#     video[i] = video[i].transpose(1,2,3,0)\n",
    "\n",
    "# video_id = 0  # videoを0から15で指定 \n",
    "data[\"video\"] = data[\"video\"].numpy().transpose(1,2,3,0)\n",
    "for i in range(8):\n",
    "    img = data[\"video\"][i:i+1, :, :, :]\n",
    "    img = np.squeeze(img)\n",
    "    frame_list.append(img)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# axes = []\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rows = 2\n",
    "cols = 4\n",
    "frame_id = 0\n",
    "\n",
    "fig, axes = plt.subplots(rows,cols,figsize=(16,16),tight_layout=True)\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img = frame_list[i*cols+j]\n",
    "        subplot_title = (\"frame:\" + str(frame_id))\n",
    "        axes[i,j].set_title(subplot_title)\n",
    "        axes[i,j].imshow(img)\n",
    "        frame_id = frame_id + 1\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "        # self.num_videos = make_num_videos(self.dataset)\n",
    "        self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_loader = DataLoader(LimitDataset(val_dataset),\n",
    "                            batch_size=2,\n",
    "                            drop_last=True\n",
    "                            )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(val_dataset.video_sampler))\n",
    "print(len(val_loader))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "16*1548"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_from_loader = iter(val_loader).__next__()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "video = []\n",
    "frame_list = []\n",
    "\n",
    "for i in range(2):\n",
    "    video.append(data_from_loader[\"video\"][i:i+1].numpy())\n",
    "    video[i] = np.squeeze(video[i])\n",
    "    video[i] = video[i].transpose(1,2,3,0)\n",
    "\n",
    "video_id = 0 # videoを0から1で指定 \n",
    "\n",
    "for i in range(8):\n",
    "    img = video[video_id][i:i+1, :, :, :]\n",
    "    img = np.squeeze(img)\n",
    "    frame_list.append(img)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# axes = []\n",
    "\n",
    "rows = 2\n",
    "cols = 4\n",
    "frame_id = 0\n",
    "\n",
    "fig, axes = plt.subplots(rows,cols,figsize=(16,16),tight_layout=True)\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img = frame_list[i*cols+j]\n",
    "        subplot_title = (\"frame:\" + str(frame_id))\n",
    "        axes[i,j].set_title(subplot_title)\n",
    "        axes[i,j].imshow(img)\n",
    "        frame_id = frame_id + 1\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}