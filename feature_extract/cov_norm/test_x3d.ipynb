{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import (\n",
    "    Ucf101, \n",
    "    RandomClipSampler, \n",
    "    UniformClipSampler, \n",
    "    Kinetics,\n",
    "    SSv2\n",
    ")\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn import mixture\n",
    "from sklearn import svm\n",
    "from sklearn import decomposition\n",
    "import os.path as osp\n",
    "import argparse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.NUM_EPOCH = 2\n",
    "        self.FRAMES_PER_CLIP = 16\n",
    "        self.STEP_BETWEEN_CLIPS = 16\n",
    "        self.BATCH_SIZE = 16\n",
    "        self.NUM_WORKERS = 32\n",
    "        # self.CLIP_DURATION = 16 / 25\n",
    "        # (num_frames * sampling_rate)/fps\n",
    "        self.kinetics_clip_duration = (8 * 8) / 30\n",
    "        self.ucf101_clip_duration = 16 / 25\n",
    "        self.VIDEO_NUM_SUBSAMPLED = 16\n",
    "        self.UCF101_NUM_CLASSES = 101\n",
    "        self.KINETIC400_NUM_CLASSES = 400"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Adapter2D(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(dim)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)       \n",
    "        self.bn2 = nn.BatchNorm2d(dim)\n",
    "    \n",
    "    def video_to_frame(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        num_frame = inputs.size(2)\n",
    "\n",
    "        inputs = inputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = inputs.reshape(batch_size * num_frame,\n",
    "                                 inputs.size(2),\n",
    "                                 inputs.size(3),\n",
    "                                 inputs.size(4))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def frame_to_video(\n",
    "            self, input: torch.Tensor, batch_size, num_frame, channel, height, width) -> torch.Tensor:\n",
    "        output = input.reshape(batch_size, num_frame, channel, height, width)\n",
    "        output = output.permute(0,2,1,3,4)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_frame = x.size(2)\n",
    "        channel= x.size(1)\n",
    "        height = x.size(3)\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = self.video_to_frame(x)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        residual = x\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        out += residual\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = self.frame_to_video(out, batch_size, num_frame, channel, height, height)\n",
    "        # print(out.shape)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class ReconstructNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torch.hub.load(\n",
    "        'facebookresearch/pytorchvideo', \"x3d_m\", pretrained=True)\n",
    "        self.model_num_features = model.blocks[5].proj.in_features\n",
    "        self.num_class = 101\n",
    "\n",
    "        self.net_bottom = nn.Sequential(\n",
    "            model.blocks[0],\n",
    "            model.blocks[1],\n",
    "            model.blocks[2],\n",
    "            model.blocks[3],\n",
    "            model.blocks[4]\n",
    "        )\n",
    "\n",
    "        self.adapter = Adapter2D(192)\n",
    "\n",
    "        self.net_top = nn.Sequential(\n",
    "            model.blocks[5].pool,\n",
    "            model.blocks[5].dropout\n",
    "        )\n",
    "\n",
    "        # self.linear = model.blocks[5].proj\n",
    "        self.linear = nn.Linear(self.model_num_features, self.num_class)\n",
    "\n",
    "        # # 学習させるパラメータ名\n",
    "        # self.update_param_names = [\"adapter.bn1.weight\", \"adapter.bn1.bias\",\n",
    "        #                            \"adapter.conv1.weight\", \"adapter.conv1.bias\",\n",
    "        #                            \"adapter.bn2.weight\", \"adapter.bn2.bias\",\n",
    "        #                            \"linear.weight\", \"linear.bias\"]\n",
    "        # # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "        # for name, param in self.named_parameters():\n",
    "        #     if name in self.update_param_names:\n",
    "        #         param.requires_grad = True\n",
    "        #         # print(name)\n",
    "        #     else:\n",
    "        #         param.requires_grad = False\n",
    "               \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x = x.permute(0,2,1,3,4)\n",
    "        x = self.net_bottom(x)\n",
    "        x = self.adapter(x)\n",
    "        x = self.net_top(x)\n",
    "        x = x.permute(0,2,3,4,1)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.num_class)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# adapter = Adapter2D(192)\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     adapter,\n",
    "#     input_size=(1,192,16,7,7),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# model_new = ReconstructNet()\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     model_new,\n",
    "#     input_size=(1,3,16,224,224),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 学習させてみる"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_kinetics(subset):\n",
    "    \"\"\"\n",
    "    Kinetics400のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"val\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    train_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    val_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(256),\n",
    "                CenterCrop(224),\n",
    "            ]),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    transform = val_transform if subset == \"val\" else train_transform\n",
    "\n",
    "    root_kinetics = '/mnt/dataset/Kinetics400/'\n",
    "\n",
    "    if subset == \"test\":\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + \"test_list.txt\",\n",
    "            video_path_prefix=root_kinetics + 'test/',\n",
    "            clip_sampler=RandomClipSampler(\n",
    "                clip_duration=args.kinetics_clip_duration),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + subset,\n",
    "            video_path_prefix=root_kinetics + subset,\n",
    "            clip_sampler=RandomClipSampler(\n",
    "                clip_duration=args.kinetics_clip_duration),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    return False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_ucf101(subset):\n",
    "    \"\"\"\n",
    "    ucf101のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    subset_root_Ucf101 = 'ucfTrainTestlist/trainlist01.txt' if subset == \"train\" else 'ucfTrainTestlist/testlist.txt'\n",
    "    # if subset == \"test\":\n",
    "    #     subset_root_Ucf101 = 'ucfTrainTestlist/testlist.txt'\n",
    "\n",
    "    args = Args()\n",
    "    train_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x - 1),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    val_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(256),\n",
    "                CenterCrop(224),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x - 1),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    transform = train_transform if subset == \"train\" else val_transform\n",
    "\n",
    "    root_ucf101 = '/mnt/dataset/UCF101/'\n",
    "    # root_ucf101 = '/mnt/NAS-TVS872XT/dataset/UCF101/'\n",
    "\n",
    "    dataset = Ucf101(\n",
    "        data_path=root_ucf101 + subset_root_Ucf101,\n",
    "        video_path_prefix=root_ucf101 + 'video/',\n",
    "        clip_sampler=RandomClipSampler(\n",
    "            clip_duration=args.ucf101_clip_duration),\n",
    "        video_sampler=RandomSampler,\n",
    "        decode_audio=False,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    return dataset\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def make_loader(dataset):\n",
    "    \"\"\"\n",
    "    データローダーを作成\n",
    "\n",
    "    Args:\n",
    "        dataset (pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset): get_datasetメソッドで取得したdataset\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: 取得したデータローダー\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    loader = DataLoader(LimitDataset(dataset),\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS,\n",
    "                        shuffle=True)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import os.path as osp\n",
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth', dir_data_name='UCF101'):\n",
    "    file_path = osp.join(dir_data_name, filename)\n",
    "    if not os.path.exists(dir_data_name):\n",
    "        os.makedirs(dir_data_name)\n",
    "    torch.save(state.state_dict(), file_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(file_path, osp.join(dir_data_name, 'model_best.pth'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# save_checkpoint()メソッドのテスト\n",
    "\n",
    "# test_net = ReconstructNet()\n",
    "# test_net.to(\"cuda\")\n",
    "# print(test_net)\n",
    "# save_checkpoint(test_net, True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def train():\n",
    "    args = Args()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = get_ucf101(\"train\")\n",
    "    val_dataset = get_ucf101(\"val\")\n",
    "    train_loader = make_loader(train_dataset)\n",
    "    val_loader = make_loader(val_dataset)\n",
    "\n",
    "    model = ReconstructNet()\n",
    "    model = model.to(device)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.1,\n",
    "        momentum=0.9,\n",
    "        weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    hyper_params = {\n",
    "        \"Dataset\": \"UCF101\",\n",
    "        \"epoch\": args.NUM_EPOCH,\n",
    "        \"batch_size\": args.BATCH_SIZE,\n",
    "        \"num_frame\": args.VIDEO_NUM_SUBSAMPLED,\n",
    "        # \"Adapter\": \"adp:1\",\n",
    "    }\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "        project_name=\"feeature-extract\",\n",
    "        workspace=\"kazukiomi\",\n",
    "    )\n",
    "\n",
    "    experiment.add_tag('pytorch')\n",
    "    experiment.log_parameters(hyper_params)\n",
    "\n",
    "    num_epochs = args.NUM_EPOCH\n",
    "\n",
    "    step = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "        for epoch in pbar_epoch:\n",
    "            pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "            \"\"\"Training mode\"\"\"\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "\n",
    "            with tqdm(enumerate(train_loader),\n",
    "                      total=len(train_loader),\n",
    "                      leave=True) as pbar_train_batch:\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_idx, batch in pbar_train_batch:\n",
    "                    pbar_train_batch.set_description(\n",
    "                        \"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                    inputs = batch['video'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss.update(loss, bs)\n",
    "                    train_acc.update(top1(outputs, labels), bs)\n",
    "\n",
    "                    pbar_train_batch.set_postfix_str(\n",
    "                        ' | loss_avg={:6.04f} , top1_avg={:6.04f}'\n",
    "                        ' | batch_loss={:6.04f} , batch_top1={:6.04f}'\n",
    "                        ''.format(\n",
    "                            train_loss.avg, train_acc.avg,\n",
    "                            train_loss.val, train_acc.val,\n",
    "                        ))\n",
    "\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_accuracy\", train_acc.val, step=step)\n",
    "                    step += 1\n",
    "\n",
    "            \"\"\"Val mode\"\"\"\n",
    "            model.eval()\n",
    "            val_loss = AverageMeter()\n",
    "            val_acc = AverageMeter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, val_batch in enumerate(val_loader):\n",
    "                    inputs = val_batch['video'].to(device)\n",
    "                    labels = val_batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    val_outputs = model(inputs)\n",
    "                    loss = criterion(val_outputs, labels)\n",
    "\n",
    "                    val_loss.update(loss, bs)\n",
    "                    val_acc.update(top1(val_outputs, labels), bs)\n",
    "            \"\"\"Finish Val mode\"\"\"\n",
    "\n",
    "            \"\"\"save model\"\"\"\n",
    "            if best_acc < val_acc.avg:\n",
    "                best_acc = val_acc.avg\n",
    "                is_best = True\n",
    "            else:\n",
    "                is_best = False\n",
    "                \n",
    "            save_checkpoint(model, is_best)\n",
    "            \n",
    "\n",
    "            pbar_epoch.set_postfix_str(\n",
    "                ' train_loss={:6.04f} , val_loss={:6.04f}, train_acc={:6.04f}, val_acc={:6.04f}'\n",
    "                ''.format(\n",
    "                    train_loss.avg,\n",
    "                    val_loss.avg,\n",
    "                    train_acc.avg,\n",
    "                    val_loss.avg)\n",
    "            )\n",
    "\n",
    "            # metrics = {\"train_accuracy\": train_acc.avg,\n",
    "            #            \"val_accuracy\": val_acc.avg\n",
    "            #            }\n",
    "            # experiment.log_multiple_metrics(metrics, epoch + 1)\n",
    "            experiment.log_metric(\"epoch_train_accuracy\",\n",
    "                                  train_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"epoch_train_loss\",\n",
    "                                  train_loss.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_accuracy\",\n",
    "                                  val_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_loss\",\n",
    "                                  val_loss.avg,\n",
    "                                  step=epoch + 1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 上で学習したモデルを読み込む"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# called_model = ReconstructNet()\n",
    "# model_path = \"UCF101/model_best.pth\"\n",
    "# called_model.load_state_dict(torch.load(model_path))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# 呼び出したモデルに1バッチだけ流して精度とロスをテスト\n",
    "\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# called_model = called_model.to(device)\n",
    "# test_loader = make_loader(get_ucf101(\"val\"))\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# test_batch = iter(test_loader).__next__()\n",
    "\n",
    "# model.eval()\n",
    "# test_loss = AverageMeter()\n",
    "# test_acc = AverageMeter()\n",
    "\n",
    "# test_inputs = test_batch[\"video\"].to(device)\n",
    "# test_labels = test_batch[\"label\"].to(device)\n",
    "# bs = test_inputs.size(0)\n",
    "\n",
    "# test_out = called_model(test_inputs)\n",
    "# loss = criterion(test_out, test_labels)\n",
    "\n",
    "# test_loss.update(loss, bs)\n",
    "# test_acc.update(top1(test_out, test_labels), bs)\n",
    "\n",
    "# print(test_acc.avg)\n",
    "# print(test_loss.avg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# valの精度をテスト\n",
    "\n",
    "# args = Args()\n",
    "# device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# val_dataset = get_ucf101(\"val\")\n",
    "# val_loader = make_loader(val_dataset)\n",
    "# called_model = called_model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # called_model = called_model.to(device)\n",
    "# called_model.eval()\n",
    "# val_loss = AverageMeter()\n",
    "# val_acc = AverageMeter()\n",
    "# with tqdm(enumerate(val_loader),\n",
    "#                       total=len(val_loader),\n",
    "#                       leave=True) as pbar_val_batch:\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, val_batch in pbar_val_batch:\n",
    "#             inputs = val_batch['video'].to(device)\n",
    "#             labels = val_batch['label'].to(device)\n",
    "\n",
    "#             bs = inputs.size(0)\n",
    "\n",
    "#             val_outputs = called_model(inputs)\n",
    "#             loss = criterion(val_outputs, labels)\n",
    "\n",
    "#             val_loss.update(loss, bs)\n",
    "#             val_acc.update(top1(val_outputs, labels), bs)\n",
    "            \n",
    "#             pbar_val_batch.set_postfix_str(\n",
    "#                         ' | loss_avg={:6.04f} , top1_avg={:6.04f}'\n",
    "#                         ' | batch_loss={:6.04f} , batch_top1={:6.04f}'\n",
    "#                         ''.format(\n",
    "#                             val_loss.avg, val_acc.avg,\n",
    "#                             val_loss.val, val_acc.val,\n",
    "#                         ))\n",
    "# print(val_acc.avg)\n",
    "# print(val_loss.avg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 新しくクラスを定義して特徴量を保存する\n",
    "- 新しくクラスを定義するのではなくアダプタのオプション（引数）で行いたい\n",
    "  - アダプタ内で行うとアダプタごとにファイルの保存場所を変更できない\n",
    "    - さらにアダプタにオプション与える？\n",
    "  - やはり新しくクラスを定義する？"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class Adapter2DExtract(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, dim, adapter_name: str):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(dim)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)       \n",
    "        self.bn2 = nn.BatchNorm2d(dim)\n",
    "\n",
    "        self.adapter_name = adapter_name\n",
    "        self.num_of_dataset = 9472\n",
    "        self.path_feature = \"UCF101/features/\"\n",
    "    \n",
    "    def video_to_frame(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        num_frame = inputs.size(2)\n",
    "\n",
    "        inputs = inputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = inputs.reshape(batch_size * num_frame,\n",
    "                                 inputs.size(2),\n",
    "                                 inputs.size(3),\n",
    "                                 inputs.size(4))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def frame_to_video(\n",
    "            self, input: torch.Tensor, batch_size, num_frame, channel, height, width) -> torch.Tensor:\n",
    "        output = input.reshape(batch_size, num_frame, channel, height, width)\n",
    "        output = output.permute(0,2,1,3,4)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, x, data_name):\n",
    "        batch_size = x.size(0)\n",
    "        num_frame = x.size(2)\n",
    "        channel= x.size(1)\n",
    "        height = x.size(3)\n",
    "\n",
    "        x = self.video_to_frame(x)\n",
    "        \n",
    "        residual = x\n",
    "        out = self.bn1(x)\n",
    "\n",
    "        \"\"\"input feature extraxt\"\"\"\n",
    "        features_in = out.data.cpu().numpy()\n",
    "        # num = 2000000 * 512 / features_in.shape[1] / self.num_of_dataset\n",
    "        # if num > features_in.shape[2]**2:\n",
    "        #     num = features_in.shape[2]**2\n",
    "        features_in = features_in.reshape(features_in.shape[1], -1).transpose(1,0)\n",
    "        # features_in = random.sample(features_in, num)\n",
    "        dir_name = osp.join(self.path_feature, self.adapter_name+'_wh')\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "        np.save(osp.join(self.path_feature, self.adapter_name+'_wh', data_name), features_in)\n",
    "\n",
    "\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        \"\"\"output feature extract\"\"\"\n",
    "        features_out = out.data.cpu().numpy()\n",
    "        features_out = features_out.reshape(features_out.shape[1], -1).transpose(1,0)\n",
    "        dir_name = osp.join(self.path_feature, self.adapter_name+'_rc')\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "        np.save(osp.join(self.path_feature, self.adapter_name+'_rc', data_name), features_out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = self.frame_to_video(out, batch_size, num_frame, channel, height, height)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torch.hub.load(\n",
    "        'facebookresearch/pytorchvideo', \"x3d_m\", pretrained=True)\n",
    "        self.model_num_features = model.blocks[5].proj.in_features\n",
    "        self.num_class = 101\n",
    "\n",
    "        self.net_bottom = nn.Sequential(\n",
    "            model.blocks[0],\n",
    "            model.blocks[1],\n",
    "            model.blocks[2],\n",
    "            model.blocks[3],\n",
    "            model.blocks[4]\n",
    "        )\n",
    "\n",
    "        self.adapter = Adapter2DExtract(192, \"adapter0\")\n",
    "\n",
    "        self.net_top = nn.Sequential(\n",
    "            model.blocks[5].pool,\n",
    "            model.blocks[5].dropout\n",
    "        )\n",
    "\n",
    "        # self.linear = model.blocks[5].proj\n",
    "        self.linear = nn.Linear(self.model_num_features, self.num_class)\n",
    "\n",
    "        # 学習させるパラメータ名\n",
    "        self.update_param_names = [\"adapter.bn1.weight\", \"adapter.bn1.bias\",\n",
    "                                   \"adapter.conv1.weight\", \"adapter.conv1.bias\",\n",
    "                                   \"adapter.bn2.weight\", \"adapter.bn2.bias\",\n",
    "                                   \"linear.weight\", \"linear.bias\"]\n",
    "        # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.update_param_names:\n",
    "                param.requires_grad = True\n",
    "                # print(name)\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "               \n",
    "\n",
    "    def forward(self, x: torch.Tensor, data_name) -> torch.Tensor:\n",
    "        x = self.net_bottom(x)\n",
    "        x = self.adapter(x, data_name)\n",
    "        x = self.net_top(x)\n",
    "        x = x.permute(0,2,3,4,1)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.num_class)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# ビデオの名前を取得するテスト\n",
    "\n",
    "# dataset = get_ucf101(\"val\")\n",
    "# test_loader = DataLoader(LimitDataset(dataset),\n",
    "#                         batch_size=1,\n",
    "#                         drop_last=False,\n",
    "#                         num_workers=32,\n",
    "#                         shuffle=False)\n",
    "# test_batch = iter(test_loader).__next__()\n",
    "\n",
    "# video_name = test_batch[\"video_name\"]\n",
    "# print(video_name)\n",
    "# video_name = video_name[0].split(\".\")[0]\n",
    "# print(video_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# 新しく定義したモデルが保存済みのモデルのパラメータになっているかどうかを確認\n",
    "\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# test_model = FeatureExtractor()\n",
    "# model_path = \"UCF101/model_best.pth\"\n",
    "# test_model.load_state_dict(torch.load(model_path))\n",
    "# test_model = test_model.to(device)\n",
    "# test_loader = make_loader(get_ucf101(\"val\"))\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# test_batch = iter(test_loader).__next__()\n",
    "\n",
    "# model.eval()\n",
    "# test_loss = AverageMeter()\n",
    "# test_acc = AverageMeter()\n",
    "\n",
    "# test_inputs = test_batch[\"video\"].to(device)\n",
    "# test_labels = test_batch[\"label\"].to(device)\n",
    "# video_name = test_batch[\"video_name\"][0].split(\".\")[0]\n",
    "# bs = test_inputs.size(0)\n",
    "\n",
    "# test_out = test_model(test_inputs, video_name)\n",
    "# loss = criterion(test_out, test_labels)\n",
    "\n",
    "# test_loss.update(loss, bs)\n",
    "# test_acc.update(top1(test_out, test_labels), bs)\n",
    "\n",
    "# print(test_acc.avg)\n",
    "# print(test_loss.avg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def feature_extract():\n",
    "    args = Args()\n",
    "    device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = get_ucf101(\"train\")\n",
    "    train_loader = DataLoader(LimitDataset(train_dataset),\n",
    "                        batch_size=1,\n",
    "                        drop_last=False,\n",
    "                        num_workers=32,\n",
    "                        shuffle=False)\n",
    "\n",
    "    model = FeatureExtractor()\n",
    "    best_model_path = \"UCF101/model_best.pth\"\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    with tqdm(enumerate(train_loader),\n",
    "                total=len(train_loader),\n",
    "                leave=True) as pbar_train_batch:\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for batch_idx, batch in pbar_train_batch:\n",
    "            inputs = batch['video'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            video_name = batch[\"video_name\"][0].split(\".\")[0]\n",
    "\n",
    "            outputs = model(inputs, video_name)\n",
    "            \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# feature_extract()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 中間特徴量を主成分分析にかけ結果を保存する"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# これはテスト，下で関数で定義した\n",
    "\n",
    "# channel_dim = [192]\n",
    "# exact_list = [\"adapter0\"]\n",
    "# pre_dir_feature = \"UCF101/features\"\n",
    "\n",
    "# for k in range(len(channel_dim)):\n",
    "#     # print(k)\n",
    "#     dim = channel_dim[k]\n",
    "\n",
    "    \n",
    "#     dir_feature_in = osp.join(pre_dir_feature, exact_list[k] + '_wh')\n",
    "#     # print(dir_feature_in)\n",
    "#     files_wh = os.listdir(dir_feature_in)\n",
    "#     # print(\"file length: {}\".format(len(files_wh)))\n",
    "\n",
    "#     for i in range(len(files_wh)):\n",
    "#         if i == 0:\n",
    "#             num = np.load(osp.join(dir_feature_in, files_wh[i])).shape[0]\n",
    "#             print(num)\n",
    "#             features_in = np.zeros((len(files_wh)*num, dim))\n",
    "#             print(features_in.shape)\n",
    "#         features_in[i*num:(i+1)*num] = np.load(osp.join(dir_feature_in, files_wh[i]))\n",
    "#     pca = decomposition.PCA(n_components=dim)\n",
    "#     pca.fit(features_in)\n",
    "#     pickle.dump(pca, open(osp.join(pre_dir_feature, exact_list[k] + '_wh') + '_pca.sav', 'wb'))\n",
    "\n",
    "#     dir_feature_out = osp.join(pre_dir_feature, exact_list[k] + '_rc')\n",
    "#     files_rc = os.listdir(dir_feature_out)\n",
    "#     features_out = np.zeros((len(files_rc)*num, dim))\n",
    "#     for i in range(len(files_rc)):\n",
    "#         features_out[i*num:(i+1)*num] = np.load(osp.join(dir_feature, files_rc[i]))\n",
    "#     pca = decomposition.PCA(n_components=dim)\n",
    "#     pca.fit(features_out)\n",
    "#     pickle.dump(pca, open(osp.join(pre_dir_feature, exact_list[k] + '_rc') + '_pca.sav', 'wb'))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 上の処理で思ったこと\n",
    "- 画像の場合\n",
    "  - 特徴量のshapeを(B, C, W, H) -> (C, B*W*H) -> (B*W*H, C)に変更してからセーブ（B=1）\n",
    "  - よってデータ全ての特徴量を格納したものは（データ数 x W x H, dim(チャンネル数)）\n",
    "- 動画の場合\n",
    "  - フレームをバッチ方向につなげているのでB=T(フレーム数)になってしまう\n",
    "\n",
    "### 処理の違い\n",
    "動画を複数の画像（複数のデータに分けて）としてみて主成分分析するか動画を1つのデータとして主成分分析するか違いが出るかと思ったけど関係なさそう\n",
    "（チャネル数とそれ以外に分けて主成分分析するから，features[0:8]まで一気に格納されるか(動画の場合)，features[0]から順に逐一格納されるかの違いしかないと思う）\n",
    "  \n",
    "<!-- ### 処理の方法による違い\n",
    "- (データ数*T*W*H, dim)のまま主成分分析\n",
    "  - 1つの動画 -->"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def save_pca():\n",
    "    channel_dim = [192]\n",
    "    exact_list = [\"adapter0\"]\n",
    "    pre_dir_feature = \"UCF101/features\"\n",
    "\n",
    "    for k in range(len(channel_dim)):\n",
    "        dim = channel_dim[k]\n",
    "\n",
    "        dir_feature_in = osp.join(pre_dir_feature, exact_list[k] + '_wh')\n",
    "        files_wh = os.listdir(dir_feature_in)\n",
    "        for i in range(len(files_wh)):\n",
    "            if i == 0:\n",
    "                num = np.load(osp.join(dir_feature_in, files_wh[i])).shape[0]\n",
    "                features_in = np.zeros((len(files_wh)*num, dim))\n",
    "            features_in[i*num:(i+1)*num] = np.load(osp.join(dir_feature_in, files_wh[i]))\n",
    "        pca_wh = decomposition.PCA(n_components=dim)\n",
    "        pca_wh.fit(features_in)\n",
    "        pickle.dump(pca_wh, open(osp.join(pre_dir_feature, exact_list[k] + '_wh') + '_pca.sav', 'wb'))\n",
    "        # print(pca_wh.explained_variance_ratio_)\n",
    "\n",
    "        dir_feature_out = osp.join(pre_dir_feature, exact_list[k] + '_rc')\n",
    "        files_rc = os.listdir(dir_feature_out)\n",
    "        features_out = np.zeros((len(files_rc)*num, dim))\n",
    "        for i in range(len(files_rc)):\n",
    "            features_out[i*num:(i+1)*num] = np.load(osp.join(dir_feature_out, files_rc[i]))\n",
    "        pca_rc = decomposition.PCA(n_components=dim)\n",
    "        pca_rc.fit(features_out)\n",
    "        pickle.dump(pca_rc, open(osp.join(pre_dir_feature, exact_list[k] + '_rc') + '_pca.sav', 'wb'))\n",
    "        # print(pca_rc.explained_variance_ratio_)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# save_pca()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 主成分分析の結果を呼び出して次元数を決定する（アダプタのチャネル数の決定）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# これはテスト，下に関数で定義した\n",
    "\n",
    "# channel_dim = [192]\n",
    "# exact_list = [\"adapter0\"]\n",
    "# pre_dir_feature = \"UCF101/features\"\n",
    "# pca_ratio = 0.995 # 次元削減を累積寄与率で指定するために用意\n",
    "\n",
    "# pca_dim = []\n",
    "# pca_dir = {}\n",
    "\n",
    "# for k in range(len(channel_dim)):\n",
    "#     dim = channel_dim[k]\n",
    "\n",
    "#     dir_feature_in = osp.join(pre_dir_feature, exact_list[k] + '_wh' + '_pca.sav')\n",
    "#     # print(dir_feature_in)\n",
    "#     # print(osp.exists(dir_feature_in))\n",
    "#     pca_wh = pickle.load(open(dir_feature_in, 'rb'))\n",
    "#     ratio_wh = pca_wh.explained_variance_ratio_  # 因子寄与率を計算\n",
    "#     # print(ratio_wh.shape)\n",
    "#     cum_wh = [np.sum(ratio_wh[0:i+1]) for i in range(dim)]  # 累積寄与率を計算 \n",
    "#     # print(cum_wh)\n",
    "#     cum_wh_dim = np.sum(np.array(cum_wh)<pca_ratio)\n",
    "#     # print(cum_wh_dim)\n",
    "#     pca_dim.append(cum_wh_dim)\n",
    "#     pca_dir[osp.join(pre_dir_feature, exact_list[k] + \"_wh\")] = osp.join(pre_dir_feature, exact_list[k] + \"_wh\") + \"_pca.sav\"\n",
    "\n",
    "#     dir_feature_out = osp.join(pre_dir_feature, exact_list[k] + '_rc' + '_pca.sav')\n",
    "#     pca_rc = pickle.load(open(dir_feature_out, 'rb'))\n",
    "#     ratio_rc = pca_rc.explained_variance_ratio_\n",
    "#     cum_rc = [np.sum(ratio_rc[0:i+1]) for i in range(dim)]\n",
    "#     # print(cum_rc)\n",
    "#     cum_rc_dim = np.sum(np.array(cum_rc)<pca_ratio)\n",
    "#     pca_dim.append(cum_rc_dim)\n",
    "#     pca_dir[osp.join(pre_dir_feature, exact_list[k] + \"_rc\")] = osp.join(pre_dir_feature, exact_list[k] + \"_rc\") + \"_pca.sav\"\n",
    "\n",
    "#     print(pca_dim)\n",
    "#     print(pca_dir)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def compute_dim():\n",
    "    channel_dim = [192]\n",
    "    exact_list = [\"adapter0\"]\n",
    "    pre_dir_feature = \"UCF101/features\"\n",
    "    pca_ratio = 0.995 # 次元削減を累積寄与率で指定するために用意\n",
    "\n",
    "    pca_dim = []\n",
    "    pca_dir = {}\n",
    "\n",
    "    for k in range(len(channel_dim)):\n",
    "        dim = channel_dim[k]\n",
    "\n",
    "        dir_feature_in = osp.join(pre_dir_feature, exact_list[k] + '_wh' + '_pca.sav')\n",
    "        pca_wh = pickle.load(open(dir_feature_in, 'rb'))\n",
    "        ratio_wh = pca_wh.explained_variance_ratio_  # 因子寄与率を計算\n",
    "        cum_wh = [np.sum(ratio_wh[0:i+1]) for i in range(dim)]  # 累積寄与率を計算 \n",
    "        cum_wh_dim = np.sum(np.array(cum_wh)<pca_ratio)\n",
    "        if cum_wh_dim < 2:\n",
    "            cum_wh_dim = 2\n",
    "        pca_dim.append(cum_wh_dim)\n",
    "        pca_dir[osp.join(pre_dir_feature, exact_list[k] + \"_wh\")] = osp.join(pre_dir_feature, exact_list[k] + \"_wh\") + \"_pca.sav\"\n",
    "\n",
    "        dir_feature_out = osp.join(pre_dir_feature, exact_list[k] + '_rc' + '_pca.sav')\n",
    "        pca_rc = pickle.load(open(dir_feature_out, 'rb'))\n",
    "        ratio_rc = pca_rc.explained_variance_ratio_\n",
    "        cum_rc = [np.sum(ratio_rc[0:i+1]) for i in range(dim)]\n",
    "        cum_rc_dim = np.sum(np.array(cum_rc)<pca_ratio)\n",
    "        if cum_rc_dim < 2:\n",
    "            cum_rc_dim = 2\n",
    "        pca_dim.append(cum_rc_dim)\n",
    "        pca_dir[osp.join(pre_dir_feature, exact_list[k] + \"_rc\")] = osp.join(pre_dir_feature, exact_list[k] + \"_rc\") + \"_pca.sav\"\n",
    "\n",
    "    return pca_dim, pca_dir"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "~~### 上の結果から同じ結果が出たので以下でどこで間違えたか確かめる~~\n",
    "- ~~保存した特徴量は異なる~~\n",
    "  - ~~diff adapter0_wh/v_ApplyEyeMakeup_g08_c01.npy adapter0_rc/v_ApplyEyeMakeup_g08_c01.npyで確認~~\n",
    "-  ~~特徴量を主成分分析にかけた結果が一致している~~\n",
    "  - ~~diff adapter0_wh_pca.sav adapter0_rc_pca.savで確認~~~\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 主成分分析の結果をアダプタに適用する"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# pca_dim, pca_dir = compute_dim()\n",
    "# print(pca_dim)\n",
    "# print(pca_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "class PcaAdapter2D(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, dim, in_dim, out_dim, pca=True):\n",
    "        super().__init__()\n",
    "        self.pca = pca\n",
    "        self.bn1 = nn.BatchNorm2d(dim)\n",
    "        self.conv_wh = nn.Conv2d(dim, in_dim, 1)\n",
    "        #for i in self.conv_wh.parameters():\n",
    "        #    i.requires_grad = False\n",
    "        if pca==True:\n",
    "            self.conv_MAL = nn.Conv2d(in_dim, out_dim, 1)\n",
    "        self.conv_rc = nn.Conv2d(out_dim, dim, 1)\n",
    "        #for i in self.conv_rc.parameters():\n",
    "        #    i.requires_grad = False        \n",
    "        self.bn2 = nn.BatchNorm2d(dim)\n",
    "\n",
    "    def video_to_frame(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        num_frame = inputs.size(2)\n",
    "\n",
    "        inputs = inputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = inputs.reshape(batch_size * num_frame,\n",
    "                                 inputs.size(2),\n",
    "                                 inputs.size(3),\n",
    "                                 inputs.size(4))\n",
    "        return outputs\n",
    "\n",
    "    def frame_to_video(\n",
    "            self, input: torch.Tensor, batch_size, num_frame, channel, height, width) -> torch.Tensor:\n",
    "        output = input.reshape(batch_size, num_frame, channel, height, width)\n",
    "        output = output.permute(0,2,1,3,4)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_frame = x.size(2)\n",
    "        channel= x.size(1)\n",
    "        height = x.size(3)\n",
    "        \n",
    "        x = self.video_to_frame(x)\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv_wh(out)\n",
    "        if self.pca == True:\n",
    "            out = self.conv_MAL(out)\n",
    "        out = self.conv_rc(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = self.frame_to_video(out, batch_size, num_frame, channel, height, height)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# 主成分分析の結果をアダプタの重みにのせるテスト\n",
    "\n",
    "# pca_dim, pca_dir = compute_dim()\n",
    "# print(pca_dir)\n",
    "\n",
    "# adapter_params = {}\n",
    "# exact_list = [\"adapter0\"]\n",
    "\n",
    "# # print(pca_dir.keys())\n",
    "# # print(type(pca_dir.keys()))\n",
    "# # print(len(pca_dir))\n",
    "\n",
    "# for i, key in enumerate(pca_dir):\n",
    "#     print(key)\n",
    "#     print(pca_dir[key])\n",
    "#     pca = pickle.load(open(pca_dir[key], \"rb\"))\n",
    "#     print(pca)\n",
    "#     print(exact_list[int(i/2)])\n",
    "#     print(pca.components_.shape)\n",
    "#     print(pca.components_[:pca_dim[i]].shape)\n",
    "#     print(pca.components_[:pca_dim[i]][:,:,np.newaxis,np.newaxis].shape)\n",
    "#     if i % 2 == 0:\n",
    "#         adapter_params[exact_list[int(i/2)]+\".conv_wh.weight\"] = torch.tensor(pca.components_[:pca_dim[i]][:,:,np.newaxis,np.newaxis])\n",
    "#         adapter_params[exact_list[int(i/2)]+\".conv_wh.bias\"] = torch.tensor(-1 * np.dot(pca.components_[:pca_dim[i]], pca.mean_))\n",
    "#     else:\n",
    "#         adapter_params[exact_list[int(i/2)]+\".conv_rc.weight\"] = torch.tensor(pca.components_[:pca_dim[i]][:,:,np.newaxis,np.newaxis])\n",
    "#         adapter_params[exact_list[int(i/2)]+\".conv_rc.bias\"] = torch.tensor(-1 * np.dot(pca.components_[:pca_dim[i]], pca.mean_))\n",
    "        \n",
    "# print(adapter_params.keys())\n",
    "# # print(adapter_params['adapter0.conv_wh.weight'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "class MultiDomainNetwork(nn.Module):\n",
    "    def __init__(self, pca_dim, pca_dir):\n",
    "        super().__init__()\n",
    "        model = torch.hub.load(\n",
    "        'facebookresearch/pytorchvideo', \"x3d_m\", pretrained=True)\n",
    "        self.model_num_features = model.blocks[5].proj.in_features\n",
    "        self.num_class = 101\n",
    "\n",
    "        self.pca_dir = pca_dir\n",
    "        self.pca_dim = pca_dim\n",
    "\n",
    "        self.net_bottom = nn.Sequential(\n",
    "            model.blocks[0],\n",
    "            model.blocks[1],\n",
    "            model.blocks[2],\n",
    "            model.blocks[3],\n",
    "            model.blocks[4]\n",
    "        )\n",
    "\n",
    "        self.adapter0 = PcaAdapter2D(192, pca_dim[0], pca_dim[1])\n",
    "\n",
    "        self.net_top = nn.Sequential(\n",
    "            model.blocks[5].pool,\n",
    "            model.blocks[5].dropout\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(self.model_num_features, self.num_class)\n",
    "\n",
    "        # 学習させるパラメータ名\n",
    "        self.update_param_names = [\"adapter0.bn1.weight\", \"adapter0.bn1.bias\",\n",
    "                                   \"adapter0.conv_wh.weight\", \"adapter0.conv_wh.bias\",\n",
    "                                   \"adapter0.conv_MAL.weight\", \"adapter0.conv_MAL.bias\",\n",
    "                                   \"adapter0.conv_rc.weight\", \"adapter0.conv_rc.bias\",\n",
    "                                   \"adapter0.bn2.weight\", \"adapter0.bn2.bias\",\n",
    "                                   \"linear.weight\", \"linear.bias\"]\n",
    "        # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.update_param_names:\n",
    "                param.requires_grad = True\n",
    "                # print(name)\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def apply_weights(self):\n",
    "        pca_dim = self.pca_dim\n",
    "        pca_dir = self.pca_dir\n",
    "        adapter_params = {}\n",
    "        exact_list = [\"adapter0\"]\n",
    "\n",
    "        for i, key in enumerate(pca_dir):\n",
    "            pca = pickle.load(open(pca_dir[key], \"rb\"))\n",
    "            if i % 2 == 0:\n",
    "                adapter_params[exact_list[int(i/2)]+\".conv_wh.weight\"] = torch.tensor(pca.components_[:pca_dim[i]][:,:,np.newaxis,np.newaxis])\n",
    "                # adapter_params[exact_list[int(i/2)]+\".conv_wh.bias\"] = torch.tensor(-1 * np.dot(pca.components_[:pca_dim[i]], pca.mean_))\n",
    "            else:\n",
    "                adapter_params[exact_list[int(i/2)]+\".conv_rc.weight\"] = torch.tensor(pca.components_[:pca_dim[i]].T[:,:,np.newaxis,np.newaxis])\n",
    "                # adapter_params[exact_list[int(i/2)]+\".conv_rc.bias\"] = torch.tensor(-1 * np.dot(pca.components_[:pca_dim[i]], pca.mean_))\n",
    "                \n",
    "        self.load_state_dict(adapter_params, strict=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.net_bottom(x)\n",
    "        x = self.adapter0(x)\n",
    "        x = self.net_top(x)\n",
    "        x = x.permute(0,2,3,4,1)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.num_class)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# アダプタの重みパラメータに主成分分析の結果が載っているかを確認\n",
    "# test_net.apply_weights()をなくすと毎回異なる重みになるがこの関数を使うと重みが一意に決まるのでアダプタの重みは問題ない(確認はupdate_param_names1)\n",
    "# アダプタ以外の重みが変化していないかはupdate_param_namesにしてapply_weight()がある時のない時で変化しないことより確認\n",
    "\n",
    "# test_net = MultiDomainNetwork(pca_dim, pca_dir)\n",
    "# # test_net.apply_weights()\n",
    "# update_param_names = [\"net_bottom.1.res_blocks.0.branch1_conv.weight\"]\n",
    "\n",
    "# update_param_names1 = [\"adapter0.conv_wh.weight\"]\n",
    "\n",
    "# for name, params in test_net.named_parameters():\n",
    "#     if name in update_param_names:\n",
    "#         print(name)\n",
    "#         print(params)\n",
    "#         print(params.shape)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# torchinfoでモデルの構造を確認\n",
    "\n",
    "# pca_dim, pca_dir = compute_dim()\n",
    "# test_net = MultiDomainNetwork(pca_dim,pca_dir)\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     test_net,\n",
    "#     input_size=(1,3,16,224,224),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 主成分分析の結果から決めたアダプタの重みで初期化したモデルでアダプタと出力層を再学習（最終的なモデル）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def train_covnorm():\n",
    "    args = Args()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = get_ucf101(\"train\")\n",
    "    val_dataset = get_ucf101(\"val\")\n",
    "    train_loader = make_loader(train_dataset)\n",
    "    val_loader = make_loader(val_dataset)\n",
    "\n",
    "    pca_dim, pca_dir = compute_dim()\n",
    "\n",
    "    model = MultiDomainNetwork(pca_dim, pca_dim)\n",
    "    model = model.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.01,\n",
    "        momentum=0.9,\n",
    "        weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    hyper_params = {\n",
    "        \"Dataset\": \"UCF101\",\n",
    "        \"epoch\": args.NUM_EPOCH,\n",
    "        \"batch_size\": args.BATCH_SIZE,\n",
    "        \"num_frame\": args.VIDEO_NUM_SUBSAMPLED,\n",
    "        \"Adapter\": \"adp:0\",\n",
    "    }\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "        project_name=\"feeature-extract\",\n",
    "        workspace=\"kazukiomi\",\n",
    "    )\n",
    "\n",
    "    experiment.add_tag('pytorch')\n",
    "    experiment.log_parameters(hyper_params)\n",
    "\n",
    "    num_epochs = args.NUM_EPOCH\n",
    "\n",
    "    step = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "        for epoch in pbar_epoch:\n",
    "            pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "            \"\"\"Training mode\"\"\"\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "\n",
    "            with tqdm(enumerate(train_loader),\n",
    "                      total=len(train_loader),\n",
    "                      leave=True) as pbar_train_batch:\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_idx, batch in pbar_train_batch:\n",
    "                    pbar_train_batch.set_description(\n",
    "                        \"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                    inputs = batch['video'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss.update(loss, bs)\n",
    "                    train_acc.update(top1(outputs, labels), bs)\n",
    "\n",
    "                    pbar_train_batch.set_postfix_str(\n",
    "                        ' | loss_avg={:6.04f} , top1_avg={:6.04f}'\n",
    "                        ' | batch_loss={:6.04f} , batch_top1={:6.04f}'\n",
    "                        ''.format(\n",
    "                            train_loss.avg, train_acc.avg,\n",
    "                            train_loss.val, train_acc.val,\n",
    "                        ))\n",
    "\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_accuracy\", train_acc.val, step=step)\n",
    "                    step += 1\n",
    "\n",
    "            \"\"\"Val mode\"\"\"\n",
    "            model.eval()\n",
    "            val_loss = AverageMeter()\n",
    "            val_acc = AverageMeter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, val_batch in enumerate(val_loader):\n",
    "                    inputs = val_batch['video'].to(device)\n",
    "                    labels = val_batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    val_outputs = model(inputs)\n",
    "                    loss = criterion(val_outputs, labels)\n",
    "\n",
    "                    val_loss.update(loss, bs)\n",
    "                    val_acc.update(top1(val_outputs, labels), bs)\n",
    "            \"\"\"Finish Val mode\"\"\"\n",
    "\n",
    "            \"\"\"save model\"\"\"\n",
    "            if best_acc < val_acc.avg:\n",
    "                best_acc = val_acc.avg\n",
    "                is_best = True\n",
    "            else:\n",
    "                is_best = False\n",
    "                \n",
    "            save_checkpoint(model, is_best)\n",
    "            \n",
    "\n",
    "            pbar_epoch.set_postfix_str(\n",
    "                ' train_loss={:6.04f} , val_loss={:6.04f}, train_acc={:6.04f}, val_acc={:6.04f}'\n",
    "                ''.format(\n",
    "                    train_loss.avg,\n",
    "                    val_loss.avg,\n",
    "                    train_acc.avg,\n",
    "                    val_loss.avg)\n",
    "            )\n",
    "\n",
    "            # metrics = {\"train_accuracy\": train_acc.avg,\n",
    "            #            \"val_accuracy\": val_acc.avg\n",
    "            #            }\n",
    "            # experiment.log_multiple_metrics(metrics, epoch + 1)\n",
    "            experiment.log_metric(\"epoch_train_accuracy\",\n",
    "                                  train_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"epoch_train_loss\",\n",
    "                                  train_loss.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_accuracy\",\n",
    "                                  val_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_loss\",\n",
    "                                  val_loss.avg,\n",
    "                                  step=epoch + 1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "train_covnorm()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/kazukiomi/feeature-extract/e6ed346a2d414307878f4314bad310c3\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a61c92384793456099d5bb53698a552e"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a401e39b61714d57a284348f1c68c32e"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}