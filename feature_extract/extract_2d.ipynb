{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchinfo"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_org = torchvision.models.vgg16(pretrained=True)\n",
    "model_org = model_org.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# データセットに合わせてモデルの出力次元を変更すするため，オリジナルのモデルの出力層への入力次元を取得\n",
    "\n",
    "# model_org_features = model_org.classifier[6].in_features\n",
    "# print(model_org_features)\n",
    "# print(type(model_org_features))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4096\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "\n",
    "batch_size = 1\n",
    "torchinfo.summary(\n",
    "    model=model_org,\n",
    "    input_size=(batch_size, 3, 256, 256),\n",
    "    col_names=[\"input_size\",\n",
    "                \"output_size\"],\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=3 \n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Input Shape               Output Shape\n",
       "==========================================================================================\n",
       "VGG                                      --                        --\n",
       "├─Sequential (features)                  [1, 3, 256, 256]          [1, 512, 8, 8]\n",
       "│    └─Conv2d (0)                        [1, 3, 256, 256]          [1, 64, 256, 256]\n",
       "│    └─ReLU (1)                          [1, 64, 256, 256]         [1, 64, 256, 256]\n",
       "│    └─Conv2d (2)                        [1, 64, 256, 256]         [1, 64, 256, 256]\n",
       "│    └─ReLU (3)                          [1, 64, 256, 256]         [1, 64, 256, 256]\n",
       "│    └─MaxPool2d (4)                     [1, 64, 256, 256]         [1, 64, 128, 128]\n",
       "│    └─Conv2d (5)                        [1, 64, 128, 128]         [1, 128, 128, 128]\n",
       "│    └─ReLU (6)                          [1, 128, 128, 128]        [1, 128, 128, 128]\n",
       "│    └─Conv2d (7)                        [1, 128, 128, 128]        [1, 128, 128, 128]\n",
       "│    └─ReLU (8)                          [1, 128, 128, 128]        [1, 128, 128, 128]\n",
       "│    └─MaxPool2d (9)                     [1, 128, 128, 128]        [1, 128, 64, 64]\n",
       "│    └─Conv2d (10)                       [1, 128, 64, 64]          [1, 256, 64, 64]\n",
       "│    └─ReLU (11)                         [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    └─Conv2d (12)                       [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    └─ReLU (13)                         [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    └─Conv2d (14)                       [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    └─ReLU (15)                         [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    └─MaxPool2d (16)                    [1, 256, 64, 64]          [1, 256, 32, 32]\n",
       "│    └─Conv2d (17)                       [1, 256, 32, 32]          [1, 512, 32, 32]\n",
       "│    └─ReLU (18)                         [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    └─Conv2d (19)                       [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    └─ReLU (20)                         [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    └─Conv2d (21)                       [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    └─ReLU (22)                         [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    └─MaxPool2d (23)                    [1, 512, 32, 32]          [1, 512, 16, 16]\n",
       "│    └─Conv2d (24)                       [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    └─ReLU (25)                         [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    └─Conv2d (26)                       [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    └─ReLU (27)                         [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    └─Conv2d (28)                       [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    └─ReLU (29)                         [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    └─MaxPool2d (30)                    [1, 512, 16, 16]          [1, 512, 8, 8]\n",
       "├─AdaptiveAvgPool2d (avgpool)            [1, 512, 8, 8]            [1, 512, 7, 7]\n",
       "├─Sequential (classifier)                [1, 25088]                [1, 1000]\n",
       "│    └─Linear (0)                        [1, 25088]                [1, 4096]\n",
       "│    └─ReLU (1)                          [1, 4096]                 [1, 4096]\n",
       "│    └─Dropout (2)                       [1, 4096]                 [1, 4096]\n",
       "│    └─Linear (3)                        [1, 4096]                 [1, 4096]\n",
       "│    └─ReLU (4)                          [1, 4096]                 [1, 4096]\n",
       "│    └─Dropout (5)                       [1, 4096]                 [1, 4096]\n",
       "│    └─Linear (6)                        [1, 4096]                 [1, 1000]\n",
       "==========================================================================================\n",
       "Total params: 138,357,544\n",
       "Trainable params: 138,357,544\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 20.19\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 141.63\n",
       "Params size (MB): 553.43\n",
       "Estimated Total Size (MB): 695.85\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(dim)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)       \n",
    "        self.bn2 = nn.BatchNorm2d(dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.bn2(out)        \n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "class ReconstructNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.vgg16(pretrained=True)\n",
    "        model_num_features = model.classifier[6].in_features\n",
    "        num_class = 400\n",
    "\n",
    "        self.net_bottom_0 = nn.Sequential(\n",
    "            model.features[:17]\n",
    "        )\n",
    "\n",
    "        self.adapter = Adapter(256)\n",
    "\n",
    "        self.net_bottom_1 = nn.Sequential(\n",
    "            model.features[17:],\n",
    "            model.avgpool\n",
    "        )\n",
    "\n",
    "        self.net_top = nn.Sequential(\n",
    "            model.classifier[:6],\n",
    "            nn.Linear(model_num_features, num_class)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_bottom_0(x)\n",
    "        x = self.adapter(x)\n",
    "        x = self.net_bottom_1(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.net_top(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "model_new = ReconstructNet()\n",
    "model_new = model_new.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "torchinfo.summary(\n",
    "    model=model_new,\n",
    "    input_size=(batch_size, 3, 256, 256),\n",
    "    col_names=[\"input_size\",\n",
    "                \"output_size\"],\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=3 \n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Input Shape               Output Shape\n",
       "==========================================================================================\n",
       "ReconstructNet                           --                        --\n",
       "├─Sequential (net_bottom_0)              [1, 3, 256, 256]          [1, 256, 32, 32]\n",
       "│    └─Sequential (0)                    [1, 3, 256, 256]          [1, 256, 32, 32]\n",
       "│    │    └─Conv2d (0)                   [1, 3, 256, 256]          [1, 64, 256, 256]\n",
       "│    │    └─ReLU (1)                     [1, 64, 256, 256]         [1, 64, 256, 256]\n",
       "│    │    └─Conv2d (2)                   [1, 64, 256, 256]         [1, 64, 256, 256]\n",
       "│    │    └─ReLU (3)                     [1, 64, 256, 256]         [1, 64, 256, 256]\n",
       "│    │    └─MaxPool2d (4)                [1, 64, 256, 256]         [1, 64, 128, 128]\n",
       "│    │    └─Conv2d (5)                   [1, 64, 128, 128]         [1, 128, 128, 128]\n",
       "│    │    └─ReLU (6)                     [1, 128, 128, 128]        [1, 128, 128, 128]\n",
       "│    │    └─Conv2d (7)                   [1, 128, 128, 128]        [1, 128, 128, 128]\n",
       "│    │    └─ReLU (8)                     [1, 128, 128, 128]        [1, 128, 128, 128]\n",
       "│    │    └─MaxPool2d (9)                [1, 128, 128, 128]        [1, 128, 64, 64]\n",
       "│    │    └─Conv2d (10)                  [1, 128, 64, 64]          [1, 256, 64, 64]\n",
       "│    │    └─ReLU (11)                    [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    │    └─Conv2d (12)                  [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    │    └─ReLU (13)                    [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    │    └─Conv2d (14)                  [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    │    └─ReLU (15)                    [1, 256, 64, 64]          [1, 256, 64, 64]\n",
       "│    │    └─MaxPool2d (16)               [1, 256, 64, 64]          [1, 256, 32, 32]\n",
       "├─Adapter (adapter)                      [1, 256, 32, 32]          [1, 256, 32, 32]\n",
       "│    └─BatchNorm2d (bn1)                 [1, 256, 32, 32]          [1, 256, 32, 32]\n",
       "│    └─Conv2d (conv1)                    [1, 256, 32, 32]          [1, 256, 32, 32]\n",
       "│    └─BatchNorm2d (bn2)                 [1, 256, 32, 32]          [1, 256, 32, 32]\n",
       "├─Sequential (net_bottom_1)              [1, 256, 32, 32]          [1, 512, 7, 7]\n",
       "│    └─Sequential (0)                    [1, 256, 32, 32]          [1, 512, 8, 8]\n",
       "│    │    └─Conv2d (17)                  [1, 256, 32, 32]          [1, 512, 32, 32]\n",
       "│    │    └─ReLU (18)                    [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    │    └─Conv2d (19)                  [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    │    └─ReLU (20)                    [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    │    └─Conv2d (21)                  [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    │    └─ReLU (22)                    [1, 512, 32, 32]          [1, 512, 32, 32]\n",
       "│    │    └─MaxPool2d (23)               [1, 512, 32, 32]          [1, 512, 16, 16]\n",
       "│    │    └─Conv2d (24)                  [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    │    └─ReLU (25)                    [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    │    └─Conv2d (26)                  [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    │    └─ReLU (27)                    [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    │    └─Conv2d (28)                  [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    │    └─ReLU (29)                    [1, 512, 16, 16]          [1, 512, 16, 16]\n",
       "│    │    └─MaxPool2d (30)               [1, 512, 16, 16]          [1, 512, 8, 8]\n",
       "│    └─AdaptiveAvgPool2d (1)             [1, 512, 8, 8]            [1, 512, 7, 7]\n",
       "├─Sequential (net_top)                   [1, 25088]                [1, 400]\n",
       "│    └─Sequential (0)                    [1, 25088]                [1, 4096]\n",
       "│    │    └─Linear (0)                   [1, 25088]                [1, 4096]\n",
       "│    │    └─ReLU (1)                     [1, 4096]                 [1, 4096]\n",
       "│    │    └─Dropout (2)                  [1, 4096]                 [1, 4096]\n",
       "│    │    └─Linear (3)                   [1, 4096]                 [1, 4096]\n",
       "│    │    └─ReLU (4)                     [1, 4096]                 [1, 4096]\n",
       "│    │    └─Dropout (5)                  [1, 4096]                 [1, 4096]\n",
       "│    └─Linear (1)                        [1, 4096]                 [1, 400]\n",
       "==========================================================================================\n",
       "Total params: 135,966,160\n",
       "Trainable params: 135,966,160\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 20.25\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 147.92\n",
       "Params size (MB): 543.86\n",
       "Estimated Total Size (MB): 692.57\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# ダミーデータを用意し，出力が一致するか確認\n",
    "data = torch.randn(1, 3, 256, 256).to(device)\n",
    "print(data.shape)\n",
    "print(type(data))\n",
    "\n",
    "# data1 = torch.full((1,3,256,256), 2).to(device)\n",
    "# print(data1.shape)\n",
    "# print(type(data1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "model_org.eval()\n",
    "model_new.eval()\n",
    "output_org = model_org(data).max(axis=1)\n",
    "output_new = model_new(data).max(axis=1)\n",
    "print(output_org)\n",
    "print(output_new)\n",
    "# output_org = model_org(data)\n",
    "# output_new = model_new(data)\n",
    "# # print(output_org.shape)\n",
    "# print(output_new.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([6.0409], device='cuda:0', grad_fn=<MaxBackward0>),\n",
      "indices=tensor([556], device='cuda:0'))\n",
      "torch.return_types.max(\n",
      "values=tensor([0.4794], device='cuda:0', grad_fn=<MaxBackward0>),\n",
      "indices=tensor([62], device='cuda:0'))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# テンソルの出力のまま比較する場合\n",
    "# flag = torch.allclose(output_org,output_new, atol=1e-8)\n",
    "# print(flag)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 実際に学習させてみる\n",
    "- data：Kinetics400\n",
    "- model:vgg16 (Imagenetでpretrain)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import Ucf101, RandomClipSampler, UniformClipSampler, Kinetics\n",
    "\n",
    "# from torchvision.transforms._transforms_video import (\n",
    "#     CenterCropVideo,\n",
    "#     NormalizeVideo,\n",
    "# )\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/NAS-TVS872XT/dataset/'\n",
    "        self.root = self.metadata_path\n",
    "        self.annotation_path = self.metadata_path\n",
    "        self.FRAMES_PER_CLIP = 16\n",
    "        self.STEP_BETWEEN_CLIPS = 16\n",
    "        self.BATCH_SIZE = 16\n",
    "        self.NUM_WORKERS = 8  # kinetics:8, ucf101:24\n",
    "        # self.CLIP_DURATION = 16 / 25\n",
    "        self.CLIP_DURATION = (8 * 8) / 30  # (num_frames * sampling_rate)/fps\n",
    "        self.VIDEO_NUM_SUBSAMPLED = 8  # 事前学習済みモデルに合わせて16→8\n",
    "        self.UCF101_NUM_CLASSES = 101\n",
    "        self.KINETIC400_NUM_CLASSES = 400\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "\n",
    "def get_kinetics(subset):\n",
    "    \"\"\"\n",
    "    Kinetics400のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"val\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(size=256),\n",
    "                # RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                # CenterCropVideo(crop_size=(256, 256)),\n",
    "                CenterCrop(256),\n",
    "                # RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    root_kinetics = '/mnt/NAS-TVS872XT/dataset/Kinetics400/'\n",
    "\n",
    "    if subset == \"test\":\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + \"test_list.txt\",\n",
    "            video_path_prefix=root_kinetics + 'test/',\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + subset,\n",
    "            video_path_prefix=root_kinetics + subset,\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def make_loader(dataset):\n",
    "    \"\"\"\n",
    "    データローダーを作成\n",
    "\n",
    "    Args:\n",
    "        dataset (pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset): get_datasetメソッドで取得したdataset\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: 取得したデータローダー\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    loader = DataLoader(LimitDataset(dataset),\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = get_kinetics(\"train\")\n",
    "train_loader = make_loader(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "model = ReconstructNet()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(train_loader),\n",
    "                  total=len(train_loader),\n",
    "                  leave=True) as pbar_batch:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, batch in pbar_batch:\n",
    "                pbar_batch.set_description(\"[train]\")\n",
    "\n",
    "                inputs, targets = batch['video'].to(device), batch['label'].to(device)\n",
    "                bs = inputs.size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, targets), bs)\n",
    "\n",
    "                pbar_batch.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                    train_loss.val, train_acc.val,\n",
    "                ))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}