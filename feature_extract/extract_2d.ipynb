{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchinfo"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_org = torchvision.models.resnet152(pretrained=True)\n",
    "# model_org = model_org.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# データセットに合わせてモデルの出力次元を変更すするため，オリジナルのモデルの出力層への入力次元を取得\n",
    "\n",
    "# model_org_features = model_org.classifier[6].in_features\n",
    "# print(model_org_features)\n",
    "# print(type(model_org_features))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "# batch_size = 1\n",
    "# torchinfo.summary(\n",
    "#     model=model_org,\n",
    "#     input_size=(batch_size, 3, 256, 256),\n",
    "#     col_names=[\"input_size\",\n",
    "#                 \"output_size\"],\n",
    "#     row_settings=[\"var_names\"],\n",
    "#     depth=3 \n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/NAS-TVS872XT/dataset/'\n",
    "        self.root = self.metadata_path\n",
    "        self.annotation_path = self.metadata_path\n",
    "        self.FRAMES_PER_CLIP = 16\n",
    "        self.STEP_BETWEEN_CLIPS = 16\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.NUM_WORKERS = 32  \n",
    "        # self.CLIP_DURATION = 16 / 25\n",
    "        self.CLIP_DURATION = (8 * 8) / 30  # (num_frames * sampling_rate)/fps\n",
    "        self.VIDEO_NUM_SUBSAMPLED = 2  # 事前学習済みモデルに合わせて16→8\n",
    "        self.UCF101_NUM_CLASSES = 101\n",
    "        self.KINETIC400_NUM_CLASSES = 400\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(dim)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)       \n",
    "        self.bn2 = nn.BatchNorm2d(dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.bn2(out)        \n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class FrameAvg(nn.Module):\n",
    "    def __init__(self, batch_size, num_frame):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_frame = num_frame\n",
    "\n",
    "\n",
    "    def frame_out_to_video_out(self, output: torch.Tensor, batch_size, num_frame) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        フレームごとの出力をビデオとしての出力に変換\n",
    "        Args:\n",
    "            output (torch.Tensor): フレームごとの出力\n",
    "            batch_size (int): バッチサイズ\n",
    "            num_frame (int): フレーム数\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: ビデオとしての出力\n",
    "        \"\"\"\n",
    "        video_output_list = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            video_output = output[i*num_frame:(i+1)*num_frame]\n",
    "            video_output = torch.mean(video_output, dim=0)\n",
    "            video_output_list.append(video_output)\n",
    "        \n",
    "        new_output =  torch.stack(video_output_list, dim=0)\n",
    "        return new_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frame_out_to_video_out(x, self.batch_size, self.num_frame)\n",
    "        return x\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class ReconstructNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.resnet152(pretrained=True)\n",
    "        model_num_features = model.fc.in_features\n",
    "        num_class = 101\n",
    "\n",
    "        self.net_bottom = nn.Sequential(\n",
    "            model.conv1,\n",
    "            model.bn1,\n",
    "            model.relu,\n",
    "            model.maxpool\n",
    "        )\n",
    "        \n",
    "        self.layer1 = model.layer1\n",
    "        self.layer2 = model.layer2\n",
    "        self.layer3 = model.layer3\n",
    "        self.layer4 = model.layer4\n",
    "        self.avgpool = model.avgpool\n",
    "\n",
    "        # self.adapter = Adapter(512)\n",
    "\n",
    "        self.net_top = nn.Sequential(\n",
    "            nn.Linear(model_num_features, num_class)\n",
    "        )\n",
    "\n",
    "        self.frame_avg = FrameAvg(batch_size=args.BATCH_SIZE,\n",
    "                                num_frame=args.VIDEO_NUM_SUBSAMPLED)\n",
    "        \n",
    "        # 学習させるパラメータ名\n",
    "        self.update_param_names = [\"adapter.bn1.weight\", \"adapter.bn1.bias\",\n",
    "                            \"adapter.conv1.weight\", \"adapter.conv1.bias\",\n",
    "                            \"adapter.bn2.weight\", \"adapter.bn2.bias\", \n",
    "                            \"net_top.0.weight\", \"net_top.0.bias\"]\n",
    "        # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.update_param_names:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_bottom(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        # x = self.adapter(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.net_top(x)\n",
    "        x = self.frame_avg(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# model_new = ReconstructNet()\n",
    "# model_new = model_new.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# batch_size = args.BATCH_SIZE*args.VIDEO_NUM_SUBSAMPLED\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     model=model_new,\n",
    "#     input_size=(batch_size, 3, 256, 256),\n",
    "#     col_names=[\"input_size\",\n",
    "#                 \"output_size\"],\n",
    "#     row_settings=[\"var_names\"],\n",
    "#     depth=3 \n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# # ダミーデータを用意し，出力が一致するか確認\n",
    "# data = torch.randn(1, 3, 256, 256).to(device)\n",
    "# print(data.shape)\n",
    "# print(type(data))\n",
    "\n",
    "# # data1 = torch.full((1,3,256,256), 2).to(device)\n",
    "# # print(data1.shape)\n",
    "# # print(type(data1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# model_org.eval()\n",
    "# model_new.eval()\n",
    "# output_org = model_org(data).max(axis=1)\n",
    "# output_new = model_new(data).max(axis=1)\n",
    "# print(output_org)\n",
    "# print(output_new)\n",
    "# output_org = model_org(data)\n",
    "# output_new = model_new(data)\n",
    "# # print(output_org.shape)\n",
    "# print(output_new.shape)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# テンソルの出力のまま比較する場合\n",
    "# flag = torch.allclose(output_org,output_new, atol=1e-8)\n",
    "# print(flag)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 実際に学習させてみる\n",
    "- data：Kinetics400\n",
    "- model:vgg16 (Imagenetでpretrain)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import Ucf101, RandomClipSampler, UniformClipSampler, Kinetics\n",
    "\n",
    "# from torchvision.transforms._transforms_video import (\n",
    "#     CenterCropVideo,\n",
    "#     NormalizeVideo,\n",
    "# )\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from comet_ml import Experiment"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_ucf101(subset):\n",
    "    \"\"\"\n",
    "    ucf101のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    subset_root_Ucf101 = 'ucfTrainTestlist/trainlist01.txt'\n",
    "    if subset == \"test\":\n",
    "        subset_root_Ucf101 = 'ucfTrainTestlist/testlist.txt'\n",
    "\n",
    "    args = Args()\n",
    "    transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x-1),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    root_ucf101 = '/mnt/dataset/UCF101/'\n",
    "\n",
    "    dataset = Ucf101(\n",
    "        data_path=root_ucf101 + subset_root_Ucf101,\n",
    "        video_path_prefix=root_ucf101 + 'video/',\n",
    "        clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "        video_sampler=RandomSampler,\n",
    "        decode_audio=False,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "\n",
    "def get_kinetics(subset):\n",
    "    \"\"\"\n",
    "    Kinetics400のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"val\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(size=256),\n",
    "                # RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                # CenterCropVideo(crop_size=(256, 256)),\n",
    "                CenterCrop(256),\n",
    "                # RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    root_kinetics = '/mnt/NAS-TVS872XT/dataset/Kinetics400/'\n",
    "\n",
    "    if subset == \"test\":\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + \"test_list.txt\",\n",
    "            video_path_prefix=root_kinetics + 'test/',\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + subset,\n",
    "            video_path_prefix=root_kinetics + subset,\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def make_loader(dataset):\n",
    "    \"\"\"\n",
    "    データローダーを作成\n",
    "\n",
    "    Args:\n",
    "        dataset (pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset): get_datasetメソッドで取得したdataset\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: 取得したデータローダー\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    loader = DataLoader(LimitDataset(dataset),\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def make_new_batch(inputs, labels):\n",
    "    \"\"\"\n",
    "    動画データを画像データに分割\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): inputs\n",
    "        labels (torch.Tensor): labels\n",
    "\n",
    "    Returns:\n",
    "        new_inputs torch.Tensor: new_inputs\n",
    "        new_labels torch.Tensor: new_labels\n",
    "    \"\"\"\n",
    "\n",
    "    num_frame = inputs.size()[2]\n",
    "    inputs = inputs.permute(0,2,1,3,4)\n",
    "    video_data_list = []\n",
    "    for i in range(inputs.size()[0]):\n",
    "        video_data_list.append(inputs[i])\n",
    "    new_inputs = torch.cat(video_data_list, dim=0)\n",
    "\n",
    "    label_list = []\n",
    "\n",
    "    for i in range(labels.size()[0]):\n",
    "        target_id = labels[i].item()\n",
    "        label = torch.full((1,num_frame), target_id)\n",
    "        label_list.append(label)\n",
    "    new_labels = torch.cat(label_list, dim=1)\n",
    "    new_labels = torch.squeeze(new_labels)\n",
    "\n",
    "    return new_inputs, new_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "dataset = get_ucf101(\"train\")\n",
    "# dataset.video_sampler._num_samples = 100\n",
    "train_loader = make_loader(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torchinfoのNon-trainable params\n",
    "- torchinfoで表示させる前に表示させいたいモデルをGPUに送るとNon-trainable paramsが正しく表示されない\n",
    "- またtorchinfoで表示させる際にデフォルトでcuda:0にモデルが送られるため，cuda:0以外にモデルを送っているとうまくいかない（torchinfoで実際にダミーデータをモデルに入れて出力のサイズとかを測っているからモデルをGPUに送っていてそれがデフォルトでcuda:0なのかも）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model = ReconstructNet()\n",
    "model = model.to(device)\n",
    "# print(device)\n",
    "\n",
    "# if device == \"cuda\":\n",
    "#     print(\"test\")\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "#     cudnn.benchmark = True\n",
    "#     print(\"test\")\n",
    "\n",
    "# model = torch.nn.DataParallel(model)\n",
    "# model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "# torch.backends.cudnn.benchmark = True\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "size = args.BATCH_SIZE*args.VIDEO_NUM_SUBSAMPLED\n",
    "\n",
    "torchinfo.summary(\n",
    "    model=model,\n",
    "    input_size=(size, 3, 256, 256),\n",
    "    col_names=[\"input_size\",\n",
    "                \"output_size\"],\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=2 \n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Input Shape               Output Shape\n",
       "==========================================================================================\n",
       "ReconstructNet                           --                        --\n",
       "├─Sequential (net_bottom)                [64, 3, 256, 256]         [64, 64, 64, 64]\n",
       "│    └─Conv2d (0)                        [64, 3, 256, 256]         [64, 64, 128, 128]\n",
       "│    └─BatchNorm2d (1)                   [64, 64, 128, 128]        [64, 64, 128, 128]\n",
       "│    └─ReLU (2)                          [64, 64, 128, 128]        [64, 64, 128, 128]\n",
       "│    └─MaxPool2d (3)                     [64, 64, 128, 128]        [64, 64, 64, 64]\n",
       "├─Sequential (layer1)                    [64, 64, 64, 64]          [64, 256, 64, 64]\n",
       "│    └─Bottleneck (0)                    [64, 64, 64, 64]          [64, 256, 64, 64]\n",
       "│    └─Bottleneck (1)                    [64, 256, 64, 64]         [64, 256, 64, 64]\n",
       "│    └─Bottleneck (2)                    [64, 256, 64, 64]         [64, 256, 64, 64]\n",
       "├─Sequential (layer2)                    [64, 256, 64, 64]         [64, 512, 32, 32]\n",
       "│    └─Bottleneck (0)                    [64, 256, 64, 64]         [64, 512, 32, 32]\n",
       "│    └─Bottleneck (1)                    [64, 512, 32, 32]         [64, 512, 32, 32]\n",
       "│    └─Bottleneck (2)                    [64, 512, 32, 32]         [64, 512, 32, 32]\n",
       "│    └─Bottleneck (3)                    [64, 512, 32, 32]         [64, 512, 32, 32]\n",
       "│    └─Bottleneck (4)                    [64, 512, 32, 32]         [64, 512, 32, 32]\n",
       "│    └─Bottleneck (5)                    [64, 512, 32, 32]         [64, 512, 32, 32]\n",
       "│    └─Bottleneck (6)                    [64, 512, 32, 32]         [64, 512, 32, 32]\n",
       "│    └─Bottleneck (7)                    [64, 512, 32, 32]         [64, 512, 32, 32]\n",
       "├─Sequential (layer3)                    [64, 512, 32, 32]         [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (0)                    [64, 512, 32, 32]         [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (1)                    [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (2)                    [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (3)                    [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (4)                    [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (5)                    [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (6)                    [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (7)                    [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (8)                    [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (9)                    [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (10)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (11)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (12)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (13)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (14)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (15)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (16)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (17)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (18)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (19)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (20)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (21)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (22)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (23)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (24)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (25)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (26)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (27)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (28)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (29)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (30)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (31)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (32)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (33)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (34)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "│    └─Bottleneck (35)                   [64, 1024, 16, 16]        [64, 1024, 16, 16]\n",
       "├─Sequential (layer4)                    [64, 1024, 16, 16]        [64, 2048, 8, 8]\n",
       "│    └─Bottleneck (0)                    [64, 1024, 16, 16]        [64, 2048, 8, 8]\n",
       "│    └─Bottleneck (1)                    [64, 2048, 8, 8]          [64, 2048, 8, 8]\n",
       "│    └─Bottleneck (2)                    [64, 2048, 8, 8]          [64, 2048, 8, 8]\n",
       "├─AdaptiveAvgPool2d (avgpool)            [64, 2048, 8, 8]          [64, 2048, 1, 1]\n",
       "├─Sequential (net_top)                   [64, 2048]                [64, 101]\n",
       "│    └─Linear (0)                        [64, 2048]                [64, 101]\n",
       "├─FrameAvg (frame_avg)                   [64, 101]                 [32, 101]\n",
       "==========================================================================================\n",
       "Total params: 58,350,757\n",
       "Trainable params: 206,949\n",
       "Non-trainable params: 58,143,808\n",
       "Total mult-adds (G): 962.30\n",
       "==========================================================================================\n",
       "Input size (MB): 50.33\n",
       "Forward/backward pass size (MB): 30165.49\n",
       "Params size (MB): 233.40\n",
       "Estimated Total Size (MB): 30449.22\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# torch.backends.cudnn.version()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# パラメータの名前を確認する\n",
    "# for name, params in model.named_parameters():\n",
    "#     print(name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# # 転移学習で学習させるパラメータを、変数params_to_updateに格納する\n",
    "# # optimizerの引数にparams_to_updataを入れることもできる\n",
    "# params_to_update = []\n",
    "\n",
    "# # 学習させるパラメータ名\n",
    "# update_param_names = [\"adapter.bn1.weight\", \"adapter.bn1.bias\",\n",
    "#                       \"adapter.conv1.weight\", \"adapter.conv1.bias\",\n",
    "#                       \"adapter.bn2.weight\", \"adapter.bn2.bias\", \n",
    "#                       \"net_top.0.weight\", \"net_top.0.bias\"]\n",
    "\n",
    "# # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "# for name, param in model.named_parameters():\n",
    "#     if name in update_param_names:\n",
    "#         param.requires_grad = True\n",
    "#         params_to_update.append(param)\n",
    "#         print(name)\n",
    "#     else:\n",
    "#         param.requires_grad = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     param.requires_grad = True\n",
    "#     print(type(param))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "hyper_params = {\n",
    "    \"Dataset\": \"UCF101\",\n",
    "    \"batch_size\": args.BATCH_SIZE,\n",
    "    \"num_frame:\":args.VIDEO_NUM_SUBSAMPLED,\n",
    "}\n",
    "\n",
    "experiment = Experiment(\n",
    "    api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "    project_name=\"feeature-extract\",\n",
    "    workspace=\"kazukiomi\",\n",
    ")\n",
    "\n",
    "experiment.add_tag('pytorch')\n",
    "experiment.log_parameters(hyper_params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/kazukiomi/feeature-extract/29f848e426384e5fa2eeeaeffe28cc42\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "num_epochs = 4\n",
    "\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "step = 0\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(train_loader),\n",
    "                  total=len(train_loader),\n",
    "                  leave=True) as pbar_batch:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "\n",
    "            for batch_idx, batch in pbar_batch:\n",
    "                pbar_batch.set_description(\"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                inputs = batch['video'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                new_inputs, new_labels = make_new_batch(inputs, labels)\n",
    "                new_inputs = new_inputs.to(device)\n",
    "                new_labels = new_labels.to(device)\n",
    "                bs = inputs.size(0)\n",
    "                new_bs = new_inputs.size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(new_inputs)\n",
    "                # print(outputs.device)\n",
    "                # print(new_labels.device)\n",
    "\n",
    "                # ここでフレームごとの出力をビデオごとの出力に変換する\n",
    "                # video_outputs = frame_out_to_video_out(outputs, bs, args.VIDEO_NUM_SUBSAMPLED) \n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                \n",
    "                preds = torch.squeeze(outputs.max(dim=1)[1])\n",
    "                # print(video_outputs.shape)\n",
    "                # print(preds.shape)\n",
    "\n",
    "                # acc = (preds == labels).float().mean().item()\n",
    "                # acc_list.append(acc)\n",
    "                # pbar_batch.set_postfix(OrderedDict(loss=loss.item(),acc=acc))\n",
    "\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, labels), bs)\n",
    "\n",
    "                pbar_batch.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                    train_loss.val, train_acc.val,\n",
    "                ))\n",
    "\n",
    "                experiment.log_metric(\"accuracy\", train_acc.val, step=step)\n",
    "                step += 1\n",
    "\n",
    "            acc_list.append(train_acc.avg)\n",
    "            loss_list.append(train_loss.avg)\n",
    "        pbar_epoch.set_postfix(OrderedDict(\n",
    "            acc=sum(acc_list)/len(acc_list),\n",
    "            loss=sum(loss_list)/len(loss_list)\n",
    "        ))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f86d6898c794ca1a6e9698dd78d4ee3"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26c1e1c10e1049c29296b69e334b07c4"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=298.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd87e5c3f0dd4dd2a076e82582d35d4e"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=298.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68893a6c94a84e99bf787f4f9fb9c7e2"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=298.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0613ddb83f9d40f199fd630799bd565f"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=298.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-79aa00466319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# pbar_batch.set_postfix(OrderedDict(loss=loss.item(),acc=acc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-90c01b1a4c9a>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, val, n)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Process Process-103:\n",
      "Process Process-121:\n",
      "Process Process-101:\n",
      "Process Process-123:\n",
      "Process Process-126:\n",
      "Process Process-105:\n",
      "Process Process-128:\n",
      "Process Process-125:\n",
      "Process Process-124:\n",
      "Process Process-100:\n",
      "Process Process-122:\n",
      "Process Process-104:\n",
      "Process Process-99:\n",
      "Process Process-127:\n",
      "Process Process-98:\n",
      "Process Process-97:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/kazukiomi/feeature-extract/29f848e426384e5fa2eeeaeffe28cc42\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     accuracy [1046] : (0.0, 0.96875)\n",
      "COMET INFO:     loss [105]      : (0.560932993888855, 10.786465644836426)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Dataset    : UCF101\n",
      "COMET INFO:     batch_size : 32\n",
      "COMET INFO:     num_frame: : 2\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (46.06 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "experiment.end()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/kazukiomi/feeature-extract/cf63b0cbf43c41ad885bdbb745318859\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     accuracy [1192] : (0.0, 0.9375)\n",
      "COMET INFO:     loss [120]      : (0.19330792129039764, 13.611698150634766)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Dataset    : UCF101\n",
      "COMET INFO:     batch_size : 32\n",
      "COMET INFO:     num_frame: : 2\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (27.87 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}