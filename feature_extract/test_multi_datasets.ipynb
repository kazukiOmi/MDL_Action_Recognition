{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchinfo\n",
    "import pytorchvideo\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import (\n",
    "    Ucf101,\n",
    "    RandomClipSampler,\n",
    "    UniformClipSampler,\n",
    "    Kinetics\n",
    ")\n",
    "\n",
    "# from torchvision.transforms._transforms_video import (\n",
    "#     CenterCropVideo,\n",
    "#     NormalizeVideo,\n",
    "# )\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import os\n",
    "import argparse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/NAS-TVS872XT/dataset/'\n",
    "        self.root = self.metadata_path\n",
    "        self.annotation_path = self.metadata_path\n",
    "        self.NUM_EPOCH = 4\n",
    "        self.FRAMES_PER_CLIP = 16\n",
    "        self.STEP_BETWEEN_CLIPS = 16\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.NUM_WORKERS = 32\n",
    "        # self.CLIP_DURATION = 16 / 25\n",
    "        self.CLIP_DURATION = (8 * 8) / 30  # (num_frames * sampling_rate)/fps\n",
    "        self.VIDEO_NUM_SUBSAMPLED = 8\n",
    "        self.UCF101_NUM_CLASSES = 101\n",
    "        self.KINETIC400_NUM_CLASSES = 400\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "        # self.num_videos = make_num_videos(self.dataset)\n",
    "        self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "class WrapperDataset(pytorchvideo.data.LabeledVideoDataset):\n",
    "    def __init__(self, dataset, labeld_video_paths, clip_sampler):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labeled_video_paths = dataset.labeled_video_paths\n",
    "        self.clip_sampler = dataset.clip_sampler\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "        # self.num_videos = make_num_videos(self.dataset)\n",
    "        self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# class WrapperDataset(pytorchvideo.data.LabeledVideoDataset):\n",
    "#     def __init__(self, dataset):\n",
    "#         super().__init__()\n",
    "#         self.dataset = dataset\n",
    "#         self.labeled_video_paths = dataset.labeled_video_paths\n",
    "#         self.clip_sampler = dataset.clip_sampler\n",
    "#         self.dataset_iter = itertools.chain.from_iterable(\n",
    "#             itertools.repeat(iter(dataset), 2)\n",
    "#         )\n",
    "#         # self.num_videos = make_num_videos(self.dataset)\n",
    "#         self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return next(self.dataset_iter)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def get_ucf101(subset):\n",
    "    \"\"\"\n",
    "    ucf101のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    subset_root_Ucf101 = 'ucfTrainTestlist/trainlist01.txt'\n",
    "    if subset == \"test\":\n",
    "        subset_root_Ucf101 = 'ucfTrainTestlist/testlist.txt'\n",
    "\n",
    "    args = Args()\n",
    "    transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x - 1),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    # root_ucf101 = '/mnt/dataset/UCF101/'\n",
    "    root_ucf101 = '/mnt/dataset/UCF101/'\n",
    "\n",
    "    dataset = Ucf101(\n",
    "        data_path=root_ucf101 + subset_root_Ucf101,\n",
    "        video_path_prefix=root_ucf101 + 'video/',\n",
    "        clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "        video_sampler=RandomSampler,\n",
    "        decode_audio=False,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def get_kinetics(subset):\n",
    "    \"\"\"\n",
    "    Kinetics400のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"val\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(size=256),\n",
    "                # RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                # CenterCropVideo(crop_size=(256, 256)),\n",
    "                CenterCrop(256),\n",
    "                # RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    root_kinetics = '/mnt/dataset/Kinetics400/'\n",
    "\n",
    "    if subset == \"test\":\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + \"test_list.txt\",\n",
    "            video_path_prefix=root_kinetics + 'test/',\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + subset,\n",
    "            video_path_prefix=root_kinetics + subset,\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def make_loader(dataset):\n",
    "    \"\"\"\n",
    "    データローダーを作成\n",
    "\n",
    "    Args:\n",
    "        dataset (pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset): get_datasetメソッドで取得したdataset\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: 取得したデータローダー\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    loader = DataLoader(LimitDataset(dataset),\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def make_multi_loader(dataset):\n",
    "    args = Args()\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "ucf_dataset = get_ucf101(\"test\")\n",
    "kinetics_dataset = get_kinetics(\"test\")\n",
    "print(ucf_dataset.num_videos)\n",
    "print(kinetics_dataset.num_videos)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3783\n",
      "35357\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py\n",
    "\n",
    "```python\n",
    "class ChainDataset(IterableDataset):\n",
    "    r\"\"\"Dataset for chaining multiple :class:`IterableDataset` s.\n",
    "    This class is useful to assemble different existing dataset streams. The\n",
    "    chaining operation is done on-the-fly, so concatenating large-scale\n",
    "    datasets with this class will be efficient.\n",
    "    Args:\n",
    "        datasets (iterable of IterableDataset): datasets to be chained together\n",
    "    \"\"\"\n",
    "    def __init__(self, datasets: Iterable[Dataset]) -> None:\n",
    "        super(ChainDataset, self).__init__()\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __iter__(self):\n",
    "        for d in self.datasets:\n",
    "            assert isinstance(d, IterableDataset), \"ChainDataset only supports IterableDataset\"\n",
    "            for x in d:\n",
    "                yield x\n",
    "\n",
    "    def __len__(self):\n",
    "        total = 0\n",
    "        for d in self.datasets:\n",
    "            assert isinstance(d, IterableDataset), \"ChainDataset only supports IterableDataset\"\n",
    "            total += len(d)\n",
    "        return total\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "chain_datasets = ucf_dataset + kinetics_dataset\n",
    "print(type(chain_datasets))\n",
    "concat_datasets = LimitDataset(ucf_dataset) + LimitDataset(kinetics_dataset)\n",
    "print(type(concat_datasets))\n",
    "wrapper_datasets = WrapperDataset(ucf_dataset, labeld_video_paths=\"a\", clip_sampler=\"b\") + WrapperDataset(kinetics_dataset, labeld_video_paths=\"a\", clip_sampler=\"b\")\n",
    "print(type(wrapper_datasets))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.utils.data.dataset.ChainDataset'>\n",
      "<class 'torch.utils.data.dataset.ConcatDataset'>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'labeled_video_paths' and 'clip_sampler'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-3789544f7c2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconcat_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLimitDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mucf_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLimitDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkinetics_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwrapper_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWrapperDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mucf_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeld_video_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mWrapperDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkinetics_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeld_video_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-d749dcb1545c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, labeld_video_paths, clip_sampler)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWrapperDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorchvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabeledVideoDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeld_video_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeled_video_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeled_video_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'labeled_video_paths' and 'clip_sampler'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(type(chain_datasets.datasets))\n",
    "print(type(chain_datasets.datasets[0]))\n",
    "print(chain_datasets.__len__())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n",
      "<class 'pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset'>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'LabeledVideoDataset' has no len()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dca2050d37c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ChainDataset only supports IterableDataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;31m# Cannot verify that all self.datasets are Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'LabeledVideoDataset' has no len()"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "print(type(concat_datasets.datasets))\n",
    "print(type(concat_datasets.datasets[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n",
      "<class '__main__.LimitDataset'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "chain_loader = make_multi_loader(chain_datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "concat_loader = make_multi_loader(concat_datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "chain_batch = iter(chain_loader).__next__()\n",
    "print(type(chain_batch))\n",
    "chain_inputs = chain_batch[\"video\"]\n",
    "# print(type(chain_inputs))\n",
    "print(chain_inputs.shape)\n",
    "chain_labels = chain_batch[\"label\"]\n",
    "# print(type(chain_labels))\n",
    "print(chain_labels.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "concat_batch = iter(concat_loader).__next__()\n",
    "print(type(concat_batch))\n",
    "concat_inputs = concat_batch[\"video\"]\n",
    "# print(type(concat_inputs))\n",
    "print(concat_inputs.shape)\n",
    "concat_labels = concat_batch[\"label\"]\n",
    "# print(type(concat_labels))\n",
    "print(concat_labels.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "class MyUcf101(pytorchvideo.data.LabeledVideoDataset):\n",
    "\n",
    "    def __next__(self):\n",
    "        dict = super().__next__()\n",
    "        dict[\"dataset\"] = \"ucf101\"\n",
    "        return dict\n",
    "\n",
    "class MyKinetics(pytorchvideo.data.LabeledVideoDataset):\n",
    "\n",
    "    def __next__(self):\n",
    "        dict = super().__next__()\n",
    "        dict[\"dataset\"] = \"kinetics\"\n",
    "        return dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## torchvision.dataset.DatasetNameとpytorchvicdo.data.LaveldVideoDatasetの違い\n",
    "- torchvisionはクラスを呼び出しているのでtorchvisionのクラスを継承したクラスを定義して新たに定義したクラスにtorchvisionと同じ引数を与えて呼び出せる\n",
    "- pytorchvideoはメソッドを呼び出して返り値がクラスなのでwrapperのようなことをするしかない？\n",
    "- LimitDatasetの___getitem__()でデータセットの名前も同時に返すとか（dictに入らないしデータセットごとにLimitDatasetクラスを用意しなければならない）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "my_ucf_dataset = MyUcf101(ucf_dataset)\n",
    "my_kinetics_dataset = MyKinetics(kinetics_dataset)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'clip_sampler'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-58dd7329d3a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_ucf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyUcf101\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mucf_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmy_kinetics_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyKinetics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkinetics_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'clip_sampler'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}