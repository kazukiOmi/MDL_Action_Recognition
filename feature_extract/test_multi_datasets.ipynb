{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchinfo\n",
    "import pytorchvideo\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import (\n",
    "    Ucf101,\n",
    "    RandomClipSampler,\n",
    "    UniformClipSampler,\n",
    "    Kinetics\n",
    ")\n",
    "\n",
    "# from torchvision.transforms._transforms_video import (\n",
    "#     CenterCropVideo,\n",
    "#     NormalizeVideo,\n",
    "# )\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import os\n",
    "import argparse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/NAS-TVS872XT/dataset/'\n",
    "        self.root = self.metadata_path\n",
    "        self.annotation_path = self.metadata_path\n",
    "        self.NUM_EPOCH = 4\n",
    "        self.FRAMES_PER_CLIP = 16\n",
    "        self.STEP_BETWEEN_CLIPS = 16\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.NUM_WORKERS = 32\n",
    "        # self.CLIP_DURATION = 16 / 25\n",
    "        self.CLIP_DURATION = (8 * 8) / 30  # (num_frames * sampling_rate)/fps\n",
    "        self.VIDEO_NUM_SUBSAMPLED = 8\n",
    "        self.UCF101_NUM_CLASSES = 101\n",
    "        self.KINETIC400_NUM_CLASSES = 400\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "        # self.num_videos = make_num_videos(self.dataset)\n",
    "        self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return next(self.dataset_iter), \"kinetics\"\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# class KineticsLimitDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, dataset):\n",
    "#         super().__init__()\n",
    "#         self.dataset = dataset\n",
    "#         self.dataset_iter = itertools.chain.from_iterable(\n",
    "#             itertools.repeat(iter(dataset), 2)\n",
    "#         )\n",
    "#         # self.num_videos = make_num_videos(self.dataset)\n",
    "#         self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return next(self.dataset_iter), \"kinetics\"\n",
    "    \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class KineticsLimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "        # self.num_videos = make_num_videos(self.dataset)\n",
    "        self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        dict = next(self.dataset_iter)\n",
    "        dict[\"dataset_name\"] = \"kinetics\"\n",
    "        return dict\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# class Ucf101LimitDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, dataset):\n",
    "#         super().__init__()\n",
    "#         self.dataset = dataset\n",
    "#         self.dataset_iter = itertools.chain.from_iterable(\n",
    "#             itertools.repeat(iter(dataset), 2)\n",
    "#         )\n",
    "#         # self.num_videos = make_num_videos(self.dataset)\n",
    "#         self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return next(self.dataset_iter), \"Ucf101\"\n",
    "    \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class Ucf101LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "        # self.num_videos = make_num_videos(self.dataset)\n",
    "        self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        dict = next(self.dataset_iter)\n",
    "        dict[\"dataset_name\"] = \"ucf101\"\n",
    "        return dict\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# class WrapperDataset(pytorchvideo.data.LabeledVideoDataset):\n",
    "#     def __init__(self, dataset, labeld_video_paths, clip_sampler):\n",
    "#         super().__init__()\n",
    "#         self.dataset = dataset\n",
    "#         self.labeled_video_paths = dataset.labeled_video_paths\n",
    "#         self.clip_sampler = dataset.clip_sampler\n",
    "#         self.dataset_iter = itertools.chain.from_iterable(\n",
    "#             itertools.repeat(iter(dataset), 2)\n",
    "#         )\n",
    "#         # self.num_videos = make_num_videos(self.dataset)\n",
    "#         self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return next(self.dataset_iter)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# class WrapperDataset(pytorchvideo.data.LabeledVideoDataset):\n",
    "#     def __init__(self, dataset):\n",
    "#         super().__init__()\n",
    "#         self.dataset = dataset\n",
    "#         self.labeled_video_paths = dataset.labeled_video_paths\n",
    "#         self.clip_sampler = dataset.clip_sampler\n",
    "#         self.dataset_iter = itertools.chain.from_iterable(\n",
    "#             itertools.repeat(iter(dataset), 2)\n",
    "#         )\n",
    "#         # self.num_videos = make_num_videos(self.dataset)\n",
    "#         self.num_videos = len(self.dataset.video_sampler)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return next(self.dataset_iter)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_ucf101(subset):\n",
    "    \"\"\"\n",
    "    ucf101のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    subset_root_Ucf101 = 'ucfTrainTestlist/trainlist01.txt'\n",
    "    if subset == \"test\":\n",
    "        subset_root_Ucf101 = 'ucfTrainTestlist/testlist.txt'\n",
    "\n",
    "    args = Args()\n",
    "    transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(size=256),\n",
    "                # RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                # CenterCropVideo(crop_size=(256, 256)),\n",
    "                CenterCrop(256),\n",
    "                # RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x - 1),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    # root_ucf101 = '/mnt/dataset/UCF101/'\n",
    "    root_ucf101 = '/mnt/dataset/UCF101/'\n",
    "\n",
    "    dataset = Ucf101(\n",
    "        data_path=root_ucf101 + subset_root_Ucf101,\n",
    "        video_path_prefix=root_ucf101 + 'video/',\n",
    "        clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "        video_sampler=RandomSampler,\n",
    "        decode_audio=False,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_kinetics(subset):\n",
    "    \"\"\"\n",
    "    Kinetics400のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"val\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(size=256),\n",
    "                # RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                # CenterCropVideo(crop_size=(256, 256)),\n",
    "                CenterCrop(256),\n",
    "                # RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    root_kinetics = '/mnt/dataset/Kinetics400/'\n",
    "\n",
    "    if subset == \"test\":\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + \"test_list.txt\",\n",
    "            video_path_prefix=root_kinetics + 'test/',\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + subset,\n",
    "            video_path_prefix=root_kinetics + subset,\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def make_loader(dataset):\n",
    "    \"\"\"\n",
    "    データローダーを作成\n",
    "\n",
    "    Args:\n",
    "        dataset (pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset): get_datasetメソッドで取得したdataset\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: 取得したデータローダー\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    loader = DataLoader(LimitDataset(dataset),\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def make_multi_loader(dataset):\n",
    "    args = Args()\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "ucf_dataset = get_ucf101(\"test\")\n",
    "# ucf_dataset.video_sampler._num_samples = 100\n",
    "kinetics_dataset = get_kinetics(\"test\")\n",
    "print(ucf_dataset.num_videos)\n",
    "print(kinetics_dataset.num_videos)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3783\n",
      "35357\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py\n",
    "\n",
    "\n",
    "\n",
    "```python:dataset.py\n",
    "class ChainDataset(IterableDataset):\n",
    "    r\"\"\"Dataset for chaining multiple :class:`IterableDataset` s.\n",
    "    This class is useful to assemble different existing dataset streams. The\n",
    "    chaining operation is done on-the-fly, so concatenating large-scale\n",
    "    datasets with this class will be efficient.\n",
    "    Args:\n",
    "        datasets (iterable of IterableDataset): datasets to be chained together\n",
    "    \"\"\"\n",
    "    def __init__(self, datasets: Iterable[Dataset]) -> None:\n",
    "        super(ChainDataset, self).__init__()\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __iter__(self):\n",
    "        for d in self.datasets:\n",
    "            assert isinstance(d, IterableDataset), \"ChainDataset only supports IterableDataset\"\n",
    "            for x in d:\n",
    "                yield x\n",
    "\n",
    "    def __len__(self):\n",
    "        total = 0\n",
    "        for d in self.datasets:\n",
    "            assert isinstance(d, IterableDataset), \"ChainDataset only supports IterableDataset\"\n",
    "            total += len(d)\n",
    "        return total\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "class ConcatDataset(Dataset[T_co]):\n",
    "    r\"\"\"Dataset as a concatenation of multiple datasets.\n",
    "    This class is useful to assemble different existing datasets.\n",
    "    Args:\n",
    "        datasets (sequence): List of datasets to be concatenated\n",
    "    \"\"\"\n",
    "    datasets: List[Dataset[T_co]]\n",
    "    cumulative_sizes: List[int]\n",
    "\n",
    "    @staticmethod\n",
    "    def cumsum(sequence):\n",
    "        r, s = [], 0\n",
    "        for e in sequence:\n",
    "            l = len(e)\n",
    "            r.append(l + s)\n",
    "            s += l\n",
    "        return r\n",
    "\n",
    "    def __init__(self, datasets: Iterable[Dataset]) -> None:\n",
    "        super(ConcatDataset, self).__init__()\n",
    "        self.datasets = list(datasets)\n",
    "        assert len(self.datasets) > 0, 'datasets should not be an empty iterable'  # type: ignore[arg-type]\n",
    "        for d in self.datasets:\n",
    "            assert not isinstance(d, IterableDataset), \"ConcatDataset does not support IterableDataset\"\n",
    "        self.cumulative_sizes = self.cumsum(self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_sizes[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0:\n",
    "            if -idx > len(self):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            idx = len(self) + idx\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return self.datasets[dataset_idx][sample_idx]\n",
    "\n",
    "    @property\n",
    "    def cummulative_sizes(self):\n",
    "        warnings.warn(\"cummulative_sizes attribute is renamed to \"\n",
    "                      \"cumulative_sizes\", DeprecationWarning, stacklevel=2)\n",
    "        return self.cumulative_sizes\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# chain_datasets = ucf_dataset + kinetics_dataset\n",
    "# print(type(chain_datasets))\n",
    "# concat_datasets = LimitDataset(ucf_dataset) + LimitDataset(kinetics_dataset)\n",
    "# print(type(concat_datasets))\n",
    "limit_datasets = Ucf101LimitDataset(ucf_dataset) + KineticsLimitDataset(kinetics_dataset)\n",
    "print(type(limit_datasets))\n",
    "# wrapper_datasets = WrapperDataset(ucf_dataset, labeld_video_paths=\"a\", clip_sampler=\"b\") + WrapperDataset(kinetics_dataset, labeld_video_paths=\"a\", clip_sampler=\"b\")\n",
    "# print(type(wrapper_datasets))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.utils.data.dataset.ConcatDataset'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# print(type(chain_datasets.datasets))\n",
    "# print(type(chain_datasets.datasets[0]))\n",
    "# print(chain_datasets.__len__())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# print(type(concat_datasets.datasets))\n",
    "# print(type(concat_datasets.datasets[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# chain_loader = make_multi_loader(chain_datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# concat_loader = make_multi_loader(concat_datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "limit_loader = make_multi_loader(limit_datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# chain_batch = iter(chain_loader).__next__()\n",
    "# print(type(chain_batch))\n",
    "# chain_inputs = chain_batch[\"video\"]\n",
    "# # print(type(chain_inputs))\n",
    "# print(chain_inputs.shape)\n",
    "# chain_labels = chain_batch[\"label\"]\n",
    "# # print(type(chain_labels))\n",
    "# print(chain_labels.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 下の2つのセルの結果からデータセット名を加えても実行時間は変わらないことが確認できた\n",
    "- DataLoaderを```Shuffle=False```だともっと時間がはやかっった\n",
    "- DataLoaderでシャッフルするならVideoSamplerはRandomでなくてもよさそう\n",
    "  - そっちの方が早いかどうかも後で検証"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# concat_batch = iter(concat_loader).__next__()\n",
    "# print(type(concat_batch))\n",
    "# print(concat_batch.keys())\n",
    "# concat_inputs = concat_batch[\"video\"]\n",
    "# # print(type(concat_inputs))\n",
    "# print(concat_inputs.shape)\n",
    "# concat_labels = concat_batch[\"label\"]\n",
    "# # print(type(concat_labels))\n",
    "# print(concat_labels.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "limit_batch = iter(limit_loader).__next__()\n",
    "print(type(limit_batch))\n",
    "print(limit_batch.keys())\n",
    "limit_inputs = limit_batch[\"video\"]\n",
    "# print(type(limit_inputs))\n",
    "print(limit_inputs.shape)\n",
    "limit_labels = limit_batch[\"label\"]\n",
    "# print(type(limit_labels))\n",
    "print(limit_labels.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label', 'dataset_name'])\n",
      "torch.Size([32, 3, 8, 256, 256])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# limit_batch, data_name = iter(limit_loader).__next__()\n",
    "# print(data_name)\n",
    "# print(type(data_name))\n",
    "# print(type(limit_batch))\n",
    "# print(limit_batch.keys())\n",
    "# limit_inputs = limit_batch[\"video\"]\n",
    "# # print(type(limit_inputs))\n",
    "# print(limit_inputs.shape)\n",
    "# limit_labels = limit_batch[\"label\"]\n",
    "# # print(type(limit_labels))\n",
    "# print(limit_labels.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "limit_batch[\"dataset_name\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'ucf101',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'ucf101',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'ucf101',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics',\n",
       " 'kinetics']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# class MyUcf101(pytorchvideo.data.LabeledVideoDataset):\n",
    "\n",
    "#     def __next__(self):\n",
    "#         dict = super().__next__()\n",
    "#         dict[\"dataset\"] = \"ucf101\"\n",
    "#         return dict\n",
    "\n",
    "# class MyKinetics(pytorchvideo.data.LabeledVideoDataset):\n",
    "\n",
    "#     def __next__(self):\n",
    "#         dict = super().__next__()\n",
    "#         dict[\"dataset\"] = \"kinetics\"\n",
    "#         return dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# y_ucf_dataset = MyUcf101(ucf_dataset)\n",
    "# my_kinetics_dataset = MyKinetics(kinetics_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## torchvision.dataset.DatasetNameとpytorchvicdo.data.LaveldVideoDatasetの違い\n",
    "- torchvisionはクラスを呼び出しているのでtorchvisionのクラスを継承したクラスを定義して新たに定義したクラスにtorchvisionと同じ引数を与えて呼び出せる\n",
    "- pytorchvideoはメソッドを呼び出して返り値がクラスなのでwrapperのようなことをするしかない？\n",
    "- LimitDatasetの```__getitem__()```でデータセットの名前も同時に返すとか（dictに入らないしデータセットごとにLimitDatasetクラスを用意しなければならない）\n",
    "  - ```__getitem__()```ではバッチごと持ってくるので1つ1つのデータにデータセット名を与えられないので（バッチ単位で同じデータセット名を与えてしまう）やっぱりdictに入れるのが良さそう"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 以下で学習ループを回しデータによってアダプタを変更するコードを設計"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(dim)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)       \n",
    "        self.bn2 = nn.BatchNorm2d(dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.bn2(out)        \n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "class VideoToFrame(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def make_new_inputs(self, inputs):\n",
    "        \"\"\"\n",
    "        動画データを画像データに分割\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): inputs\n",
    "        Returns:\n",
    "            new_inputs torch.Tensor: new_inputs\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        num_frame = inputs.size(2)\n",
    "        \n",
    "        inputs = inputs.permute(0,2,1,3,4)\n",
    "        outputs = inputs.reshape(batch_size*num_frame, \n",
    "                                inputs.size(2), \n",
    "                                inputs.size(3), \n",
    "                                inputs.size(4))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.make_new_inputs(x)\n",
    "        return x\n",
    "\n",
    "class FrameAvg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def frame_out_to_video_out(self, input: torch.Tensor, batch_size, num_frame) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        フレームごとの出力をビデオとしての出力に変換\n",
    "        Args:\n",
    "            input (torch.Tensor): フレームごとの出力\n",
    "            batch_size (int): バッチサイズ\n",
    "            num_frame (int): フレーム数\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: ビデオとしての出力\n",
    "        \"\"\"\n",
    "        input = input.reshape(batch_size, num_frame, -1)\n",
    "        output = torch.mean(input, dim=1)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, batch_size, num_frame):\n",
    "        x = self.frame_out_to_video_out(x, batch_size, num_frame)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## バッチ内に異なるデータセットがある場合のモデルの設計の問題点\n",
    "- モデル側からはどのデータがどのデータセットからなのかが不明\n",
    "  - モデルにどのデータがどのデータセットからなのかの情報を入力とともに与えることで解決\n",
    "- バッチ内で通過するモジュールが異なる（アダプタと出力層）\n",
    "  - テンソルを複数（データセットの数）に分けてそれぞれモジュールに追加させる\n",
    "    - バッチごとにあるデータセットからの数は異なるのでモジュールに流れるデータ数も異なる\n",
    "    - データ数が異なるがモジュールはうまく学習できるのか？（一般的なモデルで言うとバッチサイズが毎回異なるのに学習がうまく進むのかということ）\n",
    "    - データ数が偏っている場合はあるアダプタだけが学習が進むようなことが起きる？\n",
    "      - アダプタと出力層の学習だけなら問題ない気がするがドメイン非依存のパラメータも同時に学習する場合はドメイン非依存のパラメータがデータ数の多いアダプタからの出力から主に学習をするのでデータ数の少ないアダプタの学習が遅れる？\n",
    "  - テンソルをデータセットごとに分ける方法\n",
    "    - adapter\n",
    "      - reshapeで(batch*frame, chanel, h, w) -> (batch, frame, chanel, h, w)\n",
    "      - 理想は(batch, frame, chanel, h, w) -> (num_domain, batch_of_domain, frame, chanel, h, w)\n",
    "        - データがデータセットごとに順番かつ同数になっていたらreshapeでこれが可能\n",
    "    - head（出力層）\n",
    "      - (batch*frame, dim) -> (batch, frame, dim) -> (num_domain, batch_of_domain, frame, dim)\n",
    "      - テンソルを切り取って (num_domain, batch_of_domain, frame, dim) -> (batch_of_domain, frame, dim)　* num_domain\n",
    "      - (batch_of_domain, frame, dim) -> (bach_of_domain*frame, dim)\n",
    "      - あとはdomainに合わせたクラス数を用いてそれぞれのドメインごとに (bach_of_domain*frame, dim) -> (bach_of_domain*frame, num_class) -> (bach_of_domain, frame, dim)\n",
    "      - フレーム方向（dim=1）で平均　(bach_of_domain, frame, num_class) -> (batch_of_domain, num_class)\n",
    "      - 最後にドメインごとの結果を連結はできない（クラス数が異なるから）\n",
    "        - lossはどうやって計算する？\n",
    "          - ドメインごとにロスを設計？\n",
    "            - ドメインごとに学習しているのと同じだからダメ\n",
    "          - ドメインごとにロスの和を最終的なロスとする\n",
    "            - 中部大のmulti head型のmix loss\n",
    "  - 疑問点\n",
    "    - 学習の際は全てのデータセットでデータ数を揃えても問題ない？\n",
    "      - その方が同じくらい学習したアダプタからの出力をドメイン非依存のモジュールが受け取れるので学習が安定しそう\n",
    "- 中部大のmulti head型の内容\n",
    "  - バッチは全て同じデータセットのデータを使う\n",
    "  - バッチごとに誤差伝播をするのではなくて全てのデータセットのバッチを流して累積した誤差を逆伝播する\n",
    "    - 学習時間とメモリ使用量の削減のために自動混合精度を用いて学習 (https://arxiv.org/pdf/1710.03740.pdf)\n",
    "      - おそらくマルチドメインのためのものではなくて単に半精度と単制度の演算を組み合わせて高速化する方法のことだと思う"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "class ReconstructNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.resnet152(pretrained=True)\n",
    "        model_num_features = model.fc.in_features\n",
    "        ucf_num_class = 101\n",
    "        kinetics_num_class = 400\n",
    "\n",
    "        self.video_to_frame = VideoToFrame()\n",
    "        self.net_bottom = nn.Sequential(\n",
    "            model.conv1,\n",
    "            model.bn1,\n",
    "            model.relu,\n",
    "            model.maxpool\n",
    "        )\n",
    "        \n",
    "        self.layer1 = model.layer1\n",
    "        self.layer2 = model.layer2\n",
    "        self.layer3 = model.layer3\n",
    "        self.layer4 = model.layer4\n",
    "        self.avgpool = model.avgpool\n",
    "\n",
    "        # self.adapter = Adapter(512)\n",
    "\n",
    "        self.ucf_head = nn.Sequential(\n",
    "            nn.Linear(model_num_features, ucf_num_class)\n",
    "        )\n",
    "\n",
    "        self.kinetics_head = nn.Sequential(\n",
    "            nn.Linear(model_num_features, kinetics_num_class)\n",
    "        )\n",
    "\n",
    "        self.frame_avg = FrameAvg()\n",
    "        \n",
    "        # 学習させるパラメータ名\n",
    "        self.update_param_names = [\"adapter.bn1.weight\", \"adapter.bn1.bias\",\n",
    "                            \"adapter.conv1.weight\", \"adapter.conv1.bias\",\n",
    "                            \"adapter.bn2.weight\", \"adapter.bn2.bias\", \n",
    "                            \"ucf_head.0.weight\", \"ucf_head.0.bias\"]\n",
    "        # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.update_param_names:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_frame = x.size(2)\n",
    "\n",
    "        x = self.video_to_frame(x)\n",
    "        x = self.net_bottom(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        # x = self.adapter(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.ucf_head(x)\n",
    "        x = self.frame_avg(x, batch_size, num_frame)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "dataset_ucf = get_ucf101(\"test\")\n",
    "dataset_kinetics = get_kinetics(\"test\")\n",
    "datasets = Ucf101LimitDataset(ucf_dataset) + KineticsLimitDataset(kinetics_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "loader = make_multi_loader(datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ReconstructNet()\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 4\n",
    "\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "step = 0\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(loader),\n",
    "                  total=len(loader),\n",
    "                  leave=True) as pbar_batch:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "\n",
    "            for batch_idx, batch in pbar_batch:\n",
    "                pbar_batch.set_description(\"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                inputs = batch['video'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                bs = inputs.size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs.device)\n",
    "                # print(new_labels.device)\n",
    "\n",
    "                # ここでフレームごとの出力をビデオごとの出力に変換する\n",
    "                # video_outputs = frame_out_to_video_out(outputs, bs, args.VIDEO_NUM_SUBSAMPLED) \n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                \n",
    "                preds = torch.squeeze(outputs.max(dim=1)[1])\n",
    "                # print(video_outputs.shape)\n",
    "                # print(preds.shape)\n",
    "\n",
    "                # acc = (preds == labels).float().mean().item()\n",
    "                # acc_list.append(acc)\n",
    "                # pbar_batch.set_postfix(OrderedDict(loss=loss.item(),acc=acc))\n",
    "\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, labels), bs)\n",
    "\n",
    "                pbar_batch.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                    train_loss.val, train_acc.val,\n",
    "                ))\n",
    "\n",
    "                # experiment.log_metric(\"batch_accuracy\", train_acc.val, step=step)\n",
    "                step += 1\n",
    "\n",
    "            acc_list.append(train_acc.avg)\n",
    "            loss_list.append(train_loss.avg)\n",
    "        pbar_epoch.set_postfix(OrderedDict(\n",
    "            acc=sum(acc_list)/len(acc_list),\n",
    "            loss=sum(loss_list)/len(loss_list)\n",
    "        ))\n",
    "        # experiment.log_metric(\"epoch_accuracy\", train_acc.val, step=epoch)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}