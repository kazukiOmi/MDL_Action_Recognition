{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "import torch\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_xs', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import (\n",
    "    Ucf101, \n",
    "    RandomClipSampler, \n",
    "    UniformClipSampler, \n",
    "    Kinetics,\n",
    "    SSv2\n",
    ")\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/NAS-TVS872XT/dataset/'\n",
    "        self.root = self.metadata_path\n",
    "        self.annotation_path = self.metadata_path\n",
    "        self.FRAMES_PER_CLIP = 16\n",
    "        self.STEP_BETWEEN_CLIPS = 16\n",
    "        self.BATCH_SIZE = 8\n",
    "        self.NUM_WORKERS = 8  # kinetics:8, ucf101:24\n",
    "        # self.CLIP_DURATION = 16 / 25\n",
    "        self.CLIP_DURATION = (8 * 8) / 30  # (num_frames * sampling_rate)/fps\n",
    "        self.VIDEO_NUM_SUBSAMPLED = 16  # 事前学習済みモデルに合わせて16→8\n",
    "        self.UCF101_NUM_CLASSES = 101\n",
    "        self.KINETIC400_NUM_CLASSES = 400\n",
    "        self.MODEL_NAME = \"x3d_m\"\n",
    "\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "def get_kinetics(subset):\n",
    "    \"\"\"\n",
    "    Kinetics400のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"val\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(size=256),\n",
    "                # RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                # CenterCropVideo(crop_size=(256, 256)),\n",
    "                CenterCrop(224),\n",
    "                # RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    root_kinetics = '/mnt/NAS-TVS872XT/dataset/Kinetics400/'\n",
    "\n",
    "    if subset == \"test\":\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + \"test_list.txt\",\n",
    "            video_path_prefix=root_kinetics + 'test/',\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + subset,\n",
    "            video_path_prefix=root_kinetics + subset,\n",
    "            clip_sampler=RandomClipSampler(clip_duration=args.CLIP_DURATION),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "def make_loader(dataset):\n",
    "    \"\"\"\n",
    "    データローダーを作成\n",
    "\n",
    "    Args:\n",
    "        dataset (pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset): get_datasetメソッドで取得したdataset\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: 取得したデータローダー\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    loader = DataLoader(LimitDataset(dataset),\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# dataset = get_kinetics(\"val\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "# data = dataset.__next__()\n",
    "# data = data[\"video\"]\n",
    "# print(data.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "# dataloader = make_loader(dataset)\n",
    "# data_from_loader = iter(dataloader).__next__()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "# batch = data_from_loader[\"video\"]\n",
    "\n",
    "# print(batch.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# batch = batch.permute(0,2,1,3,4)\n",
    "# print(batch.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# video_data_list = []\n",
    "# for i in range(args.BATCH_SIZE):\n",
    "#     video_data_list.append(batch[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "# print(video_data_list[0].shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "# new_batch = torch.cat(video_data_list, dim=0)\n",
    "# print(new_batch.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "def change_dim(list):\n",
    "    new_batch = torch.cat(list, dim=0)\n",
    "    return new_batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "# test_list = []\n",
    "# input_0 = torch.zeros(16,3,224,224)\n",
    "# input_1 = torch.ones(16,3,224,224)\n",
    "# test_list.append(input_0)\n",
    "# test_list.append(input_1)\n",
    "# # print(test_list[1])\n",
    "# test_batch = change_dim(test_list)\n",
    "# print(test_batch.shape)\n",
    "# # print(test_batch[16:32, :])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5次元を4次元に変更完了"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "# # ダミーデータ用意\n",
    "# dummy_data_list = []\n",
    "\n",
    "# for i in range(args.BATCH_SIZE):\n",
    "#     dummy_data = torch.linspace(i, i, 3*16*224*224)\n",
    "#     dummy_data = dummy_data.view(16,3,224,224)\n",
    "#     dummy_data_list.append(dummy_data)\n",
    "\n",
    "# print(dummy_data_list[0].shape)\n",
    "# # print(dummy_data_list[5])  # 全ての要素が5（後でシフトが上手くできたか確認できるように）"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "# dummy_batch = change_dim(dummy_data_list)\n",
    "# print(dummy_batch.shape)\n",
    "# # print(dummy_batch[16*5:16*6,:])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "# dummy_data_list_from_batch = []\n",
    "# for i in range(args.BATCH_SIZE):\n",
    "#     dummy_data_from_batch = dummy_batch[i*16:(i+1)*16, :, :, :]\n",
    "#     # if i == 5:\n",
    "#     #     print(dummy_data_from_batch.shape)\n",
    "#     #     print(dummy_data_from_batch)\n",
    "#     dummy_data_list_from_batch.append(dummy_data_from_batch)\n",
    "\n",
    "# # print(dummy_data_list_from_batch[5])\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ビデオデータをフレームに分割し画像認識モデルに流すための実験"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "dataset = get_kinetics(\"val\")\n",
    "dataset.video_sampler._num_samples = 100\n",
    "\n",
    "dataloader = make_loader(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "def make_new_batch(x: torch.Tensor) -> torch.Tensor:\n",
    "    x = x.permute(0,2,1,3,4)\n",
    "    video_data_list = []\n",
    "    for i in range(x.size()[0]):\n",
    "        video_data_list.append(x[i])\n",
    "    new_batch = torch.cat(video_data_list, dim=0)\n",
    "    return new_batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epoch = 1\n",
    "\n",
    "with tqdm(range(num_epoch)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch {}]\".format(epoch))\n",
    "\n",
    "        with tqdm(enumerate(dataloader),\n",
    "                  total=len(dataloader),\n",
    "                  leave=True) as pbar_batch:\n",
    "            \n",
    "            for batch_idx, batch in pbar_batch:\n",
    "                inputs = batch['video'].to(device)\n",
    "                targets = batch['label'].to(device)\n",
    "                if batch_idx == 0:\n",
    "                    # new_batch = make_new_batch(inputs)\n",
    "                    # print(inputs.shape)\n",
    "                    # print(new_batch.shape)\n",
    "                    targets_list = []\n",
    "                    num_frame = inputs.size()[2]\n",
    "                    for i in range(targets.size()[0]):\n",
    "                        # print(targets[i])\n",
    "                        target_id = targets[i].item()\n",
    "                        # print(target_id)\n",
    "                        label = torch.full((1,8), target_id)\n",
    "                        # print(label.shape)\n",
    "                        # print(label)\n",
    "                        targets_list.append(label)\n",
    "                    new_targets = torch.cat(targets_list, dim=1)\n",
    "                    new_targets = torch.squeeze(new_targets)\n",
    "                    print(targets)\n",
    "                    print(new_targets)\n",
    "                    print(targets.shape)\n",
    "                    print(new_targets.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "870e5140cbb3427a896d9a29876ec793"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2adf3f668ac246599f721b2e44b64679"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([223,  36, 179, 392,  71, 350, 308, 163], device='cuda:0')\n",
      "tensor([223, 223, 223, 223, 223, 223, 223, 223,  36,  36,  36,  36,  36,  36,\n",
      "         36,  36, 179, 179, 179, 179, 179, 179, 179, 179, 392, 392, 392, 392,\n",
      "        392, 392, 392, 392,  71,  71,  71,  71,  71,  71,  71,  71, 350, 350,\n",
      "        350, 350, 350, 350, 350, 350, 308, 308, 308, 308, 308, 308, 308, 308,\n",
      "        163, 163, 163, 163, 163, 163, 163, 163])\n",
      "torch.Size([8])\n",
      "torch.Size([64])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}