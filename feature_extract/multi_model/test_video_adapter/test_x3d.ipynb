{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import (\n",
    "    Ucf101, \n",
    "    RandomClipSampler, \n",
    "    UniformClipSampler, \n",
    "    Kinetics,\n",
    "    SSv2\n",
    ")\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn import mixture\n",
    "from sklearn import svm\n",
    "from sklearn import decomposition\n",
    "import os.path as osp\n",
    "import argparse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.NUM_EPOCH = 5\n",
    "        self.FRAMES_PER_CLIP = 16\n",
    "        self.STEP_BETWEEN_CLIPS = 16\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.NUM_WORKERS = 32\n",
    "        # self.CLIP_DURATION = 16 / 25\n",
    "        # (num_frames * sampling_rate)/fps\n",
    "        self.kinetics_clip_duration = (8 * 8) / 30\n",
    "        self.ucf101_clip_duration = 16 / 25\n",
    "        self.VIDEO_NUM_SUBSAMPLED = 16\n",
    "        self.UCF101_NUM_CLASSES = 101\n",
    "        self.KINETIC400_NUM_CLASSES = 400"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class ReconstructNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torch.hub.load(\n",
    "        'facebookresearch/pytorchvideo', \"x3d_m\", pretrained=True)\n",
    "        self.model_num_features = model.blocks[5].proj.in_features\n",
    "        self.num_class = 101\n",
    "\n",
    "        self.net_bottom = nn.Sequential(\n",
    "            model.blocks[0],\n",
    "            model.blocks[1],\n",
    "            model.blocks[2],\n",
    "            model.blocks[3],\n",
    "        )\n",
    "\n",
    "        self.blocks4 = model.blocks[4]\n",
    "\n",
    "        self.net_top = nn.Sequential(\n",
    "            model.blocks[5].pool,\n",
    "            model.blocks[5].dropout\n",
    "        )\n",
    "\n",
    "        # self.linear = model.blocks[5].proj\n",
    "        self.linear = nn.Linear(self.model_num_features, self.num_class)\n",
    "\n",
    "        # 学習させるパラメータ名\n",
    "        self.update_param_names = [\"linear.weight\", \"linear.bias\"]\n",
    "        # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.update_param_names:\n",
    "                param.requires_grad = True\n",
    "                # print(name)\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "               \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x = x.permute(0,2,1,3,4)\n",
    "        x = self.net_bottom(x)\n",
    "        x = self.blocks4(x)\n",
    "        x = self.net_top(x)\n",
    "        x = x.permute(0,2,3,4,1)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.num_class)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Adapter2D(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(dim)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)       \n",
    "        self.bn2 = nn.BatchNorm2d(dim)\n",
    "    \n",
    "    def video_to_frame(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        num_frame = inputs.size(2)\n",
    "\n",
    "        inputs = inputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = inputs.reshape(batch_size * num_frame,\n",
    "                                 inputs.size(2),\n",
    "                                 inputs.size(3),\n",
    "                                 inputs.size(4))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def frame_to_video(\n",
    "            self, input: torch.Tensor, batch_size, num_frame, channel, height, width) -> torch.Tensor:\n",
    "        output = input.reshape(batch_size, num_frame, channel, height, width)\n",
    "        output = output.permute(0,2,1,3,4)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_frame = x.size(2)\n",
    "        channel= x.size(1)\n",
    "        height = x.size(3)\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = self.video_to_frame(x)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        residual = x\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        out += residual\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = self.frame_to_video(out, batch_size, num_frame, channel, height, height)\n",
    "        # print(out.shape)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class ReconstructNet2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torch.hub.load(\n",
    "        'facebookresearch/pytorchvideo', \"x3d_m\", pretrained=True)\n",
    "        self.model_num_features = model.blocks[5].proj.in_features\n",
    "        self.num_class = 101\n",
    "\n",
    "        self.net_bottom = nn.Sequential(\n",
    "            model.blocks[0],\n",
    "            model.blocks[1],\n",
    "            model.blocks[2],\n",
    "            model.blocks[3],\n",
    "        )\n",
    "        \n",
    "        self.adapter0 = Adapter2D(96)\n",
    "\n",
    "        self.blocks4 = model.blocks[4]\n",
    "\n",
    "        self.adapter1 = Adapter2D(192)\n",
    "\n",
    "        self.net_top = nn.Sequential(\n",
    "            model.blocks[5].pool,\n",
    "            model.blocks[5].dropout\n",
    "        )\n",
    "\n",
    "        # self.linear = model.blocks[5].proj\n",
    "        self.linear = nn.Linear(self.model_num_features, self.num_class)\n",
    "\n",
    "        # 学習させるパラメータ名\n",
    "        self.update_param_names = [\"adapter0.bn1.weight\", \"adapter0.bn1.bias\",\n",
    "                                   \"adapter0.conv1.weight\", \"adapter0.conv1.bias\",\n",
    "                                   \"adapter0.bn2.weight\", \"adapter0.bn2.bias\",\n",
    "                                   \"adapter1.bn1.weight\", \"adapter1.bn1.bias\",\n",
    "                                   \"adapter1.conv1.weight\", \"adapter1.conv1.bias\",\n",
    "                                   \"adapter1.bn2.weight\", \"adapter1.bn2.bias\",\n",
    "                                   \"linear.weight\", \"linear.bias\"]\n",
    "        # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.update_param_names:\n",
    "                param.requires_grad = True\n",
    "                # print(name)\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "               \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x = x.permute(0,2,1,3,4)\n",
    "        x = self.net_bottom(x)\n",
    "        x = self.adapter0(x)\n",
    "        x = self.blocks4(x)\n",
    "        x = self.adapter1(x)\n",
    "        x = self.net_top(x)\n",
    "        x = x.permute(0,2,3,4,1)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.num_class)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# adapter = Adapter2D(192)\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     adapter,\n",
    "#     input_size=(1,192,16,7,7),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# model_new = ReconstructNet()\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     model_new,\n",
    "#     input_size=(1,3,16,224,224),\n",
    "#     depth=2,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     model_new.net_bottom[4].res_blocks[0],\n",
    "#     input_size=(1,96,16,14,14),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     model_new.net_bottom[4].res_blocks[6],\n",
    "#     input_size=(1,192,16,7,7),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_kinetics(subset):\n",
    "    \"\"\"\n",
    "    Kinetics400のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"val\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    train_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    val_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(256),\n",
    "                CenterCrop(224),\n",
    "            ]),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    transform = val_transform if subset == \"val\" else train_transform\n",
    "\n",
    "    root_kinetics = '/mnt/dataset/Kinetics400/'\n",
    "\n",
    "    if subset == \"test\":\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + \"test_list.txt\",\n",
    "            video_path_prefix=root_kinetics + 'test/',\n",
    "            clip_sampler=RandomClipSampler(\n",
    "                clip_duration=args.kinetics_clip_duration),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset = Kinetics(\n",
    "            data_path=root_kinetics + subset,\n",
    "            video_path_prefix=root_kinetics + subset,\n",
    "            clip_sampler=RandomClipSampler(\n",
    "                clip_duration=args.kinetics_clip_duration),\n",
    "            video_sampler=RandomSampler,\n",
    "            decode_audio=False,\n",
    "            transform=transform,\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    return False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_ucf101(subset):\n",
    "    \"\"\"\n",
    "    ucf101のデータセットを取得\n",
    "\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "    subset_root_Ucf101 = 'ucfTrainTestlist/trainlist01.txt' if subset == \"train\" else 'ucfTrainTestlist/testlist.txt'\n",
    "    # if subset == \"test\":\n",
    "    #     subset_root_Ucf101 = 'ucfTrainTestlist/testlist.txt'\n",
    "\n",
    "    args = Args()\n",
    "    train_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x - 1),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    val_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(args.VIDEO_NUM_SUBSAMPLED),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(256),\n",
    "                CenterCrop(224),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x - 1),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    transform = train_transform if subset == \"train\" else val_transform\n",
    "\n",
    "    root_ucf101 = '/mnt/dataset/UCF101/'\n",
    "    # root_ucf101 = '/mnt/NAS-TVS872XT/dataset/UCF101/'\n",
    "\n",
    "    dataset = Ucf101(\n",
    "        data_path=root_ucf101 + subset_root_Ucf101,\n",
    "        video_path_prefix=root_ucf101 + 'video/',\n",
    "        clip_sampler=RandomClipSampler(\n",
    "            clip_duration=args.ucf101_clip_duration),\n",
    "        video_sampler=RandomSampler,\n",
    "        decode_audio=False,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    return dataset\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def make_loader(dataset):\n",
    "    \"\"\"\n",
    "    データローダーを作成\n",
    "\n",
    "    Args:\n",
    "        dataset (pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset): get_datasetメソッドで取得したdataset\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: 取得したデータローダー\n",
    "    \"\"\"\n",
    "    args = Args()\n",
    "    loader = DataLoader(LimitDataset(dataset),\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS,\n",
    "                        shuffle=True)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os.path as osp\n",
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, filename, best_model_file, dir_data_name):\n",
    "    file_path = osp.join(dir_data_name, filename)\n",
    "    if not os.path.exists(dir_data_name):\n",
    "        os.makedirs(dir_data_name)\n",
    "    torch.save(state.state_dict(), file_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(file_path, osp.join(dir_data_name, best_model_file))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# save_checkpoint()メソッドのテスト\n",
    "\n",
    "# test_net = ReconstructNet()\n",
    "# test_net.to(\"cuda\")\n",
    "# print(test_net)\n",
    "# save_checkpoint(test_net, True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def train_head():\n",
    "    args = Args()\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = get_ucf101(\"train\")\n",
    "    val_dataset = get_ucf101(\"val\")\n",
    "    train_loader = make_loader(train_dataset)\n",
    "    val_loader = make_loader(val_dataset)\n",
    "\n",
    "    model = ReconstructNet()\n",
    "    model = model.to(device)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    # optimizer = torch.optim.SGD(\n",
    "    #     model.parameters(),\n",
    "    #     lr=lr,\n",
    "    #     momentum=0.9,\n",
    "    #     weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    hyper_params = {\n",
    "        \"Dataset\": \"UCF101\",\n",
    "        \"epoch\": args.NUM_EPOCH,\n",
    "        \"batch_size\": args.BATCH_SIZE,\n",
    "        \"num_frame\": args.VIDEO_NUM_SUBSAMPLED,\n",
    "        \"optimizer\": \"Adam(0.9, 0.999)\",\n",
    "        \"learning late\": lr,\n",
    "        \"weight decay\": weight_decay,\n",
    "        \"mode\": \"train only head\",\n",
    "        # \"Adapter\": \"adp:0, adp:1\",\n",
    "    }\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "        project_name=\"feeature-extract\",\n",
    "        workspace=\"kazukiomi\",\n",
    "    )\n",
    "\n",
    "    experiment.add_tag('pytorch')\n",
    "    experiment.log_parameters(hyper_params)\n",
    "\n",
    "    num_epochs = args.NUM_EPOCH\n",
    "\n",
    "    step = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "        for epoch in pbar_epoch:\n",
    "            pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "            \"\"\"Training mode\"\"\"\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "\n",
    "            with tqdm(enumerate(train_loader),\n",
    "                      total=len(train_loader),\n",
    "                      leave=True) as pbar_train_batch:\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_idx, batch in pbar_train_batch:\n",
    "                    pbar_train_batch.set_description(\n",
    "                        \"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                    inputs = batch['video'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss.update(loss, bs)\n",
    "                    train_acc.update(top1(outputs, labels), bs)\n",
    "\n",
    "                    pbar_train_batch.set_postfix_str(\n",
    "                        ' | loss_avg={:6.04f} , top1_avg={:6.04f}'\n",
    "                        ' | batch_loss={:6.04f} , batch_top1={:6.04f}'\n",
    "                        ''.format(\n",
    "                            train_loss.avg, train_acc.avg,\n",
    "                            train_loss.val, train_acc.val,\n",
    "                        ))\n",
    "\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_accuracy\", train_acc.val, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_loss\", train_loss.val, step=step)\n",
    "                    step += 1\n",
    "\n",
    "            \"\"\"Val mode\"\"\"\n",
    "            model.eval()\n",
    "            val_loss = AverageMeter()\n",
    "            val_acc = AverageMeter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, val_batch in enumerate(val_loader):\n",
    "                    inputs = val_batch['video'].to(device)\n",
    "                    labels = val_batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    val_outputs = model(inputs)\n",
    "                    loss = criterion(val_outputs, labels)\n",
    "\n",
    "                    val_loss.update(loss, bs)\n",
    "                    val_acc.update(top1(val_outputs, labels), bs)\n",
    "            \"\"\"Finish Val mode\"\"\"\n",
    "\n",
    "            pbar_epoch.set_postfix_str(\n",
    "                ' train_loss={:6.04f} , val_loss={:6.04f}, train_acc={:6.04f}, val_acc={:6.04f}'\n",
    "                ''.format(\n",
    "                    train_loss.avg,\n",
    "                    val_loss.avg,\n",
    "                    train_acc.avg,\n",
    "                    val_acc.avg)\n",
    "            )\n",
    "\n",
    "            experiment.log_metric(\"train_accuracy\",\n",
    "                                  train_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"train_loss\",\n",
    "                                  train_loss.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_accuracy\",\n",
    "                                  val_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_loss\",\n",
    "                                  val_loss.avg,\n",
    "                                  step=epoch + 1)\n",
    "\n",
    "    experiment.end()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# train_head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def train_2d_adapter():\n",
    "    args = Args()\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = get_ucf101(\"train\")\n",
    "    val_dataset = get_ucf101(\"val\")\n",
    "    train_loader = make_loader(train_dataset)\n",
    "    val_loader = make_loader(val_dataset)\n",
    "\n",
    "    model = ReconstructNet2D()\n",
    "    model = model.to(device)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    # optimizer = torch.optim.SGD(\n",
    "    #     model.parameters(),\n",
    "    #     lr=lr,\n",
    "    #     momentum=0.9,\n",
    "    #     weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    hyper_params = {\n",
    "        \"Dataset\": \"UCF101\",\n",
    "        \"epoch\": args.NUM_EPOCH,\n",
    "        \"batch_size\": args.BATCH_SIZE,\n",
    "        \"num_frame\": args.VIDEO_NUM_SUBSAMPLED,\n",
    "        \"optimizer\": \"Adam(0.9, 0.999)\",\n",
    "        \"learning late\": lr,\n",
    "        \"weight decay\": weight_decay,\n",
    "        \"mode\": \"train 2d adapter\",\n",
    "        \"Adapter\": \"adp:0, adp:1\",\n",
    "    }\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "        project_name=\"feeature-extract\",\n",
    "        workspace=\"kazukiomi\",\n",
    "    )\n",
    "\n",
    "    experiment.add_tag('pytorch')\n",
    "    experiment.log_parameters(hyper_params)\n",
    "\n",
    "    num_epochs = args.NUM_EPOCH\n",
    "\n",
    "    step = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "        for epoch in pbar_epoch:\n",
    "            pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "            \"\"\"Training mode\"\"\"\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "\n",
    "            with tqdm(enumerate(train_loader),\n",
    "                      total=len(train_loader),\n",
    "                      leave=True) as pbar_train_batch:\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_idx, batch in pbar_train_batch:\n",
    "                    pbar_train_batch.set_description(\n",
    "                        \"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                    inputs = batch['video'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss.update(loss, bs)\n",
    "                    train_acc.update(top1(outputs, labels), bs)\n",
    "\n",
    "                    pbar_train_batch.set_postfix_str(\n",
    "                        ' | loss_avg={:6.04f} , top1_avg={:6.04f}'\n",
    "                        ' | batch_loss={:6.04f} , batch_top1={:6.04f}'\n",
    "                        ''.format(\n",
    "                            train_loss.avg, train_acc.avg,\n",
    "                            train_loss.val, train_acc.val,\n",
    "                        ))\n",
    "\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_accuracy\", train_acc.val, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_loss\", train_loss.val, step=step)\n",
    "                    step += 1\n",
    "\n",
    "            \"\"\"Val mode\"\"\"\n",
    "            model.eval()\n",
    "            val_loss = AverageMeter()\n",
    "            val_acc = AverageMeter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, val_batch in enumerate(val_loader):\n",
    "                    inputs = val_batch['video'].to(device)\n",
    "                    labels = val_batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    val_outputs = model(inputs)\n",
    "                    loss = criterion(val_outputs, labels)\n",
    "\n",
    "                    val_loss.update(loss, bs)\n",
    "                    val_acc.update(top1(val_outputs, labels), bs)\n",
    "            \"\"\"Finish Val mode\"\"\"\n",
    "\n",
    "            pbar_epoch.set_postfix_str(\n",
    "                ' train_loss={:6.04f} , val_loss={:6.04f}, train_acc={:6.04f}, val_acc={:6.04f}'\n",
    "                ''.format(\n",
    "                    train_loss.avg,\n",
    "                    val_loss.avg,\n",
    "                    train_acc.avg,\n",
    "                    val_acc.avg)\n",
    "            )\n",
    "\n",
    "            experiment.log_metric(\"train_accuracy\",\n",
    "                                  train_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"train_loss\",\n",
    "                                  train_loss.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_accuracy\",\n",
    "                                  val_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_loss\",\n",
    "                                  val_loss.avg,\n",
    "                                  step=epoch + 1)\n",
    "    \n",
    "    experiment.end()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# train_2d_adapter()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## temporal adapter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class TemporalAdapter(nn.Module):\n",
    "\n",
    "    def __init__(self, channel_dim, frame_dim):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm3d(channel_dim)\n",
    "        self.conv1 = nn.Conv2d(frame_dim, frame_dim, 1)       \n",
    "        self.bn2 = nn.BatchNorm3d(channel_dim)\n",
    "    \n",
    "    def swap_channel_frame(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        channel = inputs.size(1)\n",
    "        num_frame = inputs.size(2)\n",
    "\n",
    "        # inputs = inputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = inputs.reshape(batch_size * channel,\n",
    "                                 num_frame,\n",
    "                                 inputs.size(3),\n",
    "                                 inputs.size(4))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def frame_to_video(\n",
    "            self, input: torch.Tensor, batch_size, num_frame, channel, height, width) -> torch.Tensor:\n",
    "        output = input.reshape(batch_size, channel, num_frame, height, width)\n",
    "        # output = output.permute(0,2,1,3,4)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        channel= x.size(1)\n",
    "        num_frame = x.size(2)\n",
    "        height = x.size(3)\n",
    "        \n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.swap_channel_frame(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.frame_to_video(out, batch_size, num_frame, channel, height, height)\n",
    "        out += residual\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class ReconstructNetTemporal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torch.hub.load(\n",
    "        'facebookresearch/pytorchvideo', \"x3d_m\", pretrained=True)\n",
    "        self.model_num_features = model.blocks[5].proj.in_features\n",
    "        self.num_class = 101\n",
    "        self.num_frame = 16\n",
    "\n",
    "        self.net_bottom = nn.Sequential(\n",
    "            model.blocks[0],\n",
    "            model.blocks[1],\n",
    "            model.blocks[2],\n",
    "            model.blocks[3],\n",
    "        )\n",
    "        \n",
    "        # self.adapter0 = TemporalAdapter(96, self.num_frame)\n",
    "\n",
    "        self.blocks4 = model.blocks[4]\n",
    "\n",
    "        self.adapter1 = TemporalAdapter(192, self.num_frame)\n",
    "\n",
    "        self.net_top = nn.Sequential(\n",
    "            model.blocks[5].pool,\n",
    "            model.blocks[5].dropout\n",
    "        )\n",
    "\n",
    "        # self.linear = model.blocks[5].proj\n",
    "        self.linear = nn.Linear(self.model_num_features, self.num_class)\n",
    "\n",
    "        # 学習させるパラメータ名\n",
    "        # self.update_param_names = [\"adapter0.bn1.weight\", \"adapter0.bn1.bias\",\n",
    "        #                            \"adapter0.conv1.weight\", \"adapter0.conv1.bias\",\n",
    "        #                            \"adapter0.bn2.weight\", \"adapter0.bn2.bias\",\n",
    "        #                            \"adapter1.bn1.weight\", \"adapter1.bn1.bias\",\n",
    "        #                            \"adapter1.conv1.weight\", \"adapter1.conv1.bias\",\n",
    "        #                            \"adapter1.bn2.weight\", \"adapter1.bn2.bias\",\n",
    "        #                            \"linear.weight\", \"linear.bias\"]\n",
    "        self.update_param_names = [\"adapter1.bn1.weight\", \"adapter1.bn1.bias\",\n",
    "                                   \"adapter1.conv1.weight\", \"adapter1.conv1.bias\",\n",
    "                                   \"adapter1.bn2.weight\", \"adapter1.bn2.bias\",\n",
    "                                   \"linear.weight\", \"linear.bias\"]\n",
    "        # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.update_param_names:\n",
    "                param.requires_grad = True\n",
    "                # print(name)\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "               \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x = x.permute(0,2,1,3,4)\n",
    "        x = self.net_bottom(x)\n",
    "        # x = self.adapter0(x)\n",
    "        x = self.blocks4(x)\n",
    "        x = self.adapter1(x)\n",
    "        x = self.net_top(x)\n",
    "        x = x.permute(0,2,3,4,1)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,self.num_class)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# model_new = ReconstructNetTemporal()\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     model_new,\n",
    "#     input_size=(1,3,16,224,224),\n",
    "#     depth=2,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape\n",
       "==============================================================================================================\n",
       "ReconstructNetTemporal                                       --                        --\n",
       "├─ResStage (blocks4)                                         --                        --\n",
       "│    └─ModuleList (res_blocks)                               --                        --\n",
       "├─Sequential (net_bottom)                                    [1, 3, 16, 224, 224]      [1, 96, 16, 14, 14]\n",
       "│    └─ResNetBasicStem (0)                                   [1, 3, 16, 224, 224]      [1, 24, 16, 112, 112]\n",
       "│    └─ResStage (1)                                          [1, 24, 16, 112, 112]     [1, 24, 16, 56, 56]\n",
       "│    └─ResStage (2)                                          [1, 24, 16, 56, 56]       [1, 48, 16, 28, 28]\n",
       "│    └─ResStage (3)                                          [1, 48, 16, 28, 28]       [1, 96, 16, 14, 14]\n",
       "├─ResStage (blocks4)                                         [1, 96, 16, 14, 14]       [1, 192, 16, 7, 7]\n",
       "├─TemporalAdapter (adapter1)                                 [1, 192, 16, 7, 7]        [1, 192, 16, 7, 7]\n",
       "│    └─BatchNorm3d (bn1)                                     [1, 192, 16, 7, 7]        [1, 192, 16, 7, 7]\n",
       "│    └─Conv2d (conv1)                                        [192, 16, 7, 7]           [192, 16, 7, 7]\n",
       "│    └─BatchNorm3d (bn2)                                     [1, 192, 16, 7, 7]        [1, 192, 16, 7, 7]\n",
       "├─Sequential (net_top)                                       [1, 192, 16, 7, 7]        [1, 2048, 1, 1, 1]\n",
       "│    └─ProjectedPool (0)                                     [1, 192, 16, 7, 7]        [1, 2048, 1, 1, 1]\n",
       "│    └─Dropout (1)                                           [1, 2048, 1, 1, 1]        [1, 2048, 1, 1, 1]\n",
       "├─Linear (linear)                                            [1, 1, 1, 1, 2048]        [1, 1, 1, 1, 101]\n",
       "==============================================================================================================\n",
       "Total params: 3,182,663\n",
       "Trainable params: 207,989\n",
       "Non-trainable params: 2,974,674\n",
       "Total mult-adds (G): 4.73\n",
       "==============================================================================================================\n",
       "Input size (MB): 9.63\n",
       "Forward/backward pass size (MB): 1362.02\n",
       "Params size (MB): 12.73\n",
       "Estimated Total Size (MB): 1384.39\n",
       "=============================================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_temporal_adapter():\n",
    "    args = Args()\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = get_ucf101(\"train\")\n",
    "    val_dataset = get_ucf101(\"val\")\n",
    "    train_loader = make_loader(train_dataset)\n",
    "    val_loader = make_loader(val_dataset)\n",
    "\n",
    "    model = ReconstructNetTemporal()\n",
    "    model = model.to(device)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    # optimizer = torch.optim.SGD(\n",
    "    #     model.parameters(),\n",
    "    #     lr=lr,\n",
    "    #     momentum=0.9,\n",
    "    #     weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    hyper_params = {\n",
    "        \"Dataset\": \"UCF101\",\n",
    "        \"epoch\": args.NUM_EPOCH,\n",
    "        \"batch_size\": args.BATCH_SIZE,\n",
    "        \"num_frame\": args.VIDEO_NUM_SUBSAMPLED,\n",
    "        \"optimizer\": \"Adam(0.9, 0.999)\",\n",
    "        \"learning late\": lr,\n",
    "        \"weight decay\": weight_decay,\n",
    "        \"mode\": \"train temporal adapter\",\n",
    "        \"Adapter\": \"adp:0, adp:1\",\n",
    "    }\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "        project_name=\"feeature-extract\",\n",
    "        workspace=\"kazukiomi\",\n",
    "    )\n",
    "\n",
    "    experiment.add_tag('pytorch')\n",
    "    experiment.log_parameters(hyper_params)\n",
    "\n",
    "    num_epochs = args.NUM_EPOCH\n",
    "\n",
    "    step = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "        for epoch in pbar_epoch:\n",
    "            pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "            \"\"\"Training mode\"\"\"\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "\n",
    "            with tqdm(enumerate(train_loader),\n",
    "                      total=len(train_loader),\n",
    "                      leave=True) as pbar_train_batch:\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_idx, batch in pbar_train_batch:\n",
    "                    pbar_train_batch.set_description(\n",
    "                        \"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                    inputs = batch['video'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss.update(loss, bs)\n",
    "                    train_acc.update(top1(outputs, labels), bs)\n",
    "\n",
    "                    pbar_train_batch.set_postfix_str(\n",
    "                        ' | loss_avg={:6.04f} , top1_avg={:6.04f}'\n",
    "                        ' | batch_loss={:6.04f} , batch_top1={:6.04f}'\n",
    "                        ''.format(\n",
    "                            train_loss.avg, train_acc.avg,\n",
    "                            train_loss.val, train_acc.val,\n",
    "                        ))\n",
    "\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_accuracy\", train_acc.val, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_loss\", train_loss.val, step=step)\n",
    "                    step += 1\n",
    "\n",
    "            \"\"\"Val mode\"\"\"\n",
    "            model.eval()\n",
    "            val_loss = AverageMeter()\n",
    "            val_acc = AverageMeter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, val_batch in enumerate(val_loader):\n",
    "                    inputs = val_batch['video'].to(device)\n",
    "                    labels = val_batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    val_outputs = model(inputs)\n",
    "                    loss = criterion(val_outputs, labels)\n",
    "\n",
    "                    val_loss.update(loss, bs)\n",
    "                    val_acc.update(top1(val_outputs, labels), bs)\n",
    "            \"\"\"Finish Val mode\"\"\"\n",
    "\n",
    "            pbar_epoch.set_postfix_str(\n",
    "                ' train_loss={:6.04f} , val_loss={:6.04f}, train_acc={:6.04f}, val_acc={:6.04f}'\n",
    "                ''.format(\n",
    "                    train_loss.avg,\n",
    "                    val_loss.avg,\n",
    "                    train_acc.avg,\n",
    "                    val_acc.avg)\n",
    "            )\n",
    "\n",
    "            experiment.log_metric(\"train_accuracy\",\n",
    "                                  train_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"train_loss\",\n",
    "                                  train_loss.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_accuracy\",\n",
    "                                  val_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_loss\",\n",
    "                                  val_loss.avg,\n",
    "                                  step=epoch + 1)\n",
    "    \n",
    "    experiment.end()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}