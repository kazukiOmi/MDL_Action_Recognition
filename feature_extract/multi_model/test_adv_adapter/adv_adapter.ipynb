{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from comet_ml import Experiment\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import (\n",
    "    Ucf101, \n",
    "    RandomClipSampler, \n",
    "    UniformClipSampler, \n",
    "    Kinetics,\n",
    "    SSv2\n",
    ")\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn import mixture\n",
    "from sklearn import svm\n",
    "from sklearn import decomposition\n",
    "import os.path as osp\n",
    "import argparse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.NUM_EPOCH = 10\n",
    "        # self.FRAMES_PER_CLIP = 16\n",
    "        # self.STEP_BETWEEN_CLIPS = 16\n",
    "        self.BATCH_SIZE = 256\n",
    "        self.NUM_WORKERS = 32\n",
    "        # self.CLIP_DURATION = 16 / 25\n",
    "        # (num_frames * sampling_rate)/fps\n",
    "        # self.kinetics_clip_duration = (8 * 8) / 30\n",
    "        # self.ucf101_clip_duration = 16 / 25\n",
    "        # self.VIDEO_NUM_SUBSAMPLED = 16\n",
    "        # self.UCF101_NUM_CLASSES = 101\n",
    "        # self.KINETIC400_NUM_CLASSES = 400\n",
    "        self.mnist_num_classes = 10\n",
    "        self.svhn_num_classes = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class MyGenerator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv(x)\n",
    "        x += residual\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class MyPreNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5, padding=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 5, padding=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7*7*64, 256)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.svhn_head = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.svhn_head(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_path = \"SVHN/pretrain_best.pth\"\n",
    "        base_model = MyPreNet()\n",
    "        base_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        self.layer1 = base_model.layer1\n",
    "        self.layer2 = base_model.layer2\n",
    "        self.adapter = MyGenerator(64)\n",
    "        self.fc = base_model.fc\n",
    "        self.dropout = base_model.dropout\n",
    "        self.svhn_head = base_model.svhn_head\n",
    "        self.mnist_head = nn.Linear(256, 10)\n",
    "\n",
    "        # 学習させるパラメータ名\n",
    "        self.update_param_names = [\"adapter.conv.weight\", \"adapter.conv.bias\",\n",
    "                                   \"mnist_head.weight\", \"mnist_head.bias\"]\n",
    "        # 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.update_param_names:\n",
    "                param.requires_grad = True\n",
    "                # print(name)\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, domain):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        feat_in = x\n",
    "        if domain == \"MNIST\":\n",
    "            x = self.adapter(x)\n",
    "        feat_out = x\n",
    "\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.dropout(x)\n",
    "        if domain == \"SVHN\":\n",
    "            x = self.svhn_head(x)\n",
    "        elif domain == \"MNIST\":\n",
    "            x = self.mnist_head(x)\n",
    "        \n",
    "        return x, feat_in.detach(), feat_out\n",
    "\n",
    "    ## torchinfo用\n",
    "    # def forward(self, x):\n",
    "    #     x = self.layer1(x)\n",
    "    #     x = self.layer2(x)\n",
    "    #     x = self.adapter(x)\n",
    "    #     x = x.view(-1, 7*7*64)\n",
    "    #     x = F.relu(self.fc(x))\n",
    "    #     x = self.dropout(x)\n",
    "    #     x = self.mnist_head(x)\n",
    "        \n",
    "    #     return x\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "class MyDiscriminator(nn.Module):\n",
    "    def __init__(self, ch, h, w, dim):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(h*w*ch, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 2),\n",
    "        )\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        x = self.layers(x)\n",
    "        x = self.log_softmax(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "# test_net = MyNet()\n",
    "\n",
    "# torchinfo.summary(\n",
    "#     test_net,\n",
    "#     input_size=(1,1,28,28),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )\n",
    "\n",
    "test_net = MyDiscriminator(ch=64, h=7, w=7, dim=128)\n",
    "\n",
    "\n",
    "torchinfo.summary(\n",
    "    test_net,\n",
    "    input_size=(1, 64, 7, 7),\n",
    "    # input_size=(1, 64*7*7),\n",
    "    depth=4,\n",
    "    col_names=[\"input_size\",\n",
    "               \"output_size\"],\n",
    "    row_settings=(\"var_names\",)\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Input Shape               Output Shape\n",
       "==========================================================================================\n",
       "MyDiscriminator                          --                        --\n",
       "├─Sequential (layers)                    [1, 3136]                 [1, 2]\n",
       "│    └─Linear (0)                        [1, 3136]                 [1, 128]\n",
       "│    └─ReLU (1)                          [1, 128]                  [1, 128]\n",
       "│    └─Linear (2)                        [1, 128]                  [1, 2]\n",
       "├─LogSoftmax (log_softmax)               [1, 2]                    [1, 2]\n",
       "==========================================================================================\n",
       "Total params: 401,794\n",
       "Trainable params: 401,794\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.40\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 1.61\n",
       "Estimated Total Size (MB): 1.62\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_mnist(subset):\n",
    "    is_train = True if subset ==\"train\" else False\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(28),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=0.5, std=0.5)])\n",
    "            \n",
    "    dataset = datasets.MNIST(\n",
    "        root=\"/mnt/dataset/MNIST\",\n",
    "        train=is_train,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_svhn(subset):\n",
    "    train = \"train\" if subset ==\"train\" else \"test\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(28),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Normalize(\n",
    "            mean=0.5, std=0.5)])\n",
    "            \n",
    "    dataset = datasets.SVHN(\n",
    "        root=\"/mnt/dataset/SVHN\",\n",
    "        split=train,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def make_loader(dataset):\n",
    "    args = Args()\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=args.BATCH_SIZE,\n",
    "                        drop_last=True,\n",
    "                        num_workers=args.NUM_WORKERS,\n",
    "                        shuffle=True)\n",
    "    return loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# mnist_train_data = get_mnist(\"train\")\n",
    "# print(len(mnist_train_data))\n",
    "# mnist_train_loader = make_loader(mnist_train_data)\n",
    "# print(len(mnist_train_loader))\n",
    "\n",
    "# svhn_train_data = get_svhn(\"train\")\n",
    "# print(len(svhn_train_data))\n",
    "# svhn_train_loader = make_loader(svhn_train_data)\n",
    "# print(len(svhn_train_loader))\n",
    "\n",
    "# mnist_val_data = get_mnist(\"val\")\n",
    "# print(len(mnist_val_data))\n",
    "# mnist_val_loader = make_loader(mnist_val_data)\n",
    "# print(len(mnist_val_loader))\n",
    "\n",
    "# svhn_val_data = get_svhn(\"val\")\n",
    "# print(len(svhn_val_data))\n",
    "# svhn_val_loader = make_loader(svhn_val_data)\n",
    "# print(len(svhn_val_loader))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import os.path as osp\n",
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, filename, best_model_file, dir_data_name):\n",
    "    file_path = osp.join(dir_data_name, filename)\n",
    "    if not os.path.exists(dir_data_name):\n",
    "        os.makedirs(dir_data_name)\n",
    "    torch.save(state.state_dict(), file_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(file_path, osp.join(dir_data_name, best_model_file))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def pre_train():\n",
    "    args = Args()\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # train_data_mnist = get_mnist(\"train\")\n",
    "    # train_loader_mnist = make_loader(train_data_mnist)\n",
    "    # val_data_mnist = get_mnist(\"val\")\n",
    "    # val_loader_mnist = make_loader(val_data_mnist)\n",
    "\n",
    "    train_data_svhn = get_svhn(\"train\")\n",
    "    train_loader_svhn = make_loader(train_data_svhn)\n",
    "    val_data_svhn = get_svhn(\"val\")\n",
    "    val_loader_svhn = make_loader(val_data_svhn)\n",
    "\n",
    "    model = MyPreNet()\n",
    "    model = model.to(device)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-5\n",
    "    # optimizer = torch.optim.SGD(\n",
    "    #     model.parameters(),\n",
    "    #     lr=lr,\n",
    "    #     momentum=0.9,\n",
    "    #     weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    hyper_params = {\n",
    "        \"Dataset\": \"SVHN\",\n",
    "        \"epoch\": args.NUM_EPOCH,\n",
    "        \"batch_size\": args.BATCH_SIZE,\n",
    "        \"optimizer\": \"Adam(0.9, 0.999)\",\n",
    "        \"learning late\": lr,\n",
    "        \"weight decay\": weight_decay,\n",
    "        \"mode\": \"pre train\",\n",
    "    }\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "        project_name=\"feeature-extract\",\n",
    "        workspace=\"kazukiomi\",\n",
    "    )\n",
    "\n",
    "    experiment.add_tag('pytorch')\n",
    "    experiment.log_parameters(hyper_params)\n",
    "\n",
    "    num_epochs = args.NUM_EPOCH\n",
    "\n",
    "    step = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "        for epoch in pbar_epoch:\n",
    "            pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "            \"\"\"Training mode\"\"\"\n",
    "            model.train()\n",
    "            train_loss_svhn = AverageMeter()\n",
    "            train_acc_svhn = AverageMeter()\n",
    "\n",
    "            with tqdm(enumerate(train_loader_svhn),\n",
    "                      total=len(train_loader_svhn),\n",
    "                      leave=True) as pbar_train_batch:\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_idx, (inputs, labels) in pbar_train_batch:\n",
    "                    pbar_train_batch.set_description(\n",
    "                        \"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    bs_svhn = inputs.size(0)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss_svhn = criterion(outputs, labels)\n",
    "                    loss_svhn.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss_svhn.update(loss_svhn, bs_svhn)\n",
    "                    train_acc_svhn.update(top1(outputs, labels), bs_svhn)\n",
    "                    \n",
    "                    pbar_train_batch.set_postfix_str(\n",
    "                        ' | batch_loss_svhn={:6.04f} , batch_top1_svhn={:6.04f}'\n",
    "                        ''.format(\n",
    "                            train_loss_svhn.val, train_acc_svhn.val,\n",
    "                        ))\n",
    "\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_accuracy_svhn\", train_acc_svhn.val, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_loss_svhn\", train_loss_svhn.val, step=step)\n",
    "                    step += 1\n",
    "\n",
    "            \"\"\"Val mode\"\"\"\n",
    "            model.eval()\n",
    "            val_loss_svhn = AverageMeter()\n",
    "            val_acc_svhn = AverageMeter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, labels) in enumerate(val_loader_svhn):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    val_outputs = model(inputs)\n",
    "                    loss = criterion(val_outputs, labels)\n",
    "\n",
    "                    val_loss_svhn.update(loss, bs)\n",
    "                    val_acc_svhn.update(top1(val_outputs, labels), bs)\n",
    "            \"\"\"finish Val mode\"\"\" \n",
    "\n",
    "            \"\"\"save model\"\"\"\n",
    "            if best_acc < val_acc_svhn.avg:\n",
    "                best_acc = val_acc_svhn.avg\n",
    "                is_best = True\n",
    "            else:\n",
    "                is_best = False\n",
    "            \n",
    "            save_checkpoint(model, is_best, filename=\"pretrain_checkpoint.pth\", best_model_file=\"pretrain_best.pth\", dir_data_name=\"SVHN\")\n",
    "        \n",
    "            pbar_epoch.set_postfix_str(\n",
    "                ' train_loss={:6.04f} , val_loss={:6.04f}, train_acc={:6.04f}, val_acc={:6.04f}'\n",
    "                ''.format(\n",
    "                    train_loss_svhn.avg,\n",
    "                    val_loss_svhn.avg,\n",
    "                    train_acc_svhn.avg,\n",
    "                    val_acc_svhn.avg)\n",
    "            )\n",
    "\n",
    "            experiment.log_metric(\"epoch_train_accuracy\",\n",
    "                                  train_acc_svhn.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"epoch_train_loss\",\n",
    "                                  train_loss_svhn.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_accuracy\",\n",
    "                                  val_acc_svhn.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_loss\",\n",
    "                                  val_loss_svhn.avg,\n",
    "                                  step=epoch + 1)\n",
    "\n",
    "                \n",
    "\n",
    "    experiment.end()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "pre_train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using downloaded and verified file: /mnt/dataset/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: /mnt/dataset/SVHN/test_32x32.mat\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/kazukiomi/feeature-extract/ff8f8a0f2a024abb894eedc35e3c7742\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20317547ed8c407ca4a6d061553edf3d"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f12fbf549b4249f99bc436e849f38eaa"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bbb205f66f74fcba14c502be837266d"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bbdeed2711f4e9985fd0da20574f067"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bac9fb06255f4e6da4d71081551d4413"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ab80c92a1214c81b19146f121e54a2f"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10ea1a90c7864b498e07fd812a00584c"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff21bee3edd04791b46e713ac11bc505"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a6f036c0253410794b616af6fd2453a"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a30a4d0e77894cd598ba6034addbdf2e"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee08fb1d41b44be3acadfee5f62130c7"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/kazukiomi/feeature-extract/ff8f8a0f2a024abb894eedc35e3c7742\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_accuracy_svhn [2860] : (0.1484375, 0.96875)\n",
      "COMET INFO:     batch_loss_svhn [2860]     : (0.12685541808605194, 2.3043289184570312)\n",
      "COMET INFO:     epoch_train_accuracy [10]  : (0.6367460664335665, 0.9218203671328671)\n",
      "COMET INFO:     epoch_train_loss [10]      : (0.2606820957539798, 1.1168991713882326)\n",
      "COMET INFO:     loss [286]                 : (0.17599134147167206, 2.3043289184570312)\n",
      "COMET INFO:     val_accuracy [10]          : (0.8383740717821783, 0.9060566212871287)\n",
      "COMET INFO:     val_loss [10]              : (0.3387505091948084, 0.5597920925310342)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Dataset       : SVHN\n",
      "COMET INFO:     batch_size    : 256\n",
      "COMET INFO:     epoch         : 10\n",
      "COMET INFO:     learning late : 0.001\n",
      "COMET INFO:     mode          : pre train\n",
      "COMET INFO:     optimizer     : Adam(0.9, 0.999)\n",
      "COMET INFO:     weight decay  : 5e-05\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (14.33 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# # 呼び出したモデルに1バッチだけ流して精度とロスをテスト(モデルが正しく呼び出されていることを確認済み)\n",
    "\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = MyNet()\n",
    "# model = model.to(device)\n",
    "# test_loader = make_loader(get_svhn(\"val\"))\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# inputs, labels = iter(test_loader).__next__()\n",
    "\n",
    "# model.eval()\n",
    "# test_loss = AverageMeter()\n",
    "# test_acc = AverageMeter()\n",
    "\n",
    "# test_inputs = inputs.to(device)\n",
    "# test_labels = labels.to(device)\n",
    "# bs = test_inputs.size(0)\n",
    "\n",
    "# test_out, _, _ = model(test_inputs, \"SVHN\")\n",
    "# loss = criterion(test_out, test_labels)\n",
    "\n",
    "# test_loss.update(loss, bs)\n",
    "# test_acc.update(top1(test_out, test_labels), bs)\n",
    "\n",
    "# print(test_acc.avg)\n",
    "# print(test_loss.avg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def multi_train():\n",
    "    args = Args()\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_data_mnist = get_mnist(\"train\")\n",
    "    train_loader_mnist = make_loader(train_data_mnist)\n",
    "    val_data_mnist = get_mnist(\"val\")\n",
    "    val_loader_mnist = make_loader(val_data_mnist)\n",
    "\n",
    "    train_data_svhn = get_svhn(\"train\")\n",
    "    train_loader_svhn = make_loader(train_data_svhn)\n",
    "    val_data_svhn = get_svhn(\"val\")\n",
    "    val_loader_svhn = make_loader(val_data_svhn)\n",
    "\n",
    "    model = MyNet()\n",
    "    model = model.to(device)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    loss_lambda = 1e+3\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-5\n",
    "    # optimizer = torch.optim.SGD(\n",
    "    #     model.parameters(),\n",
    "    #     lr=lr,\n",
    "    #     momentum=0.9,\n",
    "    #     weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # mse_loss = nn.MSELoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "\n",
    "    hyper_params = {\n",
    "        \"Dataset\": \"MNIST, SVHN\",\n",
    "        \"epoch\": args.NUM_EPOCH,\n",
    "        \"batch_size\": args.BATCH_SIZE,\n",
    "        \"optimizer\": \"Adam(0.9, 0.999)\",\n",
    "        \"learning late\": lr,\n",
    "        \"weight decay\": weight_decay,\n",
    "        \"mode\": \"training adapter\",\n",
    "        \"reconstruct loss\" : \"l1 loss\",\n",
    "        \"reconstruct loss λ\" : loss_lambda,\n",
    "    }\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "        project_name=\"feeature-extract\",\n",
    "        workspace=\"kazukiomi\",\n",
    "    )\n",
    "\n",
    "    experiment.add_tag('pytorch')\n",
    "    experiment.log_parameters(hyper_params)\n",
    "\n",
    "    num_epochs = args.NUM_EPOCH\n",
    "    # kinetics_iteration = len(train_loader_kinetics)\n",
    "    iteration_mnist = len(train_loader_mnist) - 1\n",
    "    train_mnist_enm = enumerate(train_loader_mnist)\n",
    "\n",
    "    step = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "        for epoch in pbar_epoch:\n",
    "            pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "            \"\"\"Training mode\"\"\"\n",
    "\n",
    "            train_loss_svhn = AverageMeter()\n",
    "            train_acc_svhn = AverageMeter()\n",
    "            train_reconstruct_loss = AverageMeter()\n",
    "            train_loss_mnist = AverageMeter()\n",
    "            train_acc_mnist = AverageMeter()\n",
    "\n",
    "            with tqdm(enumerate(train_loader_svhn),\n",
    "                      total=len(train_loader_svhn),\n",
    "                      leave=True) as pbar_train_batch:\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_idx, (inputs, labels) in pbar_train_batch:\n",
    "                    pbar_train_batch.set_description(\n",
    "                        \"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    bs_svhn = inputs.size(0)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    outputs, feat_in, feat_out = model(inputs, \"SVHN\")\n",
    "                    loss_svhn = criterion(outputs, labels)\n",
    "                    loss_svhn.backward(retain_graph=True)  # loss_reconstructによる勾配計算のために計算グラフを維持（デフォで削除してしまう）\n",
    "                    loss_reconstruct = l1_loss(feat_out, feat_in) * loss_lambda\n",
    "                    loss_reconstruct.backward()\n",
    "                    train_loss_svhn.update(loss_svhn, bs_svhn)\n",
    "                    train_reconstruct_loss.update(loss_reconstruct, bs_svhn)\n",
    "                    train_acc_svhn.update(top1(outputs, labels), bs_svhn)\n",
    "\n",
    "                    \n",
    "                    i, (inputs, labels) = train_mnist_enm.__next__()\n",
    "                    if i+1 > iteration_mnist:\n",
    "                        train_mnist_enm = enumerate(train_loader_mnist)\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    bs_mnist = inputs.size(0)\n",
    "\n",
    "                    outputs, _, _ = model(inputs, \"MNIST\")\n",
    "                    loss_mnist = criterion(outputs, labels)\n",
    "                    loss_mnist.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss_mnist.update(loss_mnist, bs_mnist)\n",
    "                    train_acc_mnist.update(top1(outputs, labels), bs_mnist)\n",
    "\n",
    "                    pbar_train_batch.set_postfix_str(\n",
    "                        ' | batch_loss_svhn={:6.04f} , batch_top1_svhn={:6.04f}'\n",
    "                        ' | batch_loss_mnist={:6.04f} , batch_top1_mnist={:6.04f}'\n",
    "                        ' | batch_reconstruct_loss={:6.04f}'\n",
    "                        ''.format(\n",
    "                            train_loss_svhn.val, train_acc_svhn.val,\n",
    "                            train_loss_mnist.val, train_acc_mnist.val,\n",
    "                            train_reconstruct_loss.val\n",
    "                        ))\n",
    "\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_accuracy_svhn\", train_acc_svhn.val, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_loss_svhn\", train_loss_svhn.val, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_reconstruct_loss\", train_reconstruct_loss.val, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_accuracy_mnist\", train_acc_mnist.val, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_loss_mnist\", train_loss_mnist.val, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"total_acc\", (train_acc_svhn.val+train_acc_mnist.val)/2, step=step)\n",
    "                    experiment.log_metric(\n",
    "                        \"total_loss\", (train_loss_svhn.val+train_loss_mnist.val)/2, step=step)\n",
    "                    step += 1\n",
    "            \n",
    "            experiment.log_metric(\n",
    "                \"epoch_accuracy_svhn\", train_acc_svhn.avg, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"epoch_loss_svhn\", train_loss_svhn.avg, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"epoch_accuracy_mnist\", train_acc_mnist.avg, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"epoch_loss_mnist\", train_loss_mnist.avg, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"epoch_total_acc\", (train_acc_svhn.avg+train_acc_mnist.avg)/2, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"epoch_total_loss\", (train_loss_svhn.avg+train_loss_mnist.avg)/2, step=step)\n",
    "\n",
    "            \"\"\"Val mode\"\"\"\n",
    "            model.eval()\n",
    "            val_loss_svhn = AverageMeter()\n",
    "            val_acc_svhn = AverageMeter()\n",
    "            val_loss_mnist = AverageMeter()\n",
    "            val_acc_mnist = AverageMeter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, labels) in enumerate(val_loader_svhn):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    val_outputs, _, _ = model(inputs, \"SVHN\")\n",
    "                    loss = criterion(val_outputs, labels)\n",
    "\n",
    "                    val_loss_svhn.update(loss, bs)\n",
    "                    val_acc_svhn.update(top1(val_outputs, labels), bs)\n",
    "                \n",
    "                for batch_idx, (inputs, labels) in enumerate(val_loader_mnist):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    val_outputs, _, _ = model(inputs, \"MNIST\")\n",
    "                    loss = criterion(val_outputs, labels)\n",
    "\n",
    "                    val_loss_mnist.update(loss, bs)\n",
    "                    val_acc_mnist.update(top1(val_outputs, labels), bs)\n",
    "\n",
    "            experiment.log_metric(\n",
    "                \"val_accuracy_svhn\", val_acc_svhn.avg, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"val_loss_svhn\", val_loss_svhn.avg, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"val_accuracy_mnist\", val_acc_mnist.avg, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"val_loss_mnist\", val_loss_mnist.avg, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"val_total_acc\", (val_acc_svhn.avg+val_acc_mnist.avg)/2, step=step)\n",
    "            experiment.log_metric(\n",
    "                \"val_total_loss\", (val_loss_svhn.avg+val_loss_mnist.avg)/2, step=step)\n",
    "            \"\"\"finish Val mode\"\"\"\n",
    "\n",
    "    experiment.end()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "multi_train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using downloaded and verified file: /mnt/dataset/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: /mnt/dataset/SVHN/test_32x32.mat\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/kazukiomi/feeature-extract/b0e17cc1bf0a44f8ba2c35f25e5a5158\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "958b05abe16340f9bc68bd34044c1a1a"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d105837a1c14266b662c37ff35d360d"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f50d5772cde04e57a5d01c470394613b"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca04e973fb0a4dffb5264b5338b956ba"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6a385d51b7f499f986c3da9f87cd056"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "908b93805aad45b9899b299cf6f604c9"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bd0324cef204ab0a0c482cb42886526"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e44c61022ac4a1d900b1af53ec1f677"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38b85e47737c4b5ea809bdd5bfb2733c"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e99c1117a7d4d6ba49f05213b3b8546"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a6e333d8a444b2396d22679795660f4"
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=286.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/kazukiomi/feeature-extract/b0e17cc1bf0a44f8ba2c35f25e5a5158\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_accuracy_mnist [2860]   : (0.109375, 0.984375)\n",
      "COMET INFO:     batch_accuracy_svhn [2860]    : (0.12109375, 0.94921875)\n",
      "COMET INFO:     batch_loss_mnist [2860]       : (0.07565745711326599, 2.413520336151123)\n",
      "COMET INFO:     batch_loss_svhn [2860]        : (0.2151830941438675, 2.6907472610473633)\n",
      "COMET INFO:     batch_reconstruct_loss [2860] : (57.78310012817383, 227.15615844726562)\n",
      "COMET INFO:     epoch_accuracy_mnist [10]     : (0.861013986013986, 0.9421847683566433)\n",
      "COMET INFO:     epoch_accuracy_svhn [10]      : (0.7333233173076923, 0.8827168924825175)\n",
      "COMET INFO:     epoch_loss_mnist [10]         : (0.18702436009278664, 0.4726621212659182)\n",
      "COMET INFO:     epoch_loss_svhn [10]          : (0.3935486809773879, 0.8813793188208466)\n",
      "COMET INFO:     epoch_total_acc [10]          : (0.7971686516608392, 0.9119727928321679)\n",
      "COMET INFO:     epoch_total_loss [10]         : (0.2922023442684562, 0.6770207200433824)\n",
      "COMET INFO:     loss [857]                    : (0.10346154868602753, 160.99935913085938)\n",
      "COMET INFO:     total_acc [2860]              : (0.115234375, 0.94921875)\n",
      "COMET INFO:     total_loss [2860]             : (0.17196251824498177, 2.552133798599243)\n",
      "COMET INFO:     val_accuracy_mnist [10]       : (0.9564302884615384, 0.9704527243589743)\n",
      "COMET INFO:     val_accuracy_svhn [10]        : (0.8470374381188119, 0.886409344059406)\n",
      "COMET INFO:     val_loss_mnist [10]           : (0.09447937086224556, 0.13519310110654587)\n",
      "COMET INFO:     val_loss_svhn [10]            : (0.40665693654872404, 0.5257905544620929)\n",
      "COMET INFO:     val_total_acc [10]            : (0.9017338632901752, 0.9275414921299823)\n",
      "COMET INFO:     val_total_loss [10]           : (0.25377605123410873, 0.3304918277843194)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Dataset            : MNIST, SVHN\n",
      "COMET INFO:     batch_size         : 256\n",
      "COMET INFO:     epoch              : 10\n",
      "COMET INFO:     learning late      : 0.001\n",
      "COMET INFO:     mode               : training adapter\n",
      "COMET INFO:     optimizer          : Adam(0.9, 0.999)\n",
      "COMET INFO:     reconstruct loss   : l1 loss\n",
      "COMET INFO:     reconstruct loss λ : 1000.0\n",
      "COMET INFO:     weight decay       : 5e-05\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (14.48 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}