{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvision UFC101, pytorchvideo X3D scratch\n",
    "\n",
    "torchvisionのdatasetを使ってUFC101を読み込み，pytorchvideoのx3dモデルをスクラッチで学習してみる．\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．\n",
    "\n",
    "- https://pytorch.org/vision/stable/datasets.html?highlight=ucf101#torchvision.datasets.UCF101\n",
    "\n",
    "- https://github.com/facebookresearch/pytorchvideo/blob/ef2d3a96bb939b12aa0f21fb467d2175b0f05e9f/pytorchvideo/models/x3d.py#L537\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import UCF101\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "- rootで指定したディレクトリには，101クラスのサブディレクトリがあること\n",
    "- annotation_pathには，UCF101のアノテーションファイルであるtrainlist0{1,2,3}.txtなどがあること\n",
    "\n",
    "\n",
    "```bash\n",
    "$ ls UFC101/video | head\n",
    "ApplyEyeMakeup\n",
    "ApplyLipstick\n",
    "Archery\n",
    "BabyCrawling\n",
    "BalanceBeam\n",
    "BandMarching\n",
    "BaseballPitch\n",
    "Basketball\n",
    "BasketballDunk\n",
    "BenchPress\n",
    "\n",
    "$ head UCF101/ucfTrainTestlist/trainlist01.txt\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c01.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c03.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c04.avi 1\n",
    "ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c05.avi 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/HDD10TB/dataset/UFC101/'\n",
    "        self.root = self.metadata_path + 'video/'\n",
    "        self.annotation_path = self.metadata_path + 'ucfTrainTestlist/'\n",
    "        self.frames_per_clip = 16\n",
    "        self.step_between_clips = 16\n",
    "        self.model = 'X3D'\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 24\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformの定義．\n",
    "- UCF101を読み込むとuint8なので，255で割ってfloatにする．\n",
    "- torchvisionのUCF101データセットは(T, H, W, C)の形式．しかしpytorchvideoのx3dの入力形式は(B, C, T, H, W)らしいので，それに合わせる．\n",
    "- X3D-Mを想定して，画像を224x224にリサイズする．torchvision.transforms.Resizeはshapeが`[..., H, W]`ならOKなので，画像だけでなく動画もOK\n",
    " - https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/transforms/transforms.py#L227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/pevogam/starter-ucf101-with-pytorch\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x / 255.),\n",
    "    # (T, H, W, C) --> (C, T, H, W)\n",
    "    transforms.Lambda(lambda x: x.permute(3, 0, 1, 2)),\n",
    "    # transforms.Lambda(\n",
    "    #     lambda x: nn.functional.interpolate(x, (224, 224))),\n",
    "    transforms.Resize(224),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットはimage, audio, labelの三組を返すが，UCF101には音声がない動画もあり，そのまま使うとdataloaderがバッチにできないというエラーが出てしまう（audioの次元数がサンプルによって異なるため）．そこでcollateでaudioを取り除く．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_audio_collate(batch):\n",
    "    # https://www.kaggle.com/pevogam/starter-ucf101-with-pytorch\n",
    "    '''\n",
    "    remove audio channel because\n",
    "    not all of UCF101 vidoes have audio channel\n",
    "    '''\n",
    "    video_only_batch = []\n",
    "    for video, audio, label in batch:\n",
    "        video_only_batch.append((video, label))\n",
    "    return default_collate(video_only_batch)\n",
    "\n",
    "custom_collate = remove_audio_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メタデータの準備．UCF101の全動画をスキャンして，FPSなどの情報を取得するらしい．かなり時間がかかる．\n",
    "それを保存して再利用（毎回計算し直すと時間の無駄）．\n",
    "コードを見たところ，foldやtrainには無関係で，fpcとsbcにだけ依存するらしいので，それをファイル名にして保存する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filename = os.path.join(\n",
    "    args.metadata_path,\n",
    "    'UCF101metadata_fpc{}_sbc{}.pickle'.format(\n",
    "        args.frames_per_clip,\n",
    "        args.step_between_clips))\n",
    "\n",
    "if not os.path.exists(metadata_filename):\n",
    "    # precompute and save metadata\n",
    "    dataset_dict = UCF101(root=args.root,\n",
    "                            annotation_path=args.annotation_path,\n",
    "                            frames_per_clip=args.frames_per_clip,\n",
    "                            step_between_clips=args.step_between_clips,\n",
    "                            num_workers=args.num_workers,\n",
    "                            )\n",
    "    with open(metadata_filename, \"wb\") as f:\n",
    "        pickle.dump(dataset_dict.metadata, f)\n",
    "\n",
    "with open(metadata_filename, 'rb') as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCF101には3つのスプリットがあるので，foldでそれを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = UCF101(root=args.root,\n",
    "                    annotation_path=args.annotation_path,\n",
    "                    frames_per_clip=args.frames_per_clip,\n",
    "                    step_between_clips=args.step_between_clips,\n",
    "                    fold=1,\n",
    "                    train=True,\n",
    "                    transform=transform,\n",
    "                    _precomputed_metadata=metadata)\n",
    "val_set = UCF101(root=args.root,\n",
    "                    annotation_path=args.annotation_path,\n",
    "                    frames_per_clip=args.frames_per_clip,\n",
    "                    step_between_clips=args.step_between_clips,\n",
    "                    fold=1,\n",
    "                    train=False,\n",
    "                    transform=transform,\n",
    "                    _precomputed_metadata=metadata)\n",
    "n_classes = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データローダーの作成．collateをここで指定．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collate,\n",
    "                            num_workers=args.num_workers)\n",
    "val_loader = DataLoader(val_set,\n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=False,\n",
    "                        drop_last=True,\n",
    "                        collate_fn=custom_collate,\n",
    "                        num_workers=args.num_workers)\n"
   ]
  },
  {
   "source": [
    "data loaderの挙動を確認．ランダムなラベルが得られている"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[72 56  0 74 53 61 84 46 77 70 49 82 68 32 73 36]\n",
      "[73 68 30 93  8 81 87 93 84 67 70 65 46 44 64 77]\n",
      "[51 20 75 34 50  3  5 60 13 94  3 67 24 43 43 34]\n",
      "[58 77 25  9 43 94 65 61 78 17 90 12 85 67 20 74]\n",
      "[40 11 61 61 88 59 87 57 73  0 94  6 55 94 98 73]\n",
      "[ 66  19  68  75  94  10  88  91  64  46  55  14  73  50  55 100]\n",
      "[88 36  3 61 11 45 86 90 52 83 75 37 40 18 80 40]\n",
      "[41 61  7 63 55 32 35 46 69 73 59 77 38 46 61 50]\n",
      "[39 53 65 24 21 36 62 94 40 40 53 83 83 40 20 26]\n",
      "[69 70 68  7 51  1 70 68 60 13 76 11 61 73 68 71]\n",
      "[13 24 75 26 79 62 75 52 68 46 95 89 45 54 33 31]\n",
      "[51 45 70 93 70 64 12 13 77  7 62 76 97 75 70 85]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# torchvisionのvideo.pyで，ワーニングが多数出るのでそれを抑制．\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "                                   module='torchvision')\n",
    "\n",
    "\n",
    "for i, (data, label) in enumerate(train_loader):\n",
    "    print(label.cpu().numpy())\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "source": [
    "データローダーのlenを確認する．\n",
    "\n",
    "- 学習用ビデオ数は9000程度のはずなのに，train_setのlengthは非常に多い\n",
    "  - おそらく，各ビデオからサンプリしたclip数になっている\n",
    "  - 各ビデオから同じ数のクリップがサンプルされているとは限らない（確認できていない）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6692, 107085, 6692.8125)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(train_loader), len(train_set), len(train_set) / args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchvideoのx3dモデルを作成．\n",
    "webマニュアルにはないが，コードをみると，クリップ長とサイズが指定できる．\n",
    "X3Dは数種類あるが，ここではX3D-Mに合わせた数字を指定（コードのコメントに書いてある）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X3D-M\n",
    "# https://github.com/facebookresearch/pytorchvideo/blob/master/pytorchvideo/models/x3d.py#L601\n",
    "model = x3d.create_x3d(\n",
    "    input_clip_length=16,\n",
    "    input_crop_size=224,\n",
    "    depth_factor=2.2,\n",
    "    model_num_class=101\n",
    ").to(device)\n",
    "\n",
    "# model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムなデータを流し込んで出力されるかを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(2, 3, 16, 224, 224).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0114, 0.0083, 0.0091, 0.0103, 0.0102, 0.0109, 0.0089, 0.0087, 0.0097,\n",
       "         0.0104, 0.0103, 0.0089, 0.0098, 0.0101, 0.0092, 0.0092, 0.0083, 0.0104,\n",
       "         0.0082, 0.0104, 0.0098, 0.0112, 0.0095, 0.0087, 0.0114, 0.0095, 0.0095,\n",
       "         0.0077, 0.0104, 0.0122, 0.0095, 0.0110, 0.0103, 0.0105, 0.0071, 0.0101,\n",
       "         0.0090, 0.0096, 0.0085, 0.0080, 0.0127, 0.0094, 0.0129, 0.0105, 0.0089,\n",
       "         0.0094, 0.0105, 0.0091, 0.0077, 0.0091, 0.0099, 0.0104, 0.0096, 0.0086,\n",
       "         0.0083, 0.0106, 0.0087, 0.0092, 0.0102, 0.0116, 0.0101, 0.0091, 0.0099,\n",
       "         0.0103, 0.0094, 0.0102, 0.0118, 0.0099, 0.0089, 0.0094, 0.0104, 0.0139,\n",
       "         0.0102, 0.0100, 0.0075, 0.0117, 0.0102, 0.0116, 0.0089, 0.0089, 0.0092,\n",
       "         0.0118, 0.0092, 0.0098, 0.0103, 0.0113, 0.0088, 0.0105, 0.0094, 0.0081,\n",
       "         0.0127, 0.0099, 0.0097, 0.0102, 0.0103, 0.0106, 0.0101, 0.0095, 0.0106,\n",
       "         0.0109, 0.0109],\n",
       "        [0.0094, 0.0090, 0.0088, 0.0100, 0.0106, 0.0103, 0.0104, 0.0099, 0.0108,\n",
       "         0.0094, 0.0096, 0.0105, 0.0106, 0.0110, 0.0093, 0.0095, 0.0077, 0.0114,\n",
       "         0.0113, 0.0118, 0.0095, 0.0101, 0.0097, 0.0086, 0.0107, 0.0099, 0.0090,\n",
       "         0.0075, 0.0109, 0.0109, 0.0089, 0.0104, 0.0102, 0.0114, 0.0091, 0.0101,\n",
       "         0.0085, 0.0083, 0.0087, 0.0102, 0.0103, 0.0102, 0.0126, 0.0084, 0.0085,\n",
       "         0.0108, 0.0102, 0.0088, 0.0097, 0.0102, 0.0100, 0.0106, 0.0100, 0.0073,\n",
       "         0.0100, 0.0114, 0.0098, 0.0093, 0.0090, 0.0115, 0.0092, 0.0122, 0.0094,\n",
       "         0.0106, 0.0101, 0.0112, 0.0111, 0.0106, 0.0096, 0.0084, 0.0092, 0.0103,\n",
       "         0.0102, 0.0090, 0.0085, 0.0119, 0.0106, 0.0097, 0.0081, 0.0105, 0.0109,\n",
       "         0.0104, 0.0093, 0.0105, 0.0088, 0.0114, 0.0096, 0.0094, 0.0093, 0.0083,\n",
       "         0.0102, 0.0111, 0.0077, 0.0095, 0.0105, 0.0097, 0.0096, 0.0089, 0.0105,\n",
       "         0.0107, 0.0108]], device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summaryで中身を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape\n",
       "==============================================================================================================\n",
       "Net                                                          --                        --\n",
       "├─ModuleList (blocks)                                        --                        --\n",
       "│    └─ResNetBasicStem (0)                                   [4, 3, 16, 224, 224]      [4, 24, 16, 112, 112]\n",
       "│    │    └─Conv2plus1d (conv)                               [4, 3, 16, 224, 224]      [4, 24, 16, 112, 112]\n",
       "│    │    │    └─Conv3d (conv_t)                             [4, 3, 16, 224, 224]      [4, 24, 16, 112, 112]\n",
       "│    │    │    └─Conv3d (conv_xy)                            [4, 24, 16, 112, 112]     [4, 24, 16, 112, 112]\n",
       "│    │    └─BatchNorm3d (norm)                               [4, 24, 16, 112, 112]     [4, 24, 16, 112, 112]\n",
       "│    │    └─ReLU (activation)                                [4, 24, 16, 112, 112]     [4, 24, 16, 112, 112]\n",
       "│    └─ResStage (1)                                          [4, 24, 16, 112, 112]     [4, 24, 16, 56, 56]\n",
       "│    │    └─ModuleList (res_blocks)                          --                        --\n",
       "│    │    │    └─ResBlock (0)                                [4, 24, 16, 112, 112]     [4, 24, 16, 56, 56]\n",
       "│    │    │    └─ResBlock (1)                                [4, 24, 16, 56, 56]       [4, 24, 16, 56, 56]\n",
       "│    │    │    └─ResBlock (2)                                [4, 24, 16, 56, 56]       [4, 24, 16, 56, 56]\n",
       "│    └─ResStage (2)                                          [4, 24, 16, 56, 56]       [4, 48, 16, 28, 28]\n",
       "│    │    └─ModuleList (res_blocks)                          --                        --\n",
       "│    │    │    └─ResBlock (0)                                [4, 24, 16, 56, 56]       [4, 48, 16, 28, 28]\n",
       "│    │    │    └─ResBlock (1)                                [4, 48, 16, 28, 28]       [4, 48, 16, 28, 28]\n",
       "│    │    │    └─ResBlock (2)                                [4, 48, 16, 28, 28]       [4, 48, 16, 28, 28]\n",
       "│    │    │    └─ResBlock (3)                                [4, 48, 16, 28, 28]       [4, 48, 16, 28, 28]\n",
       "│    │    │    └─ResBlock (4)                                [4, 48, 16, 28, 28]       [4, 48, 16, 28, 28]\n",
       "│    └─ResStage (3)                                          [4, 48, 16, 28, 28]       [4, 96, 16, 14, 14]\n",
       "│    │    └─ModuleList (res_blocks)                          --                        --\n",
       "│    │    │    └─ResBlock (0)                                [4, 48, 16, 28, 28]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (1)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (2)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (3)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (4)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (5)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (6)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (7)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (8)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (9)                                [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    │    │    └─ResBlock (10)                               [4, 96, 16, 14, 14]       [4, 96, 16, 14, 14]\n",
       "│    └─ResStage (4)                                          [4, 96, 16, 14, 14]       [4, 192, 16, 7, 7]\n",
       "│    │    └─ModuleList (res_blocks)                          --                        --\n",
       "│    │    │    └─ResBlock (0)                                [4, 96, 16, 14, 14]       [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (1)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (2)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (3)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (4)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (5)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    │    │    └─ResBlock (6)                                [4, 192, 16, 7, 7]        [4, 192, 16, 7, 7]\n",
       "│    └─ResNetBasicHead (5)                                   [4, 192, 16, 7, 7]        [4, 101]\n",
       "│    │    └─ProjectedPool (pool)                             [4, 192, 16, 7, 7]        [4, 2048, 1, 1, 1]\n",
       "│    │    │    └─Conv3d (pre_conv)                           [4, 192, 16, 7, 7]        [4, 432, 16, 7, 7]\n",
       "│    │    │    └─BatchNorm3d (pre_norm)                      [4, 432, 16, 7, 7]        [4, 432, 16, 7, 7]\n",
       "│    │    │    └─ReLU (pre_act)                              [4, 432, 16, 7, 7]        [4, 432, 16, 7, 7]\n",
       "│    │    │    └─AvgPool3d (pool)                            [4, 432, 16, 7, 7]        [4, 432, 1, 1, 1]\n",
       "│    │    │    └─Conv3d (post_conv)                          [4, 432, 1, 1, 1]         [4, 2048, 1, 1, 1]\n",
       "│    │    │    └─ReLU (post_act)                             [4, 2048, 1, 1, 1]        [4, 2048, 1, 1, 1]\n",
       "│    │    └─Dropout (dropout)                                [4, 2048, 1, 1, 1]        [4, 2048, 1, 1, 1]\n",
       "│    │    └─Linear (proj)                                    [4, 1, 1, 1, 2048]        [4, 1, 1, 1, 101]\n",
       "│    │    └─Softmax (activation)                             [4, 101, 1, 1, 1]         [4, 101, 1, 1, 1]\n",
       "│    │    └─AdaptiveAvgPool3d (output_pool)                  [4, 101, 1, 1, 1]         [4, 101, 1, 1, 1]\n",
       "==============================================================================================================\n",
       "Total params: 3,181,623\n",
       "Trainable params: 3,181,623\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 18.93\n",
       "==============================================================================================================\n",
       "Input size (MB): 38.54\n",
       "Forward/backward pass size (MB): 5433.65\n",
       "Params size (MB): 12.73\n",
       "Estimated Total Size (MB): 5484.91\n",
       "=============================================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "torchinfo.summary(\n",
    "    model,\n",
    "    (4, 3, 16, 224, 224),\n",
    "    depth=4,\n",
    "    col_names=[\"input_size\",\n",
    "               \"output_size\"],\n",
    "    row_settings=(\"var_names\",)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "便利関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvisionのvideo.pyで，ワーニングが多数出るのでそれを抑制．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "                                   module='torchvision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f02f46b38f746edbf6103ea2f947189"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6692.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c00ca2c718744479b273f3b674966f5c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8b2d3b4aef31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             F.sgd(params_with_grad,\n\u001b[0m\u001b[1;32m    111\u001b[0m                   \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                   \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(train_loader),\n",
    "                  total=len(train_loader),\n",
    "                  leave=True) as pbar_loss:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, (inputs, targets) in pbar_loss:\n",
    "                pbar_loss.set_description(\"[train]\")\n",
    "\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                bs = inputs.size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, targets), bs)\n",
    "\n",
    "                pbar_loss.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                ))\n",
    "\n"
   ]
  },
  {
   "source": [
    "- 4GPUでおよそ2.4it/s，1エポック約50分\n",
    "- 1GPUでおよそ1.3it/s，1エポック約1時間半\n",
    "\n",
    "\n",
    "以下の設定のとおり\n",
    "- frames_per_clip = 16\n",
    "- step_between_clips = 16\n",
    "- batch_size = 16\n",
    "- num_workers = 24"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}