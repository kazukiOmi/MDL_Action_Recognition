{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# pytorchvideo UFC101, pytorchvideo X3D pretrain/scratch\n",
    "\n",
    "pytorchvideonのdatasetを使ってUFC101を読み込み，pytorchvideoのx3dモデルをfine-tuningしてみる．\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．\n",
    "\n",
    "- https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#ucf101\n",
    "\n",
    "- https://pytorch.org/hub/facebookresearch_pytorchvideo_x3d/\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ダウンロードできないというエラー\n",
    "\n",
    "torchvisionをimportした後ではエラーが発生する（ImportError: cannot import name ***）\n",
    "\n",
    "- https://github.com/pytorch/hub/issues/46\n",
    "\n",
    "\n",
    "## 対応策\n",
    "\n",
    "import torch直後に（import torchvisionをしない状態で）torch.hub.loadして，キャッシュに残しておく\n",
    "\n",
    "こうすると，以降はキャッシュ（~/.cache/torch/hub/checkpoints/）が使われるのでエラーは発生しない"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "import torch\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_xs', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import Ucf101, RandomClipSampler, UniformClipSampler\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "\n",
    "#import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "- rootで指定したディレクトリには，101クラスのサブディレクトリがあること\n",
    "- annotation_pathにはtrainlist0{1,2,3}.txtなどがあること"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/NAS-TVS872XT/dataset/UFC101/'\n",
    "        self.root = self.metadata_path + 'video/'\n",
    "        self.annotation_path = self.metadata_path + 'ucfTrainTestlist/'\n",
    "        self.frames_per_clip = 16\n",
    "        self.step_between_clips = 16\n",
    "        self.model = 'x3d_m'\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 24\n",
    "\n",
    "        self.clip_duration = 16/25  # 25FPSを想定して16枚\n",
    "        self.video_num_subsampled = 16  # 16枚抜き出す\n",
    "\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "transformの定義．\n",
    "- UniformTemporalSubsampleで固定枚数をサンプルする\n",
    " - datasetのclip_samplerには，秒単位でしか与えられないようなので，fpsが異なる動画ではサンプルされる枚数も変わってくる．そのためここで取得するフレーム数を揃える（もっといい方法はないのか？）\n",
    "- UCF101を読み込むとfloat32だが値は0-255，255で割ってfloatにする．\n",
    "- X3D-Mを想定して，短い方を256画素程度に合わせてから，画像を224x224にリサイズする．\n",
    "  - RandomShortSideScaleなら厳密には256にならない\n",
    "  - ShortSideScaleなら256になる\n",
    "\n",
    "バッチはdict形式なので，video, label, audioなどのそれぞれにtransformが設定できる\n",
    "- ApplyTransformToKeyでkeyを指定して，video用のtransformを設定\n",
    "- UCF101のラベルファイル（trainlist01.txtなど）には1から101までのラベルが付いているが，それがそのまま使われてしまうので（なぜだ．．．），このままではエラーが（不定期に）発生する．ラベルの値をtransformでから100にしておく\n",
    "- audioは使わないのでRemoveKeyで除去"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ## 以下デバッグ用\n",
    "                # transforms.Lambda(lambda x: [\n",
    "                #     x, \n",
    "                #     print(type(x)),\n",
    "                #     print(x.dtype),\n",
    "                #     print(x.max()),\n",
    "                #     print(x.min()),\n",
    "                #     print(x.mean()),\n",
    "                #     ]),\n",
    "                # transforms.Lambda(lambda x: x[0]),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(256),\n",
    "                CenterCrop(224),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "root_UCF101 = '/mnt/NAS-TVS872XT/dataset/UCF101/'\n",
    "\n",
    "train_set = Ucf101(\n",
    "    data_path=root_UCF101 + 'ucfTrainTestlist/trainlist01.txt',  # ラベルが1から101になっているので，transformで1を引いている\n",
    "    video_path_prefix=root_UCF101 + 'video/',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=args.clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    "    )\n",
    "val_set = Ucf101(\n",
    "    data_path=root_UCF101 + 'ucfTrainTestlist/testlist01.txt',\n",
    "    video_path_prefix=root_UCF101 + 'video/',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=args.clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    "    )\n",
    "\n",
    "num_classes = 101"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "train_set.num_videos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9537"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "val_set.num_videos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3783"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# https://github.com/facebookresearch/pytorchvideo/blob/ef2d3a96bb939b12aa0f21fb467d2175b0f05e9f/tutorials/video_classification_example/train.py#L343\n",
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    To ensure a constant number of samples are retrieved from the dataset we use this\n",
    "    LimitDataset wrapper. This is necessary because several of the underlying videos\n",
    "    may be corrupted while fetching or decoding, however, we always want the same\n",
    "    number of steps per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "train_loader = DataLoader(LimitDataset(train_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n",
    "val_loader = DataLoader(LimitDataset(val_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:475: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "データローダのlenを確認．\n",
    "- trainlist01.txtには9537行あるので「サンプル数＝ビデオ数」\n",
    "- バッチサイズで割るとtrain_loaderのlengthになる"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "len(train_loader), train_set.num_videos, train_set.num_videos / args.batch_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(596, 9537, 596.0625)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "data loaderの挙動を確認．\n",
    "- バッチはdictでやってくるので，`batch['video']`と`batch['label']`で取り出す\n",
    "- RandomClipSamplerならランダムなラベルが得られている．"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(batch.keys())\n",
    "        print(batch['video'].shape)\n",
    "    print(batch['label'].cpu().numpy())\n",
    "    if i > 10:\n",
    "        break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])\n",
      "torch.Size([16, 3, 16, 224, 224])\n",
      "[79 58 92 52  3 43 62 82 53 60 10 31 91 52  5  6]\n",
      "[ 4 12 69 82  4 95 50 80 49  9 80 70 10 64 70 46]\n",
      "[95 91  0 52 19 57 77 23 23 86 69  3 17 78 34 83]\n",
      "[90  5 72 48 85 81 45 53 94 54 94 42 36 98 41 78]\n",
      "[ 6 81 91 87 18 23 11  4 72 21 52 25 69 24 25 78]\n",
      "[13 84 93 87 12 41 66 16 61 72 24 73 28 17 19 64]\n",
      "[14 33 64 15 13 46 73 19 14 84 40 75 37 40 97 96]\n",
      "[41 33 86 68 54 89 67 45 11 86 16  3 65 38 64 59]\n",
      "[18 26 98 98  8 21 35 34 66  1  3 96 82 71 53 84]\n",
      "[43 26 34 92 41 33 20 10 84 70 52 21 24 82 31 36]\n",
      "[26 59 64 53 92 77 12 13 34 58 60  0 51 37 39 88]\n",
      "[41 54 41 78 12 26  8 96 14 81 37 66 50 55 12 46]\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "pytorchvideoのpretrained x3dモデルをダウンロード．\n",
    "あとでsummaryを見れば分かるように，最終線形層は`model.blocks[5].proj`だからこれをnn.Linearに置き換える\n",
    "\n",
    "- 注意：エラーが発生してダウンロードできない場合には，このnotebookの冒頭の注意書きを確認すること"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# # X3D-M\n",
    "# # https://github.com/facebookresearch/pytorchvideo/blob/master/pytorchvideo/models/x3d.py#L601\n",
    "# model = x3d.create_x3d(\n",
    "#     input_clip_length=16,\n",
    "#     input_crop_size=224,\n",
    "#     depth_factor=2.2,\n",
    "#     model_num_class=101\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)\n",
    "\n",
    "# fine-tuningするなら以下を実行．スクラッチで学習するなら，実行しない\n",
    "do_fine_tune = True\n",
    "if do_fine_tune:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.blocks[5].proj = nn.Linear(model.blocks[5].proj.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# data parallelだと性能が落ちる（設定次第？）\n",
    "# model = nn.DataParallel(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ランダムなデータを流し込んで出力されるかを確認する"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "data = torch.randn(2, 3, 16, 224, 224).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "model(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0112, 0.0102, 0.0098, 0.0099, 0.0096, 0.0096, 0.0090, 0.0101, 0.0093,\n",
       "         0.0108, 0.0096, 0.0086, 0.0101, 0.0103, 0.0109, 0.0096, 0.0094, 0.0095,\n",
       "         0.0100, 0.0093, 0.0096, 0.0097, 0.0110, 0.0092, 0.0114, 0.0088, 0.0103,\n",
       "         0.0110, 0.0097, 0.0096, 0.0107, 0.0088, 0.0087, 0.0104, 0.0105, 0.0117,\n",
       "         0.0096, 0.0090, 0.0101, 0.0104, 0.0091, 0.0105, 0.0104, 0.0093, 0.0101,\n",
       "         0.0099, 0.0114, 0.0109, 0.0095, 0.0106, 0.0086, 0.0112, 0.0098, 0.0101,\n",
       "         0.0090, 0.0092, 0.0102, 0.0096, 0.0105, 0.0117, 0.0104, 0.0102, 0.0089,\n",
       "         0.0093, 0.0109, 0.0095, 0.0111, 0.0084, 0.0085, 0.0117, 0.0110, 0.0090,\n",
       "         0.0090, 0.0101, 0.0106, 0.0093, 0.0092, 0.0095, 0.0086, 0.0106, 0.0106,\n",
       "         0.0092, 0.0099, 0.0111, 0.0089, 0.0085, 0.0094, 0.0102, 0.0108, 0.0103,\n",
       "         0.0097, 0.0109, 0.0094, 0.0087, 0.0099, 0.0101, 0.0090, 0.0101, 0.0111,\n",
       "         0.0101, 0.0078],\n",
       "        [0.0087, 0.0098, 0.0103, 0.0110, 0.0089, 0.0088, 0.0101, 0.0090, 0.0093,\n",
       "         0.0097, 0.0097, 0.0093, 0.0093, 0.0096, 0.0088, 0.0106, 0.0106, 0.0102,\n",
       "         0.0101, 0.0096, 0.0107, 0.0114, 0.0104, 0.0099, 0.0097, 0.0092, 0.0106,\n",
       "         0.0098, 0.0089, 0.0101, 0.0099, 0.0105, 0.0106, 0.0092, 0.0085, 0.0102,\n",
       "         0.0102, 0.0101, 0.0111, 0.0089, 0.0082, 0.0110, 0.0097, 0.0111, 0.0101,\n",
       "         0.0102, 0.0103, 0.0118, 0.0101, 0.0109, 0.0094, 0.0102, 0.0097, 0.0126,\n",
       "         0.0097, 0.0123, 0.0096, 0.0094, 0.0109, 0.0080, 0.0104, 0.0103, 0.0099,\n",
       "         0.0100, 0.0100, 0.0090, 0.0112, 0.0097, 0.0095, 0.0094, 0.0095, 0.0101,\n",
       "         0.0097, 0.0098, 0.0098, 0.0095, 0.0092, 0.0102, 0.0093, 0.0105, 0.0110,\n",
       "         0.0089, 0.0101, 0.0103, 0.0106, 0.0097, 0.0099, 0.0093, 0.0084, 0.0095,\n",
       "         0.0097, 0.0095, 0.0102, 0.0102, 0.0088, 0.0093, 0.0097, 0.0102, 0.0099,\n",
       "         0.0104, 0.0089]], device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "summaryで中身を確認"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# torchinfo.summary(\n",
    "#     model,\n",
    "#     (4, 3, 16, 224, 224),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-33-a91382979121>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-a91382979121>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    model,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "便利関数を定義"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(train_loader),\n",
    "                  total=len(train_loader),\n",
    "                  leave=True) as pbar_loss:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, batch in pbar_loss:\n",
    "                pbar_loss.set_description(\"[train]\")\n",
    "\n",
    "                inputs, targets = batch['video'].to(device), batch['label'].to(device)\n",
    "                bs = inputs.size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, targets), bs)\n",
    "\n",
    "                pbar_loss.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                    train_loss.val, train_acc.val,\n",
    "                ))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10a6db9edcc642cf820d605db387cdbb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a889299bf234727a27eb3770ed4e1a6"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "fine-tuningなので速い．\n",
    "- 4GPUでおよそ4.5it/s，1エポック約2分\n",
    "- 1GPUでおよそ5it/s，1エポック約3分（596 iterations）\n",
    "\n",
    "スクラッチで学習するなら\n",
    "- 4GPUでおよそ2.6it/s，1エポック約4分\n",
    "- 1GPUでおよそ1.8it/s，1エポック約5.5分（596 iterations）\n",
    "\n",
    "\n",
    "以下の設定\n",
    "- video_num_subsampled = 16\n",
    "- batch_size = 16\n",
    "- num_workers = 24"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}