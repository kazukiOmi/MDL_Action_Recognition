{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# pytorchvideo UFC101, pytorchvideo X3D pretrain/scratch\n",
    "\n",
    "pytorchvideonのdatasetを使ってUFC101を読み込み，pytorchvideoのx3dモデルをfine-tuningしてみる．\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．\n",
    "\n",
    "- https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#ucf101\n",
    "\n",
    "- https://pytorch.org/hub/facebookresearch_pytorchvideo_x3d/\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ダウンロードできないというエラー\n",
    "\n",
    "torchvisionをimportした後ではエラーが発生する（ImportError: cannot import name ***）\n",
    "\n",
    "- https://github.com/pytorch/hub/issues/46\n",
    "\n",
    "\n",
    "## 対応策\n",
    "\n",
    "import torch直後に（import torchvisionをしない状態で）torch.hub.loadして，キャッシュに残しておく\n",
    "\n",
    "こうすると，以降はキャッシュ（~/.cache/torch/hub/checkpoints/）が使われるのでエラーは発生しない"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "\n",
    "import torch\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_xs', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n",
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import Ucf101, RandomClipSampler, UniformClipSampler, Kinetics\n",
    "\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "\n",
    "\n",
    "#import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "- rootで指定したディレクトリには，101クラスのサブディレクトリがあること\n",
    "- annotation_pathにはtrainlist0{1,2,3}.txtなどがあること"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/NAS-TVS872XT/dataset/Kinetics400/'\n",
    "        self.root = self.metadata_path\n",
    "        self.annotation_path = self.metadata_path\n",
    "        self.frames_per_clip = 16\n",
    "        self.step_between_clips = 16\n",
    "        self.model = 'x3d_m'\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 24\n",
    "\n",
    "        self.clip_duration = 16/25  # 25FPSを想定して16枚\n",
    "        self.video_num_subsampled = 16  # 16枚抜き出す\n",
    "\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "transformの定義．\n",
    "- UniformTemporalSubsampleで固定枚数をサンプルする\n",
    " - datasetのclip_samplerには，秒単位でしか与えられないようなので，fpsが異なる動画ではサンプルされる枚数も変わってくる．そのためここで取得するフレーム数を揃える（もっといい方法はないのか？）\n",
    "- UCF101を読み込むとfloat32だが値は0-255，255で割ってfloatにする．\n",
    "- X3D-Mを想定して，短い方を256画素程度に合わせてから，画像を224x224にリサイズする．\n",
    "  - RandomShortSideScaleなら厳密には256にならない\n",
    "  - ShortSideScaleなら256になる\n",
    "\n",
    "バッチはdict形式なので，video, label, audioなどのそれぞれにtransformが設定できる\n",
    "- ApplyTransformToKeyでkeyを指定して，video用のtransformを設定\n",
    "- UCF101のラベルファイル（trainlist01.txtなど）には1から101までのラベルが付いているが，それがそのまま使われてしまうので（なぜだ．．．），このままではエラーが（不定期に）発生する．ラベルの値をtransformでから100にしておく\n",
    "- audioは使わないのでRemoveKeyで除去"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ## 以下デバッグ用\n",
    "                # transforms.Lambda(lambda x: [\n",
    "                #     x, \n",
    "                #     print(type(x)),\n",
    "                #     print(x.dtype),\n",
    "                #     print(x.max()),\n",
    "                #     print(x.min()),\n",
    "                #     print(x.mean()),\n",
    "                #     ]),\n",
    "                # transforms.Lambda(lambda x: x[0]),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "                UniformTemporalSubsample(args.video_num_subsampled),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(256),\n",
    "                CenterCrop(224),\n",
    "        ]),\n",
    "    ),\n",
    "    ApplyTransformToKey(\n",
    "        key=\"label\",\n",
    "        # ラベルが1から101になっているので，1を引いておく\n",
    "        transform=transforms.Lambda(lambda x: x - 1),\n",
    "    ),\n",
    "    RemoveKey(\"audio\"),\n",
    "])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "root_UCF101 = '/mnt/NAS-TVS872XT/dataset/Kinetics400/'\n",
    "\n",
    "train_set = Kinetics(\n",
    "    data_path=root_UCF101 + 'train',  # ラベルが1から101になっているので，transformで1を引いている\n",
    "    video_path_prefix=root_UCF101 + 'train',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=args.clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    "    )\n",
    "val_set = Kinetics(\n",
    "    data_path=root_UCF101 + 'val',\n",
    "    video_path_prefix=root_UCF101 + 'val',\n",
    "    clip_sampler=RandomClipSampler(clip_duration=args.clip_duration),\n",
    "    video_sampler=RandomSampler,\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    "    )\n",
    "\n",
    "num_classes = 400"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "train_set.num_videos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "240258"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "val_set.num_videos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19881"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# https://github.com/facebookresearch/pytorchvideo/blob/ef2d3a96bb939b12aa0f21fb467d2175b0f05e9f/tutorials/video_classification_example/train.py#L343\n",
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    To ensure a constant number of samples are retrieved from the dataset we use this\n",
    "    LimitDataset wrapper. This is necessary because several of the underlying videos\n",
    "    may be corrupted while fetching or decoding, however, we always want the same\n",
    "    number of steps per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "train_loader = DataLoader(LimitDataset(train_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n",
    "val_loader = DataLoader(LimitDataset(val_set),\n",
    "                            batch_size=args.batch_size,\n",
    "                            drop_last=True,\n",
    "                            num_workers=args.num_workers)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "データローダのlenを確認．\n",
    "- trainlist01.txtには9537行あるので「サンプル数＝ビデオ数」\n",
    "- バッチサイズで割るとtrain_loaderのlengthになる"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "len(train_loader), train_set.num_videos, train_set.num_videos / args.batch_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15016, 240258, 15016.125)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "data loaderの挙動を確認．\n",
    "- バッチはdictでやってくるので，`batch['video']`と`batch['label']`で取り出す\n",
    "- RandomClipSamplerならランダムなラベルが得られている．"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(batch.keys())\n",
    "        print(batch['video'].shape)\n",
    "    print(batch['label'].cpu().numpy())\n",
    "    if i > 10:\n",
    "        break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "moov atom not found\n",
      "moov atom not found\n",
      "moov atom not found\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])\n",
      "torch.Size([16, 3, 16, 224, 224])\n",
      "[339  18 226 383  92 152 272 320 101 145 292 156 352 243 259 280]\n",
      "[379  55 225  34  75 350 350 176 187  78 322 171 322 322 106  18]\n",
      "[102 208 214 199 258 122 132 163 168 202  30 195 283 288 257 255]\n",
      "[  0 303  88 120 142 363  92 214 233 158 252 124  83 125  62 203]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "moov atom not found\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[168 326   6 331 171  39  30 236 340 208 308 303 390 288 147  89]\n",
      "[221 115  61 226 364 166 345 275  24 192 236 142 123 147  90 305]\n",
      "[241 329 262 322 289 383 269  33 332 326 131 356 312 130 141 166]\n",
      "[ 52 114  65 155 375  21 378  71 142 208 392   3 318 141 312 144]\n",
      "[372 276  40 150 130 130 195 316 370 267 377 140 233  62 148  97]\n",
      "[196 107 386 155 327  36 146 308 377 269 121  76 106 262 193  68]\n",
      "[ 54 111 181  23 309 300 349  34  30 159 228 369 342  -1  22 163]\n",
      "[275 173 130 255 133 198 274 332  18  36 163 229 193 120 169 205]\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "pytorchvideoのpretrained x3dモデルをダウンロード．\n",
    "あとでsummaryを見れば分かるように，最終線形層は`model.blocks[5].proj`だからこれをnn.Linearに置き換える\n",
    "\n",
    "- 注意：エラーが発生してダウンロードできない場合には，このnotebookの冒頭の注意書きを確認すること"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# # X3D-M\n",
    "# # https://github.com/facebookresearch/pytorchvideo/blob/master/pytorchvideo/models/x3d.py#L601\n",
    "# model = x3d.create_x3d(\n",
    "#     input_clip_length=16,\n",
    "#     input_crop_size=224,\n",
    "#     depth_factor=2.2,\n",
    "#     model_num_class=101\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)\n",
    "\n",
    "# fine-tuningするなら以下を実行．スクラッチで学習するなら，実行しない\n",
    "do_fine_tune = True\n",
    "if do_fine_tune:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.blocks[5].proj = nn.Linear(model.blocks[5].proj.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# data parallelだと性能が落ちる（設定次第？）\n",
    "# model = nn.DataParallel(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/omi/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-1a0678b458f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# data parallelだと性能が落ちる（設定次第？）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    813\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ランダムなデータを流し込んで出力されるかを確認する"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "data = torch.randn(2, 3, 16, 224, 224).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "model(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0025, 0.0026, 0.0027, 0.0026, 0.0026, 0.0027, 0.0027, 0.0023, 0.0023,\n",
       "         0.0025, 0.0021, 0.0024, 0.0026, 0.0022, 0.0024, 0.0023, 0.0026, 0.0023,\n",
       "         0.0029, 0.0023, 0.0024, 0.0027, 0.0022, 0.0027, 0.0026, 0.0023, 0.0021,\n",
       "         0.0026, 0.0024, 0.0027, 0.0027, 0.0026, 0.0026, 0.0026, 0.0024, 0.0024,\n",
       "         0.0026, 0.0024, 0.0024, 0.0025, 0.0023, 0.0025, 0.0027, 0.0023, 0.0025,\n",
       "         0.0028, 0.0023, 0.0026, 0.0027, 0.0029, 0.0024, 0.0023, 0.0025, 0.0026,\n",
       "         0.0024, 0.0026, 0.0027, 0.0023, 0.0025, 0.0025, 0.0023, 0.0025, 0.0024,\n",
       "         0.0022, 0.0024, 0.0021, 0.0024, 0.0025, 0.0025, 0.0028, 0.0024, 0.0023,\n",
       "         0.0024, 0.0025, 0.0022, 0.0025, 0.0026, 0.0023, 0.0027, 0.0025, 0.0024,\n",
       "         0.0025, 0.0024, 0.0024, 0.0027, 0.0023, 0.0024, 0.0026, 0.0027, 0.0029,\n",
       "         0.0026, 0.0027, 0.0025, 0.0026, 0.0026, 0.0025, 0.0023, 0.0026, 0.0026,\n",
       "         0.0024, 0.0025, 0.0025, 0.0024, 0.0022, 0.0028, 0.0025, 0.0024, 0.0028,\n",
       "         0.0026, 0.0024, 0.0025, 0.0025, 0.0023, 0.0025, 0.0025, 0.0027, 0.0025,\n",
       "         0.0025, 0.0025, 0.0025, 0.0024, 0.0027, 0.0026, 0.0026, 0.0024, 0.0025,\n",
       "         0.0025, 0.0027, 0.0024, 0.0025, 0.0023, 0.0026, 0.0025, 0.0024, 0.0025,\n",
       "         0.0025, 0.0023, 0.0025, 0.0026, 0.0025, 0.0025, 0.0025, 0.0025, 0.0024,\n",
       "         0.0025, 0.0026, 0.0023, 0.0024, 0.0025, 0.0025, 0.0023, 0.0025, 0.0025,\n",
       "         0.0023, 0.0025, 0.0023, 0.0024, 0.0026, 0.0025, 0.0022, 0.0025, 0.0024,\n",
       "         0.0027, 0.0025, 0.0025, 0.0027, 0.0024, 0.0025, 0.0025, 0.0024, 0.0025,\n",
       "         0.0025, 0.0028, 0.0026, 0.0025, 0.0027, 0.0026, 0.0025, 0.0024, 0.0024,\n",
       "         0.0023, 0.0024, 0.0029, 0.0024, 0.0027, 0.0024, 0.0024, 0.0024, 0.0026,\n",
       "         0.0026, 0.0024, 0.0025, 0.0027, 0.0024, 0.0024, 0.0026, 0.0022, 0.0027,\n",
       "         0.0026, 0.0025, 0.0027, 0.0026, 0.0027, 0.0026, 0.0028, 0.0028, 0.0025,\n",
       "         0.0024, 0.0027, 0.0026, 0.0024, 0.0026, 0.0027, 0.0022, 0.0025, 0.0024,\n",
       "         0.0025, 0.0024, 0.0023, 0.0026, 0.0024, 0.0025, 0.0024, 0.0023, 0.0026,\n",
       "         0.0026, 0.0025, 0.0025, 0.0026, 0.0026, 0.0025, 0.0026, 0.0026, 0.0028,\n",
       "         0.0025, 0.0025, 0.0027, 0.0025, 0.0023, 0.0025, 0.0026, 0.0023, 0.0025,\n",
       "         0.0026, 0.0024, 0.0024, 0.0024, 0.0027, 0.0025, 0.0026, 0.0026, 0.0025,\n",
       "         0.0025, 0.0025, 0.0024, 0.0026, 0.0023, 0.0024, 0.0026, 0.0027, 0.0027,\n",
       "         0.0024, 0.0026, 0.0022, 0.0023, 0.0024, 0.0024, 0.0024, 0.0026, 0.0023,\n",
       "         0.0025, 0.0024, 0.0026, 0.0028, 0.0027, 0.0027, 0.0023, 0.0026, 0.0025,\n",
       "         0.0028, 0.0026, 0.0025, 0.0026, 0.0025, 0.0026, 0.0025, 0.0023, 0.0024,\n",
       "         0.0027, 0.0026, 0.0025, 0.0021, 0.0025, 0.0027, 0.0027, 0.0025, 0.0024,\n",
       "         0.0028, 0.0025, 0.0026, 0.0026, 0.0025, 0.0026, 0.0025, 0.0026, 0.0025,\n",
       "         0.0025, 0.0026, 0.0024, 0.0023, 0.0023, 0.0024, 0.0026, 0.0026, 0.0024,\n",
       "         0.0025, 0.0022, 0.0022, 0.0026, 0.0026, 0.0024, 0.0025, 0.0025, 0.0025,\n",
       "         0.0029, 0.0024, 0.0025, 0.0022, 0.0027, 0.0027, 0.0023, 0.0026, 0.0024,\n",
       "         0.0024, 0.0025, 0.0025, 0.0025, 0.0023, 0.0025, 0.0028, 0.0026, 0.0026,\n",
       "         0.0026, 0.0028, 0.0025, 0.0023, 0.0026, 0.0024, 0.0026, 0.0024, 0.0025,\n",
       "         0.0025, 0.0024, 0.0024, 0.0023, 0.0022, 0.0022, 0.0022, 0.0023, 0.0025,\n",
       "         0.0027, 0.0025, 0.0025, 0.0022, 0.0028, 0.0027, 0.0023, 0.0026, 0.0026,\n",
       "         0.0025, 0.0026, 0.0026, 0.0024, 0.0025, 0.0022, 0.0026, 0.0025, 0.0026,\n",
       "         0.0024, 0.0026, 0.0025, 0.0025, 0.0023, 0.0026, 0.0026, 0.0027, 0.0025,\n",
       "         0.0023, 0.0026, 0.0023, 0.0025, 0.0023, 0.0023, 0.0023, 0.0027, 0.0025,\n",
       "         0.0029, 0.0026, 0.0026, 0.0025],\n",
       "        [0.0025, 0.0023, 0.0021, 0.0023, 0.0024, 0.0023, 0.0026, 0.0022, 0.0024,\n",
       "         0.0026, 0.0024, 0.0027, 0.0026, 0.0024, 0.0024, 0.0023, 0.0028, 0.0025,\n",
       "         0.0026, 0.0023, 0.0026, 0.0025, 0.0023, 0.0029, 0.0021, 0.0023, 0.0027,\n",
       "         0.0023, 0.0025, 0.0029, 0.0026, 0.0027, 0.0025, 0.0026, 0.0027, 0.0026,\n",
       "         0.0022, 0.0025, 0.0024, 0.0025, 0.0026, 0.0025, 0.0022, 0.0023, 0.0024,\n",
       "         0.0023, 0.0022, 0.0023, 0.0026, 0.0025, 0.0024, 0.0028, 0.0025, 0.0023,\n",
       "         0.0025, 0.0026, 0.0025, 0.0024, 0.0024, 0.0023, 0.0023, 0.0025, 0.0032,\n",
       "         0.0026, 0.0023, 0.0027, 0.0024, 0.0024, 0.0024, 0.0025, 0.0022, 0.0022,\n",
       "         0.0021, 0.0026, 0.0027, 0.0024, 0.0024, 0.0026, 0.0023, 0.0027, 0.0024,\n",
       "         0.0028, 0.0027, 0.0028, 0.0023, 0.0026, 0.0026, 0.0028, 0.0023, 0.0028,\n",
       "         0.0026, 0.0021, 0.0028, 0.0024, 0.0029, 0.0030, 0.0026, 0.0023, 0.0023,\n",
       "         0.0023, 0.0029, 0.0025, 0.0026, 0.0026, 0.0025, 0.0026, 0.0026, 0.0025,\n",
       "         0.0027, 0.0026, 0.0022, 0.0026, 0.0023, 0.0023, 0.0027, 0.0026, 0.0025,\n",
       "         0.0025, 0.0024, 0.0027, 0.0027, 0.0026, 0.0026, 0.0024, 0.0028, 0.0027,\n",
       "         0.0029, 0.0025, 0.0023, 0.0027, 0.0022, 0.0024, 0.0025, 0.0026, 0.0024,\n",
       "         0.0022, 0.0026, 0.0023, 0.0025, 0.0024, 0.0024, 0.0024, 0.0026, 0.0026,\n",
       "         0.0023, 0.0027, 0.0029, 0.0023, 0.0025, 0.0025, 0.0024, 0.0023, 0.0025,\n",
       "         0.0024, 0.0028, 0.0024, 0.0022, 0.0027, 0.0025, 0.0023, 0.0026, 0.0023,\n",
       "         0.0024, 0.0026, 0.0025, 0.0024, 0.0022, 0.0024, 0.0024, 0.0024, 0.0027,\n",
       "         0.0026, 0.0025, 0.0026, 0.0024, 0.0027, 0.0024, 0.0028, 0.0027, 0.0025,\n",
       "         0.0029, 0.0024, 0.0023, 0.0023, 0.0025, 0.0022, 0.0025, 0.0027, 0.0026,\n",
       "         0.0025, 0.0026, 0.0028, 0.0024, 0.0025, 0.0024, 0.0027, 0.0028, 0.0024,\n",
       "         0.0022, 0.0025, 0.0025, 0.0027, 0.0026, 0.0026, 0.0022, 0.0028, 0.0026,\n",
       "         0.0026, 0.0026, 0.0025, 0.0023, 0.0025, 0.0028, 0.0026, 0.0022, 0.0026,\n",
       "         0.0024, 0.0026, 0.0027, 0.0024, 0.0025, 0.0025, 0.0025, 0.0026, 0.0027,\n",
       "         0.0028, 0.0026, 0.0024, 0.0027, 0.0027, 0.0029, 0.0024, 0.0027, 0.0026,\n",
       "         0.0029, 0.0026, 0.0030, 0.0028, 0.0023, 0.0027, 0.0026, 0.0028, 0.0023,\n",
       "         0.0026, 0.0026, 0.0023, 0.0027, 0.0026, 0.0025, 0.0026, 0.0025, 0.0025,\n",
       "         0.0024, 0.0023, 0.0025, 0.0028, 0.0024, 0.0025, 0.0027, 0.0029, 0.0024,\n",
       "         0.0026, 0.0026, 0.0026, 0.0022, 0.0023, 0.0024, 0.0025, 0.0027, 0.0026,\n",
       "         0.0024, 0.0024, 0.0022, 0.0025, 0.0023, 0.0027, 0.0024, 0.0021, 0.0024,\n",
       "         0.0029, 0.0024, 0.0023, 0.0026, 0.0026, 0.0023, 0.0027, 0.0029, 0.0025,\n",
       "         0.0027, 0.0022, 0.0026, 0.0022, 0.0025, 0.0023, 0.0024, 0.0023, 0.0024,\n",
       "         0.0024, 0.0026, 0.0025, 0.0024, 0.0028, 0.0025, 0.0024, 0.0023, 0.0023,\n",
       "         0.0026, 0.0026, 0.0023, 0.0025, 0.0024, 0.0025, 0.0027, 0.0025, 0.0025,\n",
       "         0.0025, 0.0028, 0.0021, 0.0027, 0.0024, 0.0027, 0.0024, 0.0023, 0.0023,\n",
       "         0.0024, 0.0027, 0.0025, 0.0022, 0.0023, 0.0023, 0.0026, 0.0022, 0.0023,\n",
       "         0.0028, 0.0024, 0.0025, 0.0025, 0.0023, 0.0023, 0.0026, 0.0024, 0.0026,\n",
       "         0.0026, 0.0026, 0.0022, 0.0024, 0.0027, 0.0028, 0.0026, 0.0027, 0.0023,\n",
       "         0.0026, 0.0023, 0.0027, 0.0023, 0.0022, 0.0024, 0.0024, 0.0024, 0.0025,\n",
       "         0.0025, 0.0022, 0.0023, 0.0023, 0.0028, 0.0025, 0.0024, 0.0027, 0.0024,\n",
       "         0.0026, 0.0020, 0.0022, 0.0024, 0.0024, 0.0024, 0.0026, 0.0026, 0.0025,\n",
       "         0.0023, 0.0028, 0.0024, 0.0026, 0.0025, 0.0025, 0.0024, 0.0024, 0.0023,\n",
       "         0.0022, 0.0027, 0.0021, 0.0024, 0.0023, 0.0028, 0.0029, 0.0026, 0.0025,\n",
       "         0.0024, 0.0025, 0.0027, 0.0025]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "summaryで中身を確認"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# torchinfo.summary(\n",
    "#     model,\n",
    "#     (4, 3, 16, 224, 224),\n",
    "#     depth=4,\n",
    "#     col_names=[\"input_size\",\n",
    "#                \"output_size\"],\n",
    "#     row_settings=(\"var_names\",)\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "便利関数を定義"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(train_loader),\n",
    "                  total=len(train_loader),\n",
    "                  leave=True) as pbar_loss:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, batch in pbar_loss:\n",
    "                pbar_loss.set_description(\"[train]\")\n",
    "\n",
    "                inputs, targets = batch['video'].to(device), batch['label'].to(device)\n",
    "                bs = inputs.size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, targets), bs)\n",
    "\n",
    "                pbar_loss.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                    train_loss.val, train_acc.val,\n",
    "                ))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a860adac2fb42999714a2698dd780de"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15016.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e07124f759dd48368d50e267f75f9a3c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-692dc01bbf35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-90c01b1a4c9a>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, val, n)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "moov atom not found\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "fine-tuningなので速い．\n",
    "- 4GPUでおよそ4.5it/s，1エポック約2分\n",
    "- 1GPUでおよそ5it/s，1エポック約3分（596 iterations）\n",
    "\n",
    "スクラッチで学習するなら\n",
    "- 4GPUでおよそ2.6it/s，1エポック約4分\n",
    "- 1GPUでおよそ1.8it/s，1エポック約5.5分（596 iterations）\n",
    "\n",
    "\n",
    "以下の設定\n",
    "- video_num_subsampled = 16\n",
    "- batch_size = 16\n",
    "- num_workers = 24"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}